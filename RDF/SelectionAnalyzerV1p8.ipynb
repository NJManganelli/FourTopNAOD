{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#!pip install graphviz --user\n",
    "#!echo $PYTHONPATH\n",
    "#!ls -ltr /eos/user/n/nmangane/.local/lib/python2.7/site-packages/\n",
    "#!export PATH=/eos/user/n/nmangane/.local/lib/python2.7/site-packages/:$PATH\n",
    "#!ls -ltr | grep .root\n",
    "!ls -ltr /eos/user/n/nmangane/CMSSW/CMSSW_10_2_18/src/FourTopNAOD/RDF/\n",
    "#Notes: When nJet cut is applied for variations, must account for different counts when JES changes. \n",
    "#Therefore, this must b inside the histogramming function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os, time, sys, collections, array\n",
    "import ROOT\n",
    "import numpy as np\n",
    "#from IPython.display import Image, display, SVG\n",
    "#import graphviz\n",
    "\n",
    "useSpark = False #Doesn't seem to work with gcc8 at least...\n",
    "if useSpark:\n",
    "    import PyRDF\n",
    "    PyRDF.use(\"spark\", {'npartitions': '8'}) #was 32 in example\n",
    "    RDF = PyRDF.RDataFrame\n",
    "else:\n",
    "    #print(\"DISABLING IMT for bebugging branches\")\n",
    "    ROOT.ROOT.EnableImplicitMT()\n",
    "    RS = ROOT.ROOT\n",
    "    RDF = RS.RDataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load functions, can eventually be changed to ROOT.gInterpreter.Declare(#include \"someheader.h\")\n",
    "#WARNING! Do not rerun this cell without restarting the kernel, it will kill it!\n",
    "ROOT.TH1.SetDefaultSumw2() #Make sure errors are done this way\n",
    "ROOT.gROOT.ProcessLine(\".L /eos/user/n/nmangane/CMSSW/CMSSW_10_2_18/src/FourTopNAOD/RDF/test_class.cpp\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FIXME: Need filter efficiency calculated for single lepton generator filtered sample. First approximation will be from MCCM (0.15) but as seen before, it's not ideal. \n",
    "#May need to recalculate using genWeight/sumWeights instead of sign(genWeight)/(nPositiveEvents - nNegativeEvents), confirm if there's any difference.\n",
    "lumi = {\"2017\": 41.53,\n",
    "        \"2018\": 1}\n",
    "era = \"2017\"\n",
    "leg_dict = {\"tttt\": ROOT.kAzure-2,\n",
    "            \"ttbar\": ROOT.kRed,\n",
    "            \"singletop\": ROOT.kYellow,\n",
    "            \"ttH\": ROOT.kMagenta,\n",
    "            \"ttVJets\": ROOT.kViolet,\n",
    "            \"ttultrarare\": ROOT.kGreen,\n",
    "            \"DY\": ROOT.kCyan,\n",
    "            \"Data\": ROOT.kBlack,\n",
    "            \"QCD\": ROOT.kPink,\n",
    "           }\n",
    "microbookerV2 = {\n",
    "    \"tttt\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"isData\": False,\n",
    "        \"nEvents\": 2273928,\n",
    "        \"nEventsPositive\": 1561946,\n",
    "        \"nEventsNegative\": 711982,\n",
    "        \"sumWeights\": 18645.487772,\n",
    "        \"sumWeights2\": 1094.209551,\n",
    "        \"isSignal\": True,\n",
    "        \"crossSection\": 0.012,\n",
    "        \"color\": leg_dict[\"tttt\"],\n",
    "        \"source\": {\"LJMLogic\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/tttt-*_2017_v2.root\",},\n",
    "        \"sourceSPARK\": [\"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/tttt-1_2017_v2.root\",\n",
    "                       \"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/tttt-2_2017_v2.root\"],\n",
    "        \"destination\": \"/$HIST_CLASS/$HIST/tttt/$SYSTEMATIC\",\n",
    "    },\n",
    "    \"tt_DL\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"isData\": False,\n",
    "        \"nEvents\": 69098644,\n",
    "        \"nEventsPositive\": 68818780,\n",
    "        \"nEventsNegative\": 279864,\n",
    "        \"sumWeights\": 4980769113.241218,\n",
    "        \"sumWeights2\": 364913493679.955078,\n",
    "        \"isSignal\": False,\n",
    "        \"crossSection\": 89.0482,\n",
    "        \"color\": leg_dict[\"ttbar\"],\n",
    "        \"source\": {\"LJMLogic\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/tt_DL-NOM-*_2017_v2.root\",},\n",
    "        \"sourceSPARK\": [\"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/tt_DL-NOM-1_2017_v2.root\",\n",
    "                       \"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/tt_DL-NOM-2_2017_v2.root\",\n",
    "                       \"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/tt_DL-NOM-3_2017_v2.root\",\n",
    "                       \"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/tt_DL-NOM-4_2017_v2.root\",\n",
    "                       \"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/tt_DL-NOM-5_2017_v2.root\",\n",
    "                       \"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/tt_DL-NOM-6_2017_v2.root\",\n",
    "                       \"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/tt_DL-NOM-7_2017_v2.root\",\n",
    "                       \"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/tt_DL-NOM-8_2017_v2.root\",\n",
    "                       \"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/tt_DL-NOM-9_2017_v2.root\",\n",
    "                       \"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/tt_DL-NOM-10_2017_v2.root\",\n",
    "                       \"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/tt_DL-NOM-11_2017_v2.root\",\n",
    "                       \"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/tt_DL-NOM-12_2017_v2.root\",\n",
    "                       \"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/tt_DL-NOM-13_2017_v2.root\",\n",
    "                       \"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/tt_DL-NOM-14_2017_v2.root\",],\n",
    "        \"destination\": \"/$HIST_CLASS/$HIST/tt_DL/$SYSTEMATIC\",\n",
    "        \"stitch\": {\"mode\": \"Flag\",\n",
    "                   \"source\": \"Nominal\",\n",
    "                   \"channel\": \"DL\"\n",
    "                  },\n",
    "    },\n",
    "    \"tt_SL\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"isData\": False,\n",
    "        \"nEvents\": 20122010,\n",
    "        \"nEventsPositive\": 20040607,\n",
    "        \"nEventsNegative\": 81403,\n",
    "        \"sumWeights\": 6052480345.748356,\n",
    "        \"sumWeights2\": 1850350248120.376221,\n",
    "        \"isSignal\": False,\n",
    "        \"crossSection\": 366.2073,\n",
    "        \"color\": leg_dict[\"ttbar\"],\n",
    "        \"source\": {\"LJMLogic\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/tt_SL-NOM_2017_v2.root\",},\n",
    "        \"sourceSPARK\": [\"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/tt_SL-NOM_2017_v2.root\",],\n",
    "        \"destination\": \"/$HIST_CLASS/$HIST/tt_SL/$SYSTEMATIC\",\n",
    "        \"stitch\": {\"mode\": \"Flag\",\n",
    "                   \"source\": \"Nominal\",\n",
    "                   \"channel\": \"SL\"\n",
    "                  },\n",
    "    },\n",
    "\n",
    "    \"ST_tW\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"isData\": False,\n",
    "        \"nEvents\": 7945242,\n",
    "        \"nEventsPositive\": 7914815,\n",
    "        \"nEventsNegative\": 30427,\n",
    "        \"sumWeights\": 277241050.840222,\n",
    "        \"sumWeights2\": 9823995766.508368,\n",
    "        \"isSignal\": False,\n",
    "        \"crossSection\": 35.8,\n",
    "        \"color\": leg_dict[\"singletop\"],\n",
    "        \"source\": {\"LJMLogic\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/ST_tW_2017_v2.root\",},\n",
    "        \"sourceSPARK\": [\"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/ST_tW_2017_v2.root\"],\n",
    "        \"destination\": \"/$HIST_CLASS/$HIST/ST_tW/$SYSTEMATIC\",\n",
    "    },\n",
    "    \"ST_tbarW\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"isData\": False,\n",
    "        \"nEvents\": 7745276,\n",
    "        \"nEventsPositive\": 7715654,\n",
    "        \"nEventsNegative\": 30427,\n",
    "        \"sumWeights\": 270762750.172525,\n",
    "        \"sumWeights2\": 9611964941.797768,\n",
    "        \"isSignal\": False,\n",
    "        \"crossSection\": 35.8,\n",
    "        \"color\": leg_dict[\"singletop\"],\n",
    "        \"source\": {\"LJMLogic\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/ST_tbarW_2017_v2.root\",},\n",
    "        \"sourceSPARK\": [\"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/ST_tbarW_2017_v2.root\"],\n",
    "        \"destination\": \"/$HIST_CLASS/$HIST/ST_tbarW/$SYSTEMATIC\",\n",
    "    },\n",
    "    \"DYJets_DL\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"isData\": False,\n",
    "        \"nEvents\": 49125561,\n",
    "        \"nEventsPositive\": 49103859,\n",
    "        \"nEventsNegative\": 21702,\n",
    "        \"sumWeights\": 49082157.000000,\n",
    "        \"sumWeights2\": 49125561.000000,\n",
    "        \"isSignal\": False,\n",
    "        \"crossSection\": 5075.6,\n",
    "        \"color\": leg_dict[\"DY\"],\n",
    "        \"source\": {\"LJMLogic\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/DYJets_DL-*_2017_v2.root\",},\n",
    "        \"sourceSPARK\": [\"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/DYJets_DL-1_2017_v2.root\",\n",
    "                        \"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/DYJets_DL-2_2017_v2.root\",\n",
    "                        \"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/DYJets_DL-3_2017_v2.root\",\n",
    "                        \"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/DYJets_DL-4_2017_v2.root\",\n",
    "                        \"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/DYJets_DL-5_2017_v2.root\",\n",
    "                        \"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/DYJets_DL-6_2017_v2.root\",\n",
    "                        \"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/DYJets_DL-7_2017_v2.root\",],\n",
    "        \"destination\": \"/$HIST_CLASS/$HIST/DYJets_DL/$SYSTEMATIC\",\n",
    "    },\n",
    "}\n",
    "tt_data_V2 = {\n",
    "    \"tt_DL\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"isData\": False,\n",
    "        \"nEvents\": 69098644,\n",
    "        \"nEventsPositive\": 68818780,\n",
    "        \"nEventsNegative\": 279864,\n",
    "        \"sumWeights\": 4980769113.241218,\n",
    "        \"sumWeights2\": 364913493679.955078,\n",
    "        \"isSignal\": False,\n",
    "        \"crossSection\": 89.0482,\n",
    "        \"color\": leg_dict[\"ttbar\"],\n",
    "        \"source\": {\"LJMLogic\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/tt_DL-NOM-*_2017_v2.root\",},\n",
    "        \"sourceSPARK\": [\"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/tt_DL-NOM-1_2017_v2.root\",\n",
    "                       \"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/tt_DL-NOM-2_2017_v2.root\",\n",
    "                       \"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/tt_DL-NOM-3_2017_v2.root\",\n",
    "                       \"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/tt_DL-NOM-4_2017_v2.root\",\n",
    "                       \"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/tt_DL-NOM-5_2017_v2.root\",\n",
    "                       \"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/tt_DL-NOM-6_2017_v2.root\",\n",
    "                       \"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/tt_DL-NOM-7_2017_v2.root\",\n",
    "                       \"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/tt_DL-NOM-8_2017_v2.root\",\n",
    "                       \"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/tt_DL-NOM-9_2017_v2.root\",\n",
    "                       \"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/tt_DL-NOM-10_2017_v2.root\",\n",
    "                       \"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/tt_DL-NOM-11_2017_v2.root\",\n",
    "                       \"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/tt_DL-NOM-12_2017_v2.root\",\n",
    "                       \"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/tt_DL-NOM-13_2017_v2.root\",\n",
    "                       \"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/tt_DL-NOM-14_2017_v2.root\",],\n",
    "        \"destination\": \"/$HIST_CLASS/$HIST/tt_DL/$SYSTEMATIC\",\n",
    "    },\n",
    "    \"tt_SL\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"isData\": False,\n",
    "        \"nEvents\": 20122010,\n",
    "        \"nEventsPositive\": 20040607,\n",
    "        \"nEventsNegative\": 81403,\n",
    "        \"sumWeights\": 6052480345.748356,\n",
    "        \"sumWeights2\": 1850350248120.376221,\n",
    "        \"isSignal\": False,\n",
    "        \"crossSection\": 366.2073,\n",
    "        \"color\": leg_dict[\"ttbar\"],\n",
    "        \"source\": {\"LJMLogic\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/tt_SL-NOM_2017_v2.root\",},\n",
    "        \"sourceSPARK\": [\"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/tt_SL-NOM_2017_v2.root\",],\n",
    "        \"destination\": \"/$HIST_CLASS/$HIST/tt_SL/$SYSTEMATIC\",\n",
    "    },\n",
    "    \"ElMu\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"subera\": \"BCDEF\",\n",
    "        \"channel\": \"ElMu\",\n",
    "        \"isData\": True,\n",
    "        \"nEvents\": 4453465 + 15595214 + 9164365 + 19043421 + 25776363,\n",
    "        \"color\": leg_dict[\"Data\"],\n",
    "        \"source\": {\"LJMLogic\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/ElMu_*_2017.root\",},\n",
    "        \"sourceSPARK\": [\"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/ElMu_*_2017.root\",],\n",
    "        \"destination\": \"/$HIST_CLASS/$HIST/ElMu/$SYSTEMATIC\",\n",
    "    },    \n",
    "}\n",
    "bookerV2_MC = {\n",
    "    \"tttt\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"isData\": False,\n",
    "        \"nEvents\": 2273928,\n",
    "        \"nEventsPositive\": 1561946,\n",
    "        \"nEventsNegative\": 711982,\n",
    "        \"sumWeights\": 18645.487772,\n",
    "        \"sumWeights2\": 1094.209551,\n",
    "        \"isSignal\": True,\n",
    "        \"crossSection\": 0.012,\n",
    "        \"color\": leg_dict[\"tttt\"],\n",
    "        \"source\": {\"LJMLogic\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/tttt-*_2017_v2.root\",\n",
    "                   \"LJMLogic/ElMu_selection\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/ElMu_selection/tttt-*_2017_v2*.root\",\n",
    "                   \"LJMLogic/MuMu_selection\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/MuMu_selection/tttt-*_2017_v2*.root\",\n",
    "                   \"LJMLogic/ElEl_selection\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/ElEl_selection/tttt-*_2017_v2*.root\",\n",
    "                  },\n",
    "        \"sourceSPARK\": [\"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/tttt-1_2017_v2.root\",\n",
    "                       \"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/tttt-2_2017_v2.root\"],\n",
    "        \"destination\": \"/$HIST_CLASS/$HIST/tttt/$SYSTEMATIC\",\n",
    "    },\n",
    "    \"tt_DL\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"isData\": False,\n",
    "        \"nEvents\": 69098644,\n",
    "        \"nEventsPositive\": 68818780,\n",
    "        \"nEventsNegative\": 279864,\n",
    "        \"sumWeights\": 4980769113.241218,\n",
    "        \"sumWeights2\": 364913493679.955078,\n",
    "        \"isSignal\": False,\n",
    "        \"doFilter\": True,\n",
    "        \"crossSection\": 89.0482,\n",
    "        \"color\": leg_dict[\"ttbar\"],\n",
    "        \"source\": {\"LJMLogic\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/tt_DL-NOM-*_2017_v2.root\",\n",
    "                   \"LJMLogic/ElMu_selection\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/ElMu_selection/tt_DL-NOM-*_2017_v2*.root\",\n",
    "                   \"LJMLogic/MuMu_selection\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/MuMu_selection/tt_DL-NOM-*_2017_v2*.root\",\n",
    "                   \"LJMLogic/ElEl_selection\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/ElEl_selection/tt_DL-NOM-*_2017_v2*.root\",\n",
    "                  },\n",
    "        \"sourceSPARK\": [\"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/tt_DL-NOM-1_2017_v2.root\",\n",
    "                       \"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/tt_DL-NOM-2_2017_v2.root\",\n",
    "                       \"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/tt_DL-NOM-3_2017_v2.root\",\n",
    "                       \"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/tt_DL-NOM-4_2017_v2.root\",\n",
    "                       \"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/tt_DL-NOM-5_2017_v2.root\",\n",
    "                       \"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/tt_DL-NOM-6_2017_v2.root\",\n",
    "                       \"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/tt_DL-NOM-7_2017_v2.root\",\n",
    "                       \"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/tt_DL-NOM-8_2017_v2.root\",\n",
    "                       \"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/tt_DL-NOM-9_2017_v2.root\",\n",
    "                       \"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/tt_DL-NOM-10_2017_v2.root\",\n",
    "                       \"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/tt_DL-NOM-11_2017_v2.root\",\n",
    "                       \"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/tt_DL-NOM-12_2017_v2.root\",\n",
    "                       \"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/tt_DL-NOM-13_2017_v2.root\",\n",
    "                       \"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/tt_DL-NOM-14_2017_v2.root\",],\n",
    "        \"destination\": \"/$HIST_CLASS/$HIST/tt_DL/$SYSTEMATIC\",\n",
    "        \"stitch\": {\"mode\": \"Flag\",\n",
    "                   \"source\": \"Nominal\",\n",
    "                   \"channel\": \"DL\"\n",
    "                  },\n",
    "    },\n",
    "    \"tt_DL-GF\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"isData\": False,\n",
    "        \"nEvents\": 8510388,\n",
    "        \"nEventsPositive\": 8467543,\n",
    "        \"nEventsNegative\": 42845,\n",
    "        \"sumWeights\": 612101836.284397,\n",
    "        \"sumWeights2\": 44925503249.097206,\n",
    "        \"isSignal\": False,\n",
    "        \"doFilter\": True,\n",
    "        \"crossSection\": 1.4815,\n",
    "        \"color\": leg_dict[\"ttbar\"],\n",
    "        \"source\": {\"LJMLogic\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/tt_DL-GF-*_2017_v2.root\",\n",
    "                   \"LJMLogic/ElMu_selection\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/ElMu_selection/tt_DL-GF-*_2017_v2*.root\",\n",
    "                   \"LJMLogic/MuMu_selection\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/MuMu_selection/tt_DL-GF-*_2017_v2*.root\",\n",
    "                   \"LJMLogic/ElEl_selection\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/ElEl_selection/tt_DL-GF-*_2017_v2*.root\",\n",
    "                  },\n",
    "        \"sourceSPARK\": [\"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/tt_DL-GF-1_2017_v2.root\",\n",
    "                       \"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/tt_DL-GF-2_2017_v2.root\",\n",
    "                       \"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/tt_DL-GF-3_2017_v2.root\",\n",
    "                       \"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/tt_DL-GF-4_2017_v2.root\",],\n",
    "        \"destination\": \"/$HIST_CLASS/$HIST/tt_DL-GF/$SYSTEMATIC\",\n",
    "        \"stitch\": {\"mode\": \"Flag\",\n",
    "                   \"source\": \"Filtered\",\n",
    "                   \"channel\": \"DL\"\n",
    "                  },\n",
    "    },\n",
    "    \"tt_SL\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"isData\": False,\n",
    "        \"nEvents\": 20122010,\n",
    "        \"nEventsPositive\": 20040607,\n",
    "        \"nEventsNegative\": 81403,\n",
    "        \"sumWeights\": 6052480345.748356,\n",
    "        \"sumWeights2\": 1850350248120.376221,\n",
    "        \"isSignal\": False,\n",
    "        \"doFilter\": True,\n",
    "        \"crossSection\": 366.2073,\n",
    "        \"color\": leg_dict[\"ttbar\"],\n",
    "        \"source\": {\"LJMLogic\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/tt_SL-NOM_2017_v2.root\",\n",
    "                   \"LJMLogic/ElMu_selection\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/ElMu_selection/tt_SL-NOM_2017_v2*.root\",\n",
    "                   \"LJMLogic/MuMu_selection\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/MuMu_selection/tt_SL-NOM_2017_v2*.root\",\n",
    "                   \"LJMLogic/ElEl_selection\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/ElEl_selection/tt_SL-NOM_2017_v2*.root\",\n",
    "                  },\n",
    "        \"sourceSPARK\": [\"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/tt_SL-NOM_2017_v2.root\",],\n",
    "        \"destination\": \"/$HIST_CLASS/$HIST/tt_SL/$SYSTEMATIC\",\n",
    "        \"stitch\": {\"mode\": \"Flag\",\n",
    "                   \"source\": \"Nominal\",\n",
    "                   \"channel\": \"SL\"\n",
    "                  },\n",
    "    },\n",
    "    \"tt_SL-GF\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"isData\": False,\n",
    "        \"nEvents\": 8836856,\n",
    "        \"nEventsPositive\": 8794464,\n",
    "        \"nEventsNegative\": 42392,\n",
    "        \"sumWeights\": 2653328498.476976,\n",
    "        \"sumWeights2\": 812201885978.209229,\n",
    "        \"isSignal\": False,\n",
    "        \"doFilter\": True,\n",
    "        \"crossSection\": 12.4071,\n",
    "        \"color\": leg_dict[\"ttbar\"],\n",
    "        \"source\": {\"LJMLogic\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/tt_SL-GF_2017_v2.root\",\n",
    "                   \"LJMLogic/ElMu_selection\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/ElMu_selection/tt_SL-GF_2017_v2*.root\",\n",
    "                   \"LJMLogic/MuMu_selection\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/MuMu_selection/tt_SL-GF_2017_v2*.root\",\n",
    "                   \"LJMLogic/ElEl_selection\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/ElEl_selection/tt_SL-GF_2017_v2*.root\",\n",
    "                  },\n",
    "        \"sourceSPARK\": [\"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/tt_SL-GF_2017_v2.root\"],\n",
    "        \"destination\": \"/$HIST_CLASS/$HIST/tt_SL-GF/$SYSTEMATIC\",\n",
    "        \"stitch\": {\"mode\": \"Flag\",\n",
    "                   \"source\": \"Filtered\",\n",
    "                   \"channel\": \"SL\"\n",
    "                  },\n",
    "    },\n",
    "    \"ST_tW\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"isData\": False,\n",
    "        \"nEvents\": 7945242,\n",
    "        \"nEventsPositive\": 7914815,\n",
    "        \"nEventsNegative\": 30427,\n",
    "        \"sumWeights\": 277241050.840222,\n",
    "        \"sumWeights2\": 9823995766.508368,\n",
    "        \"isSignal\": False,\n",
    "        \"crossSection\": 35.8,\n",
    "        \"color\": leg_dict[\"singletop\"],\n",
    "        \"source\": {\"LJMLogic\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/ST_tW_2017_v2.root\",\n",
    "                   \"LJMLogic/ElMu_selection\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/ElMu_selection/ST_tW_2017_v2*.root\",\n",
    "                   \"LJMLogic/MuMu_selection\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/MuMu_selection/ST_tW_2017_v2*.root\",\n",
    "                   \"LJMLogic/ElEl_selection\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/ElEl_selection/ST_tW_2017_v2*.root\",\n",
    "                  },\n",
    "        \"sourceSPARK\": [\"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/ST_tW_2017_v2.root\"],\n",
    "        \"destination\": \"/$HIST_CLASS/$HIST/ST_tW/$SYSTEMATIC\",\n",
    "    },\n",
    "    \"ST_tbarW\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"isData\": False,\n",
    "        \"nEvents\": 7745276,\n",
    "        \"nEventsPositive\": 7715654,\n",
    "        \"nEventsNegative\": 30427,\n",
    "        \"sumWeights\": 270762750.172525,\n",
    "        \"sumWeights2\": 9611964941.797768,\n",
    "        \"isSignal\": False,\n",
    "        \"crossSection\": 35.8,\n",
    "        \"color\": leg_dict[\"singletop\"],\n",
    "        \"source\": {\"LJMLogic\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/ST_tbarW_2017_v2.root\",\n",
    "                   \"LJMLogic/ElMu_selection\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/ElMu_selection/ST_tbarW_2017_v2*.root\",\n",
    "                   \"LJMLogic/MuMu_selection\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/MuMu_selection/ST_tbarW_2017_v2*.root\",\n",
    "                   \"LJMLogic/ElEl_selection\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/ElEl_selection/ST_tbarW_2017_v2*.root\",\n",
    "                  },\n",
    "        \"sourceSPARK\": [\"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/ST_tbarW_2017_v2.root\"],\n",
    "        \"destination\": \"/$HIST_CLASS/$HIST/ST_tbarW/$SYSTEMATIC\",\n",
    "    },\n",
    "    \"DYJets_DL\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"isData\": False,\n",
    "        \"nEvents\": 49125561,\n",
    "        \"nEventsPositive\": 49103859,\n",
    "        \"nEventsNegative\": 21702,\n",
    "        \"sumWeights\": 49082157.000000,\n",
    "        \"sumWeights2\": 49125561.000000,\n",
    "        \"isSignal\": False,\n",
    "        \"crossSection\": 5075.6,\n",
    "        \"color\": leg_dict[\"DY\"],\n",
    "        \"source\": {\"LJMLogic\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/DYJets_DL-*_2017_v2.root\",\n",
    "                   \"LJMLogic/ElMu_selection\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/ElMu_selection/DYJets_DL-*_2017_v2*.root\",\n",
    "                   \"LJMLogic/MuMu_selection\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/MuMu_selection/DYJets_DL-*_2017_v2*.root\",\n",
    "                   \"LJMLogic/ElEl_selection\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/ElEl_selection/DYJets_DL-*_2017_v2*.root\",\n",
    "                  },\n",
    "        \"sourceSPARK\": [\"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/DYJets_DL-1_2017_v2.root\",\n",
    "                        \"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/DYJets_DL-2_2017_v2.root\",\n",
    "                        \"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/DYJets_DL-3_2017_v2.root\",\n",
    "                        \"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/DYJets_DL-4_2017_v2.root\",\n",
    "                        \"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/DYJets_DL-5_2017_v2.root\",\n",
    "                        \"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/DYJets_DL-6_2017_v2.root\",\n",
    "                        \"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/DYJets_DL-7_2017_v2.root\",],\n",
    "        \"destination\": \"/$HIST_CLASS/$HIST/DYJets_DL/$SYSTEMATIC\",\n",
    "    },\n",
    "    \"ttH\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"isData\": False,\n",
    "        \"nEvents\": 8000000,\n",
    "        \"nEventsPositive\": 7916867,\n",
    "        \"nEventsNegative\": 83133,\n",
    "        \"sumWeights\": 4216319.315884,\n",
    "        \"sumWeights2\": 2317497.816608,\n",
    "        \"isSignal\": False,\n",
    "        \"crossSection\": 0.2934,\n",
    "        \"color\": leg_dict[\"ttH\"],\n",
    "        \"source\": {\"LJMLogic\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/ttH_2017_v2.root\",\n",
    "                   \"LJMLogic/ElMu_selection\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/ElMu_selection/ttH_2017_v2*.root\",\n",
    "                   \"LJMLogic/MuMu_selection\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/MuMu_selection/ttH_2017_v2*.root\",\n",
    "                   \"LJMLogic/ElEl_selection\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/ElEl_selection/ttH_2017_v2*.root\",\n",
    "                  },\n",
    "        \"sourceSPARK\": [\"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/ttH_2017_v2.root\",],\n",
    "        \"destination\": \"/$HIST_CLASS/$HIST/ttH/$SYSTEMATIC\",\n",
    "    },\n",
    "    \"ttWJets\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"isData\": False,\n",
    "        \"nEvents\": 9425384,\n",
    "        \"nEventsPositive\": 9404856,\n",
    "        \"nEventsNegative\": 20528,\n",
    "        \"sumWeights\": 9384328.000000,\n",
    "        \"sumWeights2\": 9425384.000000,\n",
    "        \"isSignal\": False,\n",
    "        \"crossSection\": 0.611,\n",
    "        \"color\": leg_dict[\"ttVJets\"],\n",
    "        \"source\": {\"LJMLogic\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/ttWJets_2017_v2.root\",\n",
    "                   \"LJMLogic/ElMu_selection\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/ElMu_selection/ttWJets_2017_v2*.root\",\n",
    "                   \"LJMLogic/MuMu_selection\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/MuMu_selection/ttWJets_2017_v2*.root\",\n",
    "                   \"LJMLogic/ElEl_selection\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/ElEl_selection/ttWJets_2017_v2*.root\",\n",
    "                  },\n",
    "        \"sourceSPARK\": [\"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/ttWJets_2017_v2.root\",],\n",
    "        \"destination\": \"/$HIST_CLASS/$HIST/ttWJets/$SYSTEMATIC\",\n",
    "    },\n",
    "    \"ttZJets\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"isData\": False,\n",
    "        \"nEvents\": 8536618,\n",
    "        \"nEventsPositive\": 8527846,\n",
    "        \"nEventsNegative\": 8772,\n",
    "        \"sumWeights\": 8519074.000000,\n",
    "        \"sumWeights2\": 8536618.000000,\n",
    "        \"isSignal\": False,\n",
    "        \"crossSection\": 0.783,\n",
    "        \"color\": leg_dict[\"ttVJets\"],\n",
    "        \"source\": {\"LJMLogic\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/ttZJets_2017_v2.root\",\n",
    "                   \"LJMLogic/ElMu_selection\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/ElMu_selection/ttZJets_2017_v2*.root\",\n",
    "                   \"LJMLogic/MuMu_selection\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/MuMu_selection/ttZJets_2017_v2*.root\",\n",
    "                   \"LJMLogic/ElEl_selection\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/ElEl_selection/ttZJets_2017_v2*.root\",\n",
    "                  },\n",
    "        \"sourceSPARK\": [\"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/ttZJets_2017_v2.root\",],\n",
    "        \"destination\": \"/$HIST_CLASS/$HIST/ttZJets/$SYSTEMATIC\",\n",
    "    },\n",
    "    \"ttWH\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"isData\": False,\n",
    "        \"nEvents\": 200000,\n",
    "        \"nEventsPositive\": 199491,\n",
    "        \"nEventsNegative\": 509,\n",
    "        \"sumWeights\": 198839.680865,\n",
    "        \"sumWeights2\": 199704.039588,\n",
    "        \"isSignal\": False,\n",
    "        \"crossSection\": 0.001572,\n",
    "        \"color\": leg_dict[\"ttultrarare\"],\n",
    "        \"source\": {\"LJMLogic\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/ttWH_2017_v2.root\",\n",
    "                   \"LJMLogic/ElMu_selection\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/ElMu_selection/ttWH_2017_v2*.root\",\n",
    "                   \"LJMLogic/MuMu_selection\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/MuMu_selection/ttWH_2017_v2*.root\",\n",
    "                   \"LJMLogic/ElEl_selection\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/ElEl_selection/ttWH_2017_v2*.root\",\n",
    "                  },\n",
    "        \"sourceSPARK\": [\"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/ttWH_2017_v2.root\",],\n",
    "        \"destination\": \"/$HIST_CLASS/$HIST/ttWH/$SYSTEMATIC\",\n",
    "    },\n",
    "    \"ttWW\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"isData\": False,\n",
    "        \"nEvents\": 962000,\n",
    "        \"nEventsPositive\": 962000,\n",
    "        \"nEventsNegative\": 0,\n",
    "        \"sumWeights\": 962000.000000,\n",
    "        \"sumWeights2\": 962000.000000,\n",
    "        \"isSignal\": False,\n",
    "        \"crossSection\": 0.007882,\n",
    "        \"color\": leg_dict[\"ttultrarare\"],\n",
    "        \"source\": {\"LJMLogic\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/ttWW_2017_v2.root\",\n",
    "                   \"LJMLogic/ElMu_selection\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/ElMu_selection/ttWW_2017_v2*.root\",\n",
    "                   \"LJMLogic/MuMu_selection\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/MuMu_selection/ttWW_2017_v2*.root\",\n",
    "                   \"LJMLogic/ElEl_selection\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/ElEl_selection/ttWW_2017_v2*.root\",\n",
    "                  },\n",
    "        \"sourceSPARK\": [\"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/ttWW_2017_v2.root\",],\n",
    "        \"destination\": \"/$HIST_CLASS/$HIST/ttWW/$SYSTEMATIC\",\n",
    "    },\n",
    "    \"ttWZ\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"isData\": False,\n",
    "        \"nEvents\": 200000,\n",
    "        \"nEventsPositive\": 199379,\n",
    "        \"nEventsNegative\": 621,\n",
    "        \"sumWeights\": 198625.183551,\n",
    "        \"sumWeights2\": 199708.972601,\n",
    "        \"isSignal\": False,\n",
    "        \"crossSection\": 0.002974,\n",
    "        \"color\": leg_dict[\"ttultrarare\"],\n",
    "        \"source\": {\"LJMLogic\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/ttWZ_2017_v2.root\",\n",
    "                   \"LJMLogic/ElMu_selection\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/ElMu_selection/ttWZ_2017_v2*.root\",\n",
    "                   \"LJMLogic/MuMu_selection\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/MuMu_selection/ttWZ_2017_v2*.root\",\n",
    "                   \"LJMLogic/ElEl_selection\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/ElEl_selection/ttWZ_2017_v2*.root\",\n",
    "                  },\n",
    "        \"sourceSPARK\": [\"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/ttWZ_2017_v2.root\",],\n",
    "        \"destination\": \"/$HIST_CLASS/$HIST/ttWZ/$SYSTEMATIC\",\n",
    "    },\n",
    "    \"ttZZ\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"isData\": False,\n",
    "        \"nEvents\": 200000,\n",
    "        \"nEventsPositive\": 199686,\n",
    "        \"nEventsNegative\": 314,\n",
    "        \"sumWeights\": 199286.628891,\n",
    "        \"sumWeights2\": 199816.306332,\n",
    "        \"isSignal\": False,\n",
    "        \"crossSection\": 0.001572,\n",
    "        \"color\": leg_dict[\"ttultrarare\"],\n",
    "        \"source\": {\"LJMLogic\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/ttZZ_2017_v2.root\",\n",
    "                   \"LJMLogic/ElMu_selection\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/ElMu_selection/ttZZ_2017_v2*.root\",\n",
    "                   \"LJMLogic/MuMu_selection\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/MuMu_selection/ttZZ_2017_v2*.root\",\n",
    "                   \"LJMLogic/ElEl_selection\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/ElEl_selection/ttZZ_2017_v2*.root\",\n",
    "                  },\n",
    "        \"sourceSPARK\": [\"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/ttZZ_2017_v2.root\",],\n",
    "        \"destination\": \"/$HIST_CLASS/$HIST/ttZZ/$SYSTEMATIC\",\n",
    "    },\n",
    "    \"ttZH\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"isData\": False,\n",
    "        \"nEvents\": 200000,\n",
    "        \"nEventsPositive\": 199643,\n",
    "        \"nEventsNegative\": 357,\n",
    "        \"sumWeights\": 199192.234990,\n",
    "        \"sumWeights2\": 199794.753976,\n",
    "        \"isSignal\": False,\n",
    "        \"crossSection\": 0.01253,\n",
    "        \"color\": leg_dict[\"ttultrarare\"],\n",
    "        \"source\": {\"LJMLogic\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/ttZH_2017_v2.root\",\n",
    "                   \"LJMLogic/ElMu_selection\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/ElMu_selection/ttZH_2017_v2*.root\",\n",
    "                   \"LJMLogic/MuMu_selection\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/MuMu_selection/ttZH_2017_v2*.root\",\n",
    "                   \"LJMLogic/ElEl_selection\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/ElEl_selection/ttZH_2017_v2*.root\",\n",
    "                  },\n",
    "        \"sourceSPARK\": [\"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/ttZH_2017_v2.root\",],\n",
    "        \"destination\": \"/$HIST_CLASS/$HIST/ttZH/$SYSTEMATIC\",\n",
    "    },\n",
    "    \"ttHH\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"isData\": False,\n",
    "        \"nEvents\": 194817,\n",
    "        \"nEventsPositive\": 194516,\n",
    "        \"nEventsNegative\": 301,\n",
    "        \"sumWeights\": 194116.909912,\n",
    "        \"sumWeights2\": 194611.090542,\n",
    "        \"isSignal\": False,\n",
    "        \"crossSection\": 0.0007408,\n",
    "        \"color\": leg_dict[\"ttultrarare\"],\n",
    "        \"source\": {\"LJMLogic\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/ttHH_2017_v2.root\",\n",
    "                   \"LJMLogic/ElMu_selection\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/ElMu_selection/ttHH_2017_v2*.root\",\n",
    "                   \"LJMLogic/MuMu_selection\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/MuMu_selection/ttHH_2017_v2*.root\",\n",
    "                   \"LJMLogic/ElEl_selection\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/ElEl_selection/ttHH_2017_v2*.root\",\n",
    "                  },\n",
    "        \"sourceSPARK\": [\"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/ttHH_2017_v2.root\",],\n",
    "        \"destination\": \"/$HIST_CLASS/$HIST/ttHH/$SYSTEMATIC\",\n",
    "    },    \n",
    "    \"tttW\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"isData\": False,\n",
    "        \"nEvents\": 200000,\n",
    "        \"nEventsPositive\": 199852,\n",
    "        \"nEventsNegative\": 148,\n",
    "        \"sumWeights\": 199552.187377,\n",
    "        \"sumWeights2\": 199697.648421,\n",
    "        \"isSignal\": False,\n",
    "        \"crossSection\": 0.007882,\n",
    "        \"color\": leg_dict[\"ttultrarare\"],\n",
    "        \"source\": {\"LJMLogic\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/tttW_2017_v2.root\",\n",
    "                   \"LJMLogic/ElMu_selection\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/ElMu_selection/tttW_2017_v2*.root\",\n",
    "                   \"LJMLogic/MuMu_selection\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/MuMu_selection/tttW_2017_v2*.root\",\n",
    "                   \"LJMLogic/ElEl_selection\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/ElEl_selection/tttW_2017_v2*.root\",\n",
    "                  },\n",
    "        \"sourceSPARK\": [\"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/tttW_2017_v2.root\",],\n",
    "        \"destination\": \"/$HIST_CLASS/$HIST/tttW/$SYSTEMATIC\",\n",
    "    },\n",
    "    \"tttJ\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"isData\": False,\n",
    "        \"nEvents\": 200000,\n",
    "        \"nEventsPositive\": 199273,\n",
    "        \"nEventsNegative\": 727,\n",
    "        \"sumWeights\": 198394.878491,\n",
    "        \"sumWeights2\": 199663.384954,\n",
    "        \"isSignal\": False,\n",
    "        \"crossSection\": 0.0004741,\n",
    "        \"color\": leg_dict[\"ttultrarare\"],\n",
    "        \"source\": {\"LJMLogic\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/tttJ_2017_v2.root\",\n",
    "                   \"LJMLogic/ElMu_selection\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/ElMu_selection/tttJ_2017_v2*.root\",\n",
    "                   \"LJMLogic/MuMu_selection\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/MuMu_selection/tttJ_2017_v2*.root\",\n",
    "                   \"LJMLogic/ElEl_selection\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/ElEl_selection/tttJ_2017_v2*.root\",\n",
    "                  },\n",
    "        \"sourceSPARK\": [\"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/tttJ_2017_v2.root\",],\n",
    "        \"destination\": \"/$HIST_CLASS/$HIST/tttJ/$SYSTEMATIC\",\n",
    "    },\n",
    "}\n",
    "bookerV2_ElMu = {\n",
    "    \"ElMu\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"subera\": \"BCDEF\",\n",
    "        \"channel\": \"ElMu\",\n",
    "        \"isData\": True,\n",
    "        \"nEvents\": 4453465 + 15595214 + 9164365 + 19043421 + 25776363,\n",
    "        \"color\": leg_dict[\"Data\"],\n",
    "        \"source\": {\"LJMLogic\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/data_ElMu_*_2017_v2.root\",\n",
    "                   \"LJMLogic/ElMu_selection\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/ElMu_selection/data_ElMu_*_2017_v2*.root\",\n",
    "                   \"LJMLogic/MuMu_selection\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/MuMu_selection/data_ElMu_*_2017_v2*.root\",\n",
    "                   \"LJMLogic/ElEl_selection\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/ElEl_selection/data_ElMu_*_2017_v2*.root\",\n",
    "                  },\n",
    "        \"sourceSPARK\": [\"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/data_ElMu_B_2017_v2.root\",\n",
    "                        \"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/data_ElMu_C_2017_v2.root\",\n",
    "                        \"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/data_ElMu_D_2017_v2.root\",\n",
    "                        \"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/data_ElMu_E_2017_v2.root\",\n",
    "                        \"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/data_ElMu_F_2017_v2.root\",],\n",
    "        \"destination\": \"/$HIST_CLASS/$HIST/ElMu/NOMINAL\",\n",
    "    },\n",
    "}\n",
    "bookerV2_MuMu = {\n",
    "    \"MuMu\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"subera\": \"BCDEF\",\n",
    "        \"channel\": \"MuMu\",\n",
    "        \"isData\": True,\n",
    "        \"nEvents\": 14501767 + 49636525 + 23075733 + 51589091 + 79756560,\n",
    "        \"color\": leg_dict[\"Data\"],\n",
    "        \"source\": {\"LJMLogic\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/data_MuMu_*_2017_v2.root\",\n",
    "                   \"LJMLogic/ElMu_selection\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/ElMu_selection/data_MuMu_*_2017_v2*.root\",\n",
    "                   \"LJMLogic/MuMu_selection\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/MuMu_selection/data_MuMu_*_2017_v2*.root\",\n",
    "                   \"LJMLogic/ElEl_selection\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/ElEl_selection/data_MuMu_*_2017_v2*.root\",\n",
    "                  },\n",
    "        \"sourceSPARK\": [\"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/data_MuMu_B_2017_v2.root\",\n",
    "                        \"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/data_MuMu_C_2017_v2.root\",\n",
    "                        \"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/data_MuMu_D_2017_v2.root\",\n",
    "                        \"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/data_MuMu_E_2017_v2.root\",\n",
    "                        \"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/data_MuMu_F_2017_v2.root\",],\n",
    "        \"destination\": \"/$HIST_CLASS/$HIST/MuMu/NOMINAL\",\n",
    "        },\n",
    "}\n",
    "bookerV2_ElEl = {\n",
    "    \"ElEl\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"subera\": \"B\",\n",
    "        \"channel\": \"ElEl\",\n",
    "        \"isData\": True,\n",
    "        \"nEvents\": 58088760 + 65181125 + 25911432 + 56233597 + 74307066,\n",
    "        \"color\": leg_dict[\"Data\"],\n",
    "        \"source\": {\"LJMLogic\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/data_ElEl_*_2017_v2.root\",\n",
    "                   \"LJMLogic/ElMu_selection\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/ElMu_selection/data_ElEl_*_2017_v2*.root\",\n",
    "                   \"LJMLogic/MuMu_selection\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/MuMu_selection/data_ElEl_*_2017_v2*.root\",\n",
    "                   \"LJMLogic/ElEl_selection\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/ElEl_selection/data_ElEl_*_2017_v2*.root\",\n",
    "                  },\n",
    "        \"sourceSPARK\": [\"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/data_ElEl_B_2017_v2.root\",\n",
    "                        \"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/data_ElEl_C_2017_v2.root\",\n",
    "                        \"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/data_ElEl_D_2017_v2.root\",\n",
    "                        \"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/data_ElEl_E_2017_v2.root\",\n",
    "                        \"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/data_ElEl_F_2017_v2.root\",],\n",
    "        \"destination\": \"/$HIST_CLASS/$HIST/ElEl/NOMINAL\",\n",
    "        },\n",
    "}\n",
    "cutoutV2_ToBeFixed = {\n",
    "    \"Mu\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"subera\": \"BCDEF\",\n",
    "        \"channel\": \"Mu\",\n",
    "        \"isData\": True,\n",
    "        \"nEvents\": 136300266 + 165652756 + 70361660 + 154630534 + 242135500,\n",
    "        \"color\": leg_dict[\"Data\"],\n",
    "        \"source\": {\"LJMLogic\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/data_Mu_*_2017_v2.root\",\n",
    "                   \"LJMLogic/ElMu_selection\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/ElMu_selection/data_Mu_*_2017_v2*.root\",\n",
    "                   \"LJMLogic/MuMu_selection\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/MuMu_selection/data_Mu_*_2017_v2*.root\",\n",
    "                   \"LJMLogic/ElEl_selection\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/ElEl_selection/data_Mu_*_2017_v2*.root\",\n",
    "                  },\n",
    "        },\n",
    "    \"El\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"subera\": \"BCDEF\",\n",
    "        \"channel\": \"El\",\n",
    "        \"isData\": True,\n",
    "        \"nEvents\": 60537490 + 136637888 + 51526710 + 102121689 + 128467223,\n",
    "        \"color\": leg_dict[\"Data\"],\n",
    "        \"source\": {\"LJMLogic\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/data_El_*_2017_v2.root\",\n",
    "                   \"LJMLogic/ElMu_selection\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/ElMu_selection/data_El_*_2017_v2*.root\",\n",
    "                   \"LJMLogic/MuMu_selection\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/MuMu_selection/data_El_*_2017_v2*.root\",\n",
    "                   \"LJMLogic/ElEl_selection\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/ElEl_selection/data_El_*_2017_v2*.root\",\n",
    "                  },\n",
    "        },\n",
    "    \"QCD_HT200\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"isData\": False,\n",
    "        \"nEvents\": 59200263,\n",
    "        \"nEventsPositive\": 59166789,\n",
    "        \"nEventsNegative\": 32544,\n",
    "        \"sumWeights\": 59133315.000000,\n",
    "        \"sumWeights2\": 59200263.000000,\n",
    "        \"isSignal\": False,\n",
    "        \"crossSection\": 1712000.0,\n",
    "        \"color\": leg_dict[\"QCD\"],\n",
    "        \"source\": {\"LJMLogic\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/QCD_HT200_2017_v2.root\",\n",
    "                   \"LJMLogic/ElMu_selection\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/ElMu_selection/QCD_HT200_2017_v2*.root\",\n",
    "                   \"LJMLogic/MuMu_selection\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/MuMu_selection/QCD_HT200_2017_v2*.root\",\n",
    "                   \"LJMLogic/ElEl_selection\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/ElEl_selection/QCD_HT200_2017_v2*.root\",\n",
    "                  },\n",
    "        \"sourceSPARK\": [\"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/QCD_HT200_2017_v2.root\",],\n",
    "    },\n",
    "    \"QCD_HT300\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"isData\": False,\n",
    "        \"nEvents\": 59569132,\n",
    "        \"nEventsPositive\": 59514373,\n",
    "        \"nEventsNegative\": 54759,\n",
    "        \"sumWeights\": 59459614.000000,\n",
    "        \"sumWeights2\": 59569132.000000,\n",
    "        \"isSignal\": False,\n",
    "        \"crossSection\": 347700.0,\n",
    "        \"color\": leg_dict[\"QCD\"],\n",
    "        \"source\": {\"LJMLogic\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/QCD_HT300_2017_v2.root\",\n",
    "                   \"LJMLogic/ElMu_selection\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/ElMu_selection/QCD_HT300_2017_v2*.root\",\n",
    "                   \"LJMLogic/MuMu_selection\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/MuMu_selection/QCD_HT300_2017_v2*.root\",\n",
    "                   \"LJMLogic/ElEl_selection\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/ElEl_selection/QCD_HT300_2017_v2*.root\",\n",
    "                  },\n",
    "        \"sourceSPARK\": [\"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/QCD_HT300_2017_v2.root\",],\n",
    "    },   \n",
    "    \"QCD_HT500\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"isData\": False,\n",
    "        \"nEvents\": 56207744,\n",
    "        \"nEventsPositive\": 56124381,\n",
    "        \"nEventsNegative\": 83363,\n",
    "        \"sumWeights\": 56041018.000000,\n",
    "        \"sumWeights2\": 56207744.000000,\n",
    "        \"isSignal\": False,\n",
    "        \"crossSection\": 32100.0,\n",
    "        \"color\": leg_dict[\"QCD\"],\n",
    "        \"source\": {\"LJMLogic\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/QCD_HT500_2017_v2.root\",\n",
    "                   \"LJMLogic/ElMu_selection\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/ElMu_selection/QCD_HT500_2017_v2*.root\",\n",
    "                   \"LJMLogic/MuMu_selection\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/MuMu_selection/QCD_HT500_2017_v2*.root\",\n",
    "                   \"LJMLogic/ElEl_selection\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/ElEl_selection/QCD_HT500_2017_v2*.root\",\n",
    "                  },\n",
    "        \"sourceSPARK\": [\"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/QCD_HT500_2017_v2.root\",],\n",
    "    },\n",
    "    \"QCD_HT700\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"isData\": False,\n",
    "        \"nEvents\": 46840955,\n",
    "        \"nEventsPositive\": 46739970,\n",
    "        \"nEventsNegative\": 100985,\n",
    "        \"sumWeights\": 46638985.000000,\n",
    "        \"sumWeights2\": 46840955.000000,\n",
    "        \"isSignal\": False,\n",
    "        \"crossSection\": 6831.0,\n",
    "        \"color\": leg_dict[\"QCD\"],\n",
    "        \"source\": {\"LJMLogic\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/QCD_HT700_2017_v2.root\",\n",
    "                   \"LJMLogic/ElMu_selection\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/ElMu_selection/QCD_HT700_2017_v2*.root\",\n",
    "                   \"LJMLogic/MuMu_selection\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/MuMu_selection/QCD_HT700_2017_v2*.root\",\n",
    "                   \"LJMLogic/ElEl_selection\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/ElEl_selection/QCD_HT700_2017_v2*.root\",\n",
    "                  },\n",
    "        \"sourceSPARK\": [\"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/QCD_HT700_2017_v2.root\",],\n",
    "    },\n",
    "    \"QCD_HT1000\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"isData\": False,\n",
    "        \"nEvents\": 16882838,\n",
    "        \"nEventsPositive\": 16826800,\n",
    "        \"nEventsNegative\": 56038,\n",
    "        \"sumWeights\": 16770762.000000,\n",
    "        \"sumWeights2\": 16882838.000000,\n",
    "        \"isSignal\": False,\n",
    "        \"crossSection\": 1207.0,\n",
    "        \"color\": leg_dict[\"QCD\"],\n",
    "        \"source\": {\"LJMLogic\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/QCD_HT1000_2017_v2.root\",\n",
    "                   \"LJMLogic/ElMu_selection\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/ElMu_selection/QCD_HT1000_2017_v2*.root\",\n",
    "                   \"LJMLogic/MuMu_selection\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/MuMu_selection/QCD_HT1000_2017_v2*.root\",\n",
    "                   \"LJMLogic/ElEl_selection\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/ElEl_selection/QCD_HT1000_2017_v2*.root\",\n",
    "                  },\n",
    "        \"sourceSPARK\": [\"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/QCD_HT1000_2017_v2.root\",],\n",
    "    },\n",
    "    \"QCD_HT1500\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"isData\": False,\n",
    "        \"nEvents\": 11634434,\n",
    "        \"nEventsPositive\": 11571519,\n",
    "        \"nEventsNegative\": 62915,\n",
    "        \"sumWeights\": 11508604.000000,\n",
    "        \"sumWeights2\": 11634434.000000,\n",
    "        \"isSignal\": False,\n",
    "        \"crossSection\": 119.9,\n",
    "        \"color\": leg_dict[\"QCD\"],\n",
    "        \"source\": {\"LJMLogic\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/QCD_HT1500_2017_v2.root\",\n",
    "                   \"LJMLogic/ElMu_selection\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/ElMu_selection/QCD_HT1500_2017_v2*.root\",\n",
    "                   \"LJMLogic/MuMu_selection\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/MuMu_selection/QCD_HT1500_2017_v2*.root\",\n",
    "                   \"LJMLogic/ElEl_selection\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/ElEl_selection/QCD_HT1500_2017_v2*.root\",\n",
    "                  },\n",
    "        \"sourceSPARK\": [\"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/QCD_HT1500_2017_v2.root\",],\n",
    "    },\n",
    "    \"QCD_HT2000\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"isData\": False,\n",
    "        \"nEvents\": 5941306,\n",
    "        \"nEventsPositive\": 5883436,\n",
    "        \"nEventsNegative\": 57870,\n",
    "        \"sumWeights\": 5825566.000000,\n",
    "        \"sumWeights2\": 5941306.000000,\n",
    "        \"isSignal\": False,\n",
    "        \"crossSection\": 25.24,\n",
    "        \"color\": leg_dict[\"QCD\"],\n",
    "        \"source\": {\"LJMLogic\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/QCD_HT2000_2017_v2.root\",\n",
    "                   \"LJMLogic/ElMu_selection\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/ElMu_selection/QCD_HT2000_2017_v2*.root\",\n",
    "                   \"LJMLogic/MuMu_selection\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/MuMu_selection/QCD_HT2000_2017_v2*.root\",\n",
    "                   \"LJMLogic/ElEl_selection\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/ElEl_selection/QCD_HT2000_2017_v2*.root\",\n",
    "                  },\n",
    "        \"sourceSPARK\": [\"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/QCD_HT2000_2017_v2.root\",],\n",
    "    },\n",
    "    \"ElMu_B\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"subera\": \"B\",\n",
    "        \"channel\": \"ElMu\",\n",
    "        \"isData\": True,\n",
    "        \"nEvents\": 4453465,\n",
    "        \"color\": leg_dict[\"Data\"],\n",
    "        \"source\": {\"LJMLogic\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/data_ElMu_B*_2017_v2.root\",},\n",
    "        \"sourceSPARK\": [\"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/ElMu_B_2017.root\",],\n",
    "        },\n",
    "    \"ElMu_C\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"subera\": \"C\",\n",
    "        \"channel\": \"ElMu\",\n",
    "        \"isData\": True,\n",
    "        \"nEvents\": 15595214, \n",
    "        \"color\": leg_dict[\"Data\"],\n",
    "        \"source\": {\"LJMLogic\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/data_ElMu_C*_2017_v2.root\",},\n",
    "        \"sourceSPARK\": [\"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/ElMu_C_2017.root\",],\n",
    "        },\n",
    "    \"ElMu_D\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"subera\": \"D\",\n",
    "        \"channel\": \"ElMu\",\n",
    "        \"isData\": True,\n",
    "        \"nEvents\": 9164365,\n",
    "        \"color\": leg_dict[\"Data\"],\n",
    "        \"source\": {\"LJMLogic\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/data_ElMu_D*_2017_v2.root\",},\n",
    "        \"sourceSPARK\": [\"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/ElMu_D_2017.root\",],\n",
    "        },\n",
    "    \"ElMu_E\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"subera\": \"E\",\n",
    "        \"channel\": \"ElMu\",\n",
    "        \"isData\": True,\n",
    "        \"nEvents\": 19043421,\n",
    "        \"color\": leg_dict[\"Data\"],\n",
    "        \"source\": {\"LJMLogic\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/data_ElMu_E*_2017_v2.root\",},\n",
    "        \"sourceSPARK\": [\"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/ElMu_E_2017.root\",],\n",
    "        },\n",
    "    \"ElMu_F\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"subera\": \"F\",\n",
    "        \"channel\": \"ElMu\",\n",
    "        \"isData\": True,\n",
    "        \"nEvents\": 25776363,\n",
    "        \"color\": leg_dict[\"Data\"],\n",
    "        \"source\": {\"LJMLogic\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/data_ElMu_F*_2017_v2.root\",},\n",
    "        \"sourceSPARK\": [\"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/ElMu_F_2017.root\",],\n",
    "        },\n",
    "    \"MuMu_B\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"subera\": \"B\",\n",
    "        \"channel\": \"MuMu\",\n",
    "        \"isData\": True,\n",
    "        \"nEvents\": 14501767,\n",
    "        \"color\": leg_dict[\"Data\"],\n",
    "        \"source\": {\"LJMLogic\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/data_MuMu_B*_2017_v2.root\",},\n",
    "        },\n",
    "    \"MuMu_C\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"subera\": \"C\",\n",
    "        \"channel\": \"MuMu\",\n",
    "        \"isData\": True,\n",
    "        \"nEvents\": 49636525,\n",
    "        \"color\": leg_dict[\"Data\"],\n",
    "        \"source\": {\"LJMLogic\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/data_MuMu_C*_2017_v2.root\",},\n",
    "        },\n",
    "    \"MuMu_D\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"subera\": \"D\",\n",
    "        \"channel\": \"MuMu\",\n",
    "        \"isData\": True,\n",
    "        \"nEvents\": 23075733,\n",
    "        \"color\": leg_dict[\"Data\"],\n",
    "        \"source\": {\"LJMLogic\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/data_MuMu_D*_2017_v2.root\",},\n",
    "        },\n",
    "    \"MuMu_E\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"subera\": \"E\",\n",
    "        \"channel\": \"MuMu\",\n",
    "        \"isData\": True,\n",
    "        \"nEvents\": 51589091,\n",
    "        \"color\": leg_dict[\"Data\"],\n",
    "        \"source\": {\"LJMLogic\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/data_MuMu_E*_2017_v2.root\",},\n",
    "        },\n",
    "    \"MuMu_F\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"subera\": \"F\",\n",
    "        \"channel\": \"MuMu\",\n",
    "        \"isData\": True,\n",
    "        \"nEvents\": 79756560,\n",
    "        \"color\": leg_dict[\"Data\"],\n",
    "        \"source\": {\"LJMLogic\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/data_MuMu_F*_2017_v2.root\",},\n",
    "        },\n",
    "    \"ElEl_B\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"subera\": \"B\",\n",
    "        \"channel\": \"ElEl\",\n",
    "        \"isData\": True,\n",
    "        \"nEvents\": 58088760,\n",
    "        \"color\": leg_dict[\"Data\"],\n",
    "        \"source\": {\"LJMLogic\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/data_ElEl_B*_2017_v2.root\",},\n",
    "        },\n",
    "    \"ElEl_C\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"subera\": \"C\",\n",
    "        \"channel\": \"ElEl\",\n",
    "        \"isData\": True,\n",
    "        \"nEvents\": 65181125,\n",
    "        \"color\": leg_dict[\"Data\"],\n",
    "        \"source\": {\"LJMLogic\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/data_ElEl_C*_2017_v2.root\",},\n",
    "        },\n",
    "    \"ElEl_D\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"subera\": \"D\",\n",
    "        \"channel\": \"ElEl\",\n",
    "        \"isData\": True,\n",
    "        \"nEvents\": 25911432,\n",
    "        \"color\": leg_dict[\"Data\"],\n",
    "        \"source\": {\"LJMLogic\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/data_ElEl_D*_2017_v2.root\",},\n",
    "        },\n",
    "    \"ElEl_E\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"subera\": \"E\",\n",
    "        \"channel\": \"ElEl\",\n",
    "        \"isData\": True,\n",
    "        \"nEvents\": 56233597,\n",
    "        \"color\": leg_dict[\"Data\"],\n",
    "        \"source\": {\"LJMLogic\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/data_ElEl_E*_2017_v2.root\",},\n",
    "        },\n",
    "    \"ElEl_F\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"subera\": \"F\",\n",
    "        \"channel\": \"ElEl\",\n",
    "        \"isData\": True,\n",
    "        \"nEvents\": 74307066,\n",
    "        \"color\": leg_dict[\"Data\"],\n",
    "        \"source\": {\"LJMLogic\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/data_ElEl_F*_2017_v2.root\",},\n",
    "        },\n",
    "    \"Mu_B\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"subera\": \"B\",\n",
    "        \"channel\": \"Mu\",\n",
    "        \"isData\": True,\n",
    "        \"nEvents\": 136300266,\n",
    "        \"color\": leg_dict[\"Data\"],\n",
    "        \"source\": {\"LJMLogic\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/data_Mu_B*_2017_v2.root\",},\n",
    "        },\n",
    "    \"Mu_C\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"subera\": \"C\",\n",
    "        \"channel\": \"Mu\",\n",
    "        \"isData\": True,\n",
    "        \"nEvents\": 165652756,\n",
    "        \"color\": leg_dict[\"Data\"],\n",
    "        \"source\": {\"LJMLogic\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/data_Mu_C*_2017_v2.root\",},\n",
    "        },\n",
    "    \"Mu_D\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"subera\": \"D\",\n",
    "        \"channel\": \"Mu\",\n",
    "        \"isData\": True,\n",
    "        \"nEvents\": 70361660,\n",
    "        \"color\": leg_dict[\"Data\"],\n",
    "        \"source\": {\"LJMLogic\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/data_Mu_D*_2017_v2.root\",},\n",
    "        },\n",
    "    \"Mu_E\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"subera\": \"E\",\n",
    "        \"channel\": \"Mu\",\n",
    "        \"isData\": True,\n",
    "        \"nEvents\": 154630534,\n",
    "        \"color\": leg_dict[\"Data\"],\n",
    "        \"source\": {\"LJMLogic\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/data_Mu_E*_2017_v2.root\",},\n",
    "        },\n",
    "    \"Mu_F\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"subera\": \"F\",\n",
    "        \"channel\": \"Mu\",\n",
    "        \"isData\": True,\n",
    "        \"nEvents\": 242135500,\n",
    "        \"color\": leg_dict[\"Data\"],\n",
    "        \"source\": {\"LJMLogic\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/data_Mu_F*_2017_v2.root\",},\n",
    "        },\n",
    "    \"El_B\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"subera\": \"B\",\n",
    "        \"channel\": \"El\",\n",
    "        \"isData\": True,\n",
    "        \"nEvents\": 60537490,\n",
    "        \"color\": leg_dict[\"Data\"],\n",
    "        \"source\": {\"LJMLogic\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/data_El_B*_2017_v2.root\",},\n",
    "        },\n",
    "    \"El_C\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"subera\": \"C\",\n",
    "        \"channel\": \"El\",\n",
    "        \"isData\": True,\n",
    "        \"nEvents\": 136637888,\n",
    "        \"color\": leg_dict[\"Data\"],\n",
    "        \"source\": {\"LJMLogic\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/data_El_C*_2017_v2.root\",},\n",
    "        },\n",
    "    \"El_D\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"subera\": \"D\",\n",
    "        \"channel\": \"El\",\n",
    "        \"isData\": True,\n",
    "        \"nEvents\": 51526710,\n",
    "        \"color\": leg_dict[\"Data\"],\n",
    "        \"source\": {\"LJMLogic\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/data_El_D*_2017_v2.root\",},\n",
    "        },\n",
    "    \"El_E\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"subera\": \"E\",\n",
    "        \"channel\": \"El\",\n",
    "        \"isData\": True,\n",
    "        \"nEvents\": 102121689,\n",
    "        \"color\": leg_dict[\"Data\"],\n",
    "        \"source\": {\"LJMLogic\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/data_El_E*_2017_v2.root\",},\n",
    "        },\n",
    "    \"El_F\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"subera\": \"F\",\n",
    "        \"channel\": \"El\",\n",
    "        \"isData\": True,\n",
    "        \"nEvents\": 128467223,\n",
    "        \"color\": leg_dict[\"Data\"],\n",
    "        \"source\": {\"LJMLogic\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/data_El_F*_2017_v2.root\",},\n",
    "    },\n",
    "}\n",
    "bookerV2UNSTITCHED = {\n",
    "    \"tt_SL-UNSTITCHED\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"isData\": False,\n",
    "        \"nEvents\": 20122010,\n",
    "        \"nEventsPositive\": 20040607,\n",
    "        \"nEventsNegative\": 81403,\n",
    "        \"sumWeights\": 6052480345.748356,\n",
    "        \"sumWeights2\": 1850350248120.376221,\n",
    "        \"isSignal\": False,\n",
    "        \"doFilter\": True,\n",
    "        \"crossSection\": 366.2073,\n",
    "        \"color\": leg_dict[\"ttbar\"],\n",
    "        \"source\": {\"LJMLogic\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/tt_SL-NOM_2017_v2.root\",\n",
    "                   \"LJMLogic/ElMu_selection\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/ElMu_selection/tt_SL-NOM_2017_v2.root\",\n",
    "                   \"LJMLogic/MuMu_selection\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/MuMu_selection/tt_SL-NOM_2017_v2.root\",\n",
    "                   \"LJMLogic/ElEl_selection\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/ElEl_selection/tt_SL-NOM_2017_v2.root\",\n",
    "                  },\n",
    "        \"sourceSPARK\": [\"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/tt_SL-NOM_2017_v2.root\",],\n",
    "        \"destination\": \"/$HIST_CLASS/$HIST/tt_SL/$SYSTEMATIC\",\n",
    "    },  \n",
    "    \"tt_DL-UNSTITCHED\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"isData\": False,\n",
    "        \"nEvents\": 69098644,\n",
    "        \"nEventsPositive\": 68818780,\n",
    "        \"nEventsNegative\": 279864,\n",
    "        \"sumWeights\": 4980769113.241218,\n",
    "        \"sumWeights2\": 364913493679.955078,\n",
    "        \"isSignal\": False,\n",
    "        \"doFilter\": True,\n",
    "        \"crossSection\": 89.0482,\n",
    "        \"color\": leg_dict[\"ttbar\"],\n",
    "        \"source\": {\"LJMLogic\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/tt_DL-NOM-*_2017_v2.root\",\n",
    "                   \"LJMLogic/ElMu_selection\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/ElMu_selection/tt_DL-NOM-*_2017_v2.root\",\n",
    "                   \"LJMLogic/MuMu_selection\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/MuMu_selection/tt_DL-NOM-*_2017_v2.root\",\n",
    "                   \"LJMLogic/ElEl_selection\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/ElEl_selection/tt_DL-NOM-*_2017_v2.root\",\n",
    "                  },\n",
    "        \"sourceSPARK\": [\"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/tt_DL-NOM-1_2017_v2.root\",\n",
    "                       \"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/tt_DL-NOM-2_2017_v2.root\",\n",
    "                       \"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/tt_DL-NOM-3_2017_v2.root\",\n",
    "                       \"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/tt_DL-NOM-4_2017_v2.root\",\n",
    "                       \"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/tt_DL-NOM-5_2017_v2.root\",\n",
    "                       \"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/tt_DL-NOM-6_2017_v2.root\",\n",
    "                       \"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/tt_DL-NOM-7_2017_v2.root\",\n",
    "                       \"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/tt_DL-NOM-8_2017_v2.root\",\n",
    "                       \"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/tt_DL-NOM-9_2017_v2.root\",\n",
    "                       \"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/tt_DL-NOM-10_2017_v2.root\",\n",
    "                       \"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/tt_DL-NOM-11_2017_v2.root\",\n",
    "                       \"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/tt_DL-NOM-12_2017_v2.root\",\n",
    "                       \"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/tt_DL-NOM-13_2017_v2.root\",\n",
    "                       \"root://eoshome-n.cern.ch//eos/user/n/nmangane/SWAN_projects/LogicChainRDF/FilesV2/tt_DL-NOM-14_2017_v2.root\",],\n",
    "        \"destination\": \"/$HIST_CLASS/$HIST/tt_DL/$SYSTEMATIC\",\n",
    "    },\n",
    "}\n",
    "ttbooker = {\n",
    "    \"tt_DL\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"isData\": False,\n",
    "        \"nEvents\": 69098644,\n",
    "        \"nEventsPositive\": 68818780,\n",
    "        \"nEventsNegative\": 279864,\n",
    "        \"sumWeights\": 4980769113.241218,\n",
    "        \"sumWeights2\": 364913493679.955078,\n",
    "        \"isSignal\": False,\n",
    "        \"crossSection\": 89.0482,\n",
    "        \"color\": leg_dict[\"ttbar\"],\n",
    "        \"source\": {\"Unknown\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/tt_DL_2017_PUFix.root\",},\n",
    "        },\n",
    "}\n",
    "ttttbooker = {\n",
    "    \"tttt\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"isData\": False,\n",
    "        \"nEvents\": 2273928,\n",
    "        \"nEventsPositive\": 1561946,\n",
    "        \"nEventsNegative\": 711982,\n",
    "        \"sumWeights\": 18645.487772,\n",
    "        \"sumWeights2\": 1094.209551,\n",
    "        \"isSignal\": True,\n",
    "        \"crossSection\": 0.012,\n",
    "        \"color\": leg_dict[\"tttt\"],\n",
    "        \"source\": {\"Unknown\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/tttt_2017_PUFix.root\",},\n",
    "        },\n",
    "}\n",
    "microbooker = {\n",
    "    \"tttt\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"isData\": False,\n",
    "        \"nEvents\": 2273928,\n",
    "        \"nEventsPositive\": 1561946,\n",
    "        \"nEventsNegative\": 711982,\n",
    "        \"sumWeights\": 18645.487772,\n",
    "        \"sumWeights2\": 1094.209551,\n",
    "        \"isSignal\": True,\n",
    "        \"crossSection\": 0.012,\n",
    "        \"color\": leg_dict[\"tttt\"],\n",
    "        \"source\": {\"Unknown\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/tttt_2017_PUFix.root\",},\n",
    "        },\n",
    "    \"tt_DL\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"isData\": False,\n",
    "        \"nEvents\": 69098644,\n",
    "        \"nEventsPositive\": 68818780,\n",
    "        \"nEventsNegative\": 279864,\n",
    "        \"sumWeights\": 4980769113.241218,\n",
    "        \"sumWeights2\": 364913493679.955078,\n",
    "        \"isSignal\": False,\n",
    "        \"crossSection\": 89.0482,\n",
    "        \"color\": leg_dict[\"ttbar\"],\n",
    "        \"source\": {\"Unknown\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/tt_DL_2017_PUFix.root\",},\n",
    "        },\n",
    "    \"ST_tW\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"isData\": False,\n",
    "        \"nEvents\": 7945242,\n",
    "        \"nEventsPositive\": 7914815,\n",
    "        \"nEventsNegative\": 30427,\n",
    "        \"sumWeights\": 277241050.840222,\n",
    "        \"sumWeights2\": 9823995766.508368,\n",
    "        \"isSignal\": False,\n",
    "        \"crossSection\": 35.8,\n",
    "        \"color\": leg_dict[\"singletop\"],\n",
    "        \"source\": {\"Unknown\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/ST_tW_2017_PUFix.root\",},\n",
    "        },\n",
    "}\n",
    "theOriginal = {\n",
    "    \"tttt_orig\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"isData\": False,\n",
    "        \"nEvents\": 2273928,\n",
    "        \"nEventsPositive\": 1561946,\n",
    "        \"nEventsNegative\": 711982,\n",
    "        \"sumWeights\": 18645.487772,\n",
    "        \"sumWeights2\": 1094.209551,\n",
    "        \"isSignal\": False,\n",
    "        \"crossSection\": 0.012,\n",
    "        \"color\": leg_dict[\"ttbar\"],\n",
    "        \"source\": {\"Unknown\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/tttt-orig_2.root\",},\n",
    "        },\n",
    "}\n",
    "pyrdfbooker = {\n",
    "    \"tt_DL-GF\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"isData\": False,\n",
    "        \"nEvents\": 8510388,\n",
    "        \"nEventsPositive\": 8467543,\n",
    "        \"nEventsNegative\": 42845,\n",
    "        \"sumWeights\": 612101836.284397,\n",
    "        \"sumWeights2\": 44925503249.097206,\n",
    "        \"isSignal\": False,\n",
    "        \"crossSection\": 1.4705,\n",
    "        \"color\": leg_dict[\"ttbar\"],\n",
    "        \"source\": {\"Unknown\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/tt_DL-GF-*_2017_PUFix.root\",},\n",
    "        },\n",
    "}\n",
    "booker = {\n",
    "    \"tttt\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"isData\": False,\n",
    "        \"nEvents\": 2273928,\n",
    "        \"nEventsPositive\": 1561946,\n",
    "        \"nEventsNegative\": 711982,\n",
    "        \"sumWeights\": 18645.487772,\n",
    "        \"sumWeights2\": 1094.209551,\n",
    "        \"isSignal\": True,\n",
    "        \"crossSection\": 0.012,\n",
    "        \"color\": leg_dict[\"tttt\"],\n",
    "        \"source\": {\"Unknown\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/tttt_2017_PUFix.root\",},\n",
    "        },\n",
    "    \"tt_SL-GF\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"isData\": False,\n",
    "        \"nEvents\": 8836856,\n",
    "        \"nEventsPositive\": 8794464,\n",
    "        \"nEventsNegative\": 42392,\n",
    "        \"sumWeights\": 2653328498.476976,\n",
    "        \"sumWeights2\": 812201885978.209229,\n",
    "        \"isSignal\": False,\n",
    "        \"crossSection\": 6,\n",
    "        \"color\": leg_dict[\"ttbar\"],\n",
    "        \"source\": {\"Unknown\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/tt_SL-GF_2017_PUFix.root\",},\n",
    "        },\n",
    "    \"tt_DL-GF\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"isData\": False,\n",
    "        \"nEvents\": 8510388,\n",
    "        \"nEventsPositive\": 8467543,\n",
    "        \"nEventsNegative\": 42845,\n",
    "        \"sumWeights\": 612101836.284397,\n",
    "        \"sumWeights2\": 44925503249.097206,\n",
    "        \"isSignal\": False,\n",
    "        \"crossSection\": 1.4705,\n",
    "        \"color\": leg_dict[\"ttbar\"],\n",
    "        \"source\": {\"Unknown\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/tt_DL-GF-*_2017_PUFix.root\",},\n",
    "        },\n",
    "    \"tt_SL\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"isData\": False,\n",
    "        \"nEvents\": 20122010,\n",
    "        \"nEventsPositive\": 20040607,\n",
    "        \"nEventsNegative\": 81403,\n",
    "        \"sumWeights\": 6052480345.748356,\n",
    "        \"sumWeights2\": 1850350248120.376221,\n",
    "        \"isSignal\": False,\n",
    "        \"crossSection\": 366.2073,\n",
    "        \"color\": leg_dict[\"ttbar\"],\n",
    "        \"source\": {\"Unknown\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/tt_SL_2017_PUFix.root\",},\n",
    "        },\n",
    "    \"tt_DL\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"isData\": False,\n",
    "        \"nEvents\": 69098644,\n",
    "        \"nEventsPositive\": 68818780,\n",
    "        \"nEventsNegative\": 279864,\n",
    "        \"sumWeights\": 4980769113.241218,\n",
    "        \"sumWeights2\": 364913493679.955078,\n",
    "        \"isSignal\": False,\n",
    "        \"crossSection\": 89.0482,\n",
    "        \"color\": leg_dict[\"ttbar\"],\n",
    "        \"source\": {\"Unknown\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/tt_DL_2017_PUFix.root\",},\n",
    "        },\n",
    "    \"ST_tW\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"isData\": False,\n",
    "        \"nEvents\": 7945242,\n",
    "        \"nEventsPositive\": 7914815,\n",
    "        \"nEventsNegative\": 30427,\n",
    "        \"sumWeights\": 277241050.840222,\n",
    "        \"sumWeights2\": 9823995766.508368,\n",
    "        \"isSignal\": False,\n",
    "        \"crossSection\": 35.8,\n",
    "        \"color\": leg_dict[\"singletop\"],\n",
    "        \"source\": {\"Unknown\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/ST_tW_2017_PUFix.root\",},\n",
    "        },\n",
    "    \"ST_tbarW\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"isData\": False,\n",
    "        \"nEvents\": 7745276,\n",
    "        \"nEventsPositive\": 7715654,\n",
    "        \"nEventsNegative\": 30427,\n",
    "        \"sumWeights\": 270762750.172525,\n",
    "        \"sumWeights2\": 9611964941.797768,\n",
    "        \"isSignal\": False,\n",
    "        \"crossSection\": 35.8,\n",
    "        \"color\": leg_dict[\"singletop\"],\n",
    "        \"source\": {\"Unknown\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/ST_tbarW_2017_PUFix.root\",},\n",
    "        },\n",
    "    \"ttH\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"isData\": False,\n",
    "        \"nEvents\": 8000000,\n",
    "        \"nEventsPositive\": 7916867,\n",
    "        \"nEventsNegative\": 83133,\n",
    "        \"sumWeights\": 4216319.315884,\n",
    "        \"sumWeights2\": 2317497.816608,\n",
    "        \"isSignal\": False,\n",
    "        \"crossSection\": 0.2934,\n",
    "        \"color\": leg_dict[\"ttH\"],\n",
    "        \"source\": {\"Unknown\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/ttH_2017_PUFix.root\",},\n",
    "        },\n",
    "    \"ttWJets\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"isData\": False,\n",
    "        \"nEvents\": 9425384,\n",
    "        \"nEventsPositive\": 9404856,\n",
    "        \"nEventsNegative\": 20528,\n",
    "        \"sumWeights\": 9384328.000000,\n",
    "        \"sumWeights2\": 9425384.000000,\n",
    "        \"isSignal\": False,\n",
    "        \"crossSection\": 0.611,\n",
    "        \"color\": leg_dict[\"ttVJets\"],\n",
    "        \"source\": {\"Unknown\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/ttWJets_2017_PUFix.root\",},\n",
    "        },\n",
    "    \"ttZJets\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"isData\": False,\n",
    "        \"nEvents\": 8536618,\n",
    "        \"nEventsPositive\": 8527846,\n",
    "        \"nEventsNegative\": 8772,\n",
    "        \"sumWeights\": 8519074.000000,\n",
    "        \"sumWeights2\": 8536618.000000,\n",
    "        \"isSignal\": False,\n",
    "        \"crossSection\": 0.783,\n",
    "        \"color\": leg_dict[\"ttVJets\"],\n",
    "        \"source\": {\"Unknown\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/ttZJets_2017_PUFix.root\",},\n",
    "        },\n",
    "    \"ttWH\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"isData\": False,\n",
    "        \"nEvents\": 200000,\n",
    "        \"nEventsPositive\": 199491,\n",
    "        \"nEventsNegative\": 509,\n",
    "        \"sumWeights\": 198839.680865,\n",
    "        \"sumWeights2\": 199704.039588,\n",
    "        \"isSignal\": False,\n",
    "        \"crossSection\": 0.001572,\n",
    "        \"color\": leg_dict[\"ttultrarare\"],\n",
    "        \"source\": {\"Unknown\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/ttWH_2017_PUFix.root\",},\n",
    "        },\n",
    "    \"ttWW\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"isData\": False,\n",
    "        \"nEvents\": 962000,\n",
    "        \"nEventsPositive\": 962000,\n",
    "        \"nEventsNegative\": 0,\n",
    "        \"sumWeights\": 962000.000000,\n",
    "        \"sumWeights2\": 962000.000000,\n",
    "        \"isSignal\": False,\n",
    "        \"crossSection\": 0.007882,\n",
    "        \"color\": leg_dict[\"ttultrarare\"],\n",
    "        \"source\": {\"Unknown\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/ttWW_2017_PUFix.root\",},\n",
    "        },\n",
    "    \"ttWZ\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"isData\": False,\n",
    "        \"nEvents\": 200000,\n",
    "        \"nEventsPositive\": 199379,\n",
    "        \"nEventsNegative\": 621,\n",
    "        \"sumWeights\": 198625.183551,\n",
    "        \"sumWeights2\": 199708.972601,\n",
    "        \"isSignal\": False,\n",
    "        \"crossSection\": 0.002974,\n",
    "        \"color\": leg_dict[\"ttultrarare\"],\n",
    "        \"source\": {\"Unknown\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/ttWZ_2017_PUFix.root\",},\n",
    "        },\n",
    "    \"ttZZ\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"isData\": False,\n",
    "        \"nEvents\": 200000,\n",
    "        \"nEventsPositive\": 199686,\n",
    "        \"nEventsNegative\": 314,\n",
    "        \"sumWeights\": 199286.628891,\n",
    "        \"sumWeights2\": 199816.306332,\n",
    "        \"isSignal\": False,\n",
    "        \"crossSection\": 0.001572,\n",
    "        \"color\": leg_dict[\"ttultrarare\"],\n",
    "        \"source\": {\"Unknown\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/ttZZ_2017_PUFix.root\",},\n",
    "        },\n",
    "    \"ttZH\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"isData\": False,\n",
    "        \"nEvents\": 200000,\n",
    "        \"nEventsPositive\": 199643,\n",
    "        \"nEventsNegative\": 357,\n",
    "        \"sumWeights\": 199192.234990,\n",
    "        \"sumWeights2\": 199794.753976,\n",
    "        \"isSignal\": False,\n",
    "        \"crossSection\": 0.01253,\n",
    "        \"color\": leg_dict[\"ttultrarare\"],\n",
    "        \"source\": {\"Unknown\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/ttZH_2017_PUFix.root\",},\n",
    "        },\n",
    "    \"ttHH\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"isData\": False,\n",
    "        \"nEvents\": 194817,\n",
    "        \"nEventsPositive\": 194516,\n",
    "        \"nEventsNegative\": 301,\n",
    "        \"sumWeights\": 194116.909912,\n",
    "        \"sumWeights2\": 194611.090542,\n",
    "        \"isSignal\": False,\n",
    "        \"crossSection\": 0.0007408,\n",
    "        \"color\": leg_dict[\"ttultrarare\"],\n",
    "        \"source\": {\"Unknown\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/ttHH_2017_PUFix.root\",},\n",
    "        },\n",
    "    \"tttW\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"isData\": False,\n",
    "        \"nEvents\": 200000,\n",
    "        \"nEventsPositive\": 199852,\n",
    "        \"nEventsNegative\": 148,\n",
    "        \"sumWeights\": 199552.187377,\n",
    "        \"sumWeights2\": 199697.648421,\n",
    "        \"isSignal\": False,\n",
    "        \"crossSection\": 0.007882,\n",
    "        \"color\": leg_dict[\"ttultrarare\"],\n",
    "        \"source\": {\"Unknown\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/tttW_2017_PUFix.root\",},\n",
    "        },\n",
    "    \"tttJ\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"isData\": False,\n",
    "        \"nEvents\": 200000,\n",
    "        \"nEventsPositive\": 199273,\n",
    "        \"nEventsNegative\": 727,\n",
    "        \"sumWeights\": 198394.878491,\n",
    "        \"sumWeights2\": 199663.384954,\n",
    "        \"isSignal\": False,\n",
    "        \"crossSection\": 0.0004741,\n",
    "        \"color\": leg_dict[\"ttultrarare\"],\n",
    "        \"source\": {\"Unknown\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/tttJ_2017_PUFix.root\",},\n",
    "        },\n",
    "    \"DYJets_DL\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"isData\": False,\n",
    "        \"nEvents\": 49125561,\n",
    "        \"nEventsPositive\": 49103859,\n",
    "        \"nEventsNegative\": 21702,\n",
    "        \"sumWeights\": 49082157.000000,\n",
    "        \"sumWeights2\": 49125561.000000,\n",
    "        \"isSignal\": False,\n",
    "        \"crossSection\": 5075.6,\n",
    "        \"color\": leg_dict[\"DY\"],\n",
    "        \"source\": {\"Unknown\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/DYJets_DL_2017_PUFix.root\",},\n",
    "        },\n",
    "    \"ElMu_B\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"subera\": \"B\",\n",
    "        \"channel\": \"ElMu\",\n",
    "        \"isData\": True,\n",
    "        \"nEvents\": 4453465,\n",
    "        \"color\": leg_dict[\"Data\"],\n",
    "        \"source\": {\"Unknown\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/ElMu_B_2017.root\",},\n",
    "        },\n",
    "    \"ElMu_C\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"subera\": \"C\",\n",
    "        \"channel\": \"ElMu\",\n",
    "        \"isData\": True,\n",
    "        \"nEvents\": 15595214,\n",
    "        \"color\": leg_dict[\"Data\"],\n",
    "        \"source\": {\"Unknown\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/ElMu_C_2017.root\",},\n",
    "        },\n",
    "    \"ElMu_D\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"subera\": \"D\",\n",
    "        \"channel\": \"ElMu\",\n",
    "        \"isData\": True,\n",
    "        \"nEvents\": 9164365,\n",
    "        \"color\": leg_dict[\"Data\"],\n",
    "        \"source\": {\"Unknown\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/ElMu_D_2017.root\",},\n",
    "        },\n",
    "    \"ElMu_E\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"subera\": \"E\",\n",
    "        \"channel\": \"ElMu\",\n",
    "        \"isData\": True,\n",
    "        \"nEvents\": 19043421,\n",
    "        \"color\": leg_dict[\"Data\"],\n",
    "        \"source\": {\"Unknown\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/ElMu_E_2017.root\",},\n",
    "        },\n",
    "    \"ElMu_F\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"subera\": \"F\",\n",
    "        \"channel\": \"ElMu\",\n",
    "        \"isData\": True,\n",
    "        \"nEvents\": 25776363,\n",
    "        \"color\": leg_dict[\"Data\"],\n",
    "        \"source\": {\"Unknown\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/ElMu_F_2017.root\",},\n",
    "        },\n",
    "}\n",
    "cutout = {\n",
    "    \"MuMu_B\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"subera\": \"B\",\n",
    "        \"channel\": \"MuMu\",\n",
    "        \"isData\": True,\n",
    "        \"nEvents\": 14501767,\n",
    "        \"color\": leg_dict[\"Data\"],\n",
    "        \"source\": {\"Unknown\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/MuMu_B_2017.root\",},\n",
    "        },\n",
    "    \"MuMu_C\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"subera\": \"C\",\n",
    "        \"channel\": \"MuMu\",\n",
    "        \"isData\": True,\n",
    "        \"nEvents\": 49636525,\n",
    "        \"color\": leg_dict[\"Data\"],\n",
    "        \"source\": {\"Unknown\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/MuMu_C_2017.root\",},\n",
    "        },\n",
    "    \"MuMu_D\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"subera\": \"D\",\n",
    "        \"channel\": \"MuMu\",\n",
    "        \"isData\": True,\n",
    "        \"nEvents\": 23075733,\n",
    "        \"color\": leg_dict[\"Data\"],\n",
    "        \"source\": {\"Unknown\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/MuMu_D_2017.root\",},\n",
    "        },\n",
    "    \"MuMu_E\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"subera\": \"E\",\n",
    "        \"channel\": \"MuMu\",\n",
    "        \"isData\": True,\n",
    "        \"nEvents\": 51589091,\n",
    "        \"color\": leg_dict[\"Data\"],\n",
    "        \"source\": {\"Unknown\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/MuMu_E_2017.root\",},\n",
    "        },\n",
    "    \"MuMu_F\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"subera\": \"F\",\n",
    "        \"channel\": \"MuMu\",\n",
    "        \"isData\": True,\n",
    "        \"nEvents\": 79756560,\n",
    "        \"color\": leg_dict[\"Data\"],\n",
    "        \"source\": {\"Unknown\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/MuMu_F_2017.root\",},\n",
    "        },\n",
    "    \"ElEl_B\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"subera\": \"B\",\n",
    "        \"channel\": \"ElEl\",\n",
    "        \"isData\": True,\n",
    "        \"nEvents\": 58088760,\n",
    "        \"color\": leg_dict[\"Data\"],\n",
    "        \"source\": {\"Unknown\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/ElEl_B_2017.root\",},\n",
    "        },\n",
    "    \"ElEl_C\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"subera\": \"C\",\n",
    "        \"channel\": \"ElEl\",\n",
    "        \"isData\": True,\n",
    "        \"nEvents\": 65181125,\n",
    "        \"color\": leg_dict[\"Data\"],\n",
    "        \"source\": {\"Unknown\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/ElEl_C_2017.root\",},\n",
    "        },\n",
    "    \"ElEl_D\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"subera\": \"D\",\n",
    "        \"channel\": \"ElEl\",\n",
    "        \"isData\": True,\n",
    "        \"nEvents\": 25911432,\n",
    "        \"color\": leg_dict[\"Data\"],\n",
    "        \"source\": {\"Unknown\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/ElEl_D_2017.root\",},\n",
    "        },\n",
    "    \"ElEl_E\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"subera\": \"E\",\n",
    "        \"channel\": \"ElEl\",\n",
    "        \"isData\": True,\n",
    "        \"nEvents\": 56233597,\n",
    "        \"color\": leg_dict[\"Data\"],\n",
    "        \"source\": {\"Unknown\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/ElEl_E_2017.root\",},\n",
    "        },\n",
    "    \"ElEl_F\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"subera\": \"F\",\n",
    "        \"channel\": \"ElEl\",\n",
    "        \"isData\": True,\n",
    "        \"nEvents\": 74307066,\n",
    "        \"color\": leg_dict[\"Data\"],\n",
    "        \"source\": {\"Unknown\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/ElEl_F_2017.root\",},\n",
    "        },\n",
    "    \"Mu_B\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"subera\": \"B\",\n",
    "        \"channel\": \"Mu\",\n",
    "        \"isData\": True,\n",
    "        \"nEvents\": 136300266,\n",
    "        \"color\": leg_dict[\"Data\"],\n",
    "        \"source\": {\"Unknown\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/Mu_B_2017.root\",},\n",
    "        },\n",
    "    \"Mu_C\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"subera\": \"C\",\n",
    "        \"channel\": \"Mu\",\n",
    "        \"isData\": True,\n",
    "        \"nEvents\": 165652756,\n",
    "        \"color\": leg_dict[\"Data\"],\n",
    "        \"source\": {\"Unknown\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/Mu_C_2017.root\",},\n",
    "        },\n",
    "    \"Mu_D\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"subera\": \"D\",\n",
    "        \"channel\": \"Mu\",\n",
    "        \"isData\": True,\n",
    "        \"nEvents\": 70361660,\n",
    "        \"color\": leg_dict[\"Data\"],\n",
    "        \"source\": {\"Unknown\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/Mu_D_2017.root\",},\n",
    "        },\n",
    "    \"Mu_E\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"subera\": \"E\",\n",
    "        \"channel\": \"Mu\",\n",
    "        \"isData\": True,\n",
    "        \"nEvents\": 154630534,\n",
    "        \"color\": leg_dict[\"Data\"],\n",
    "        \"source\": {\"Unknown\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/Mu_E_2017.root\",},\n",
    "        },\n",
    "    \"Mu_F\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"subera\": \"F\",\n",
    "        \"channel\": \"Mu\",\n",
    "        \"isData\": True,\n",
    "        \"nEvents\": 242135500,\n",
    "        \"color\": leg_dict[\"Data\"],\n",
    "        \"source\": {\"Unknown\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/Mu_F_2017.root\",},\n",
    "        },\n",
    "    \"El_B\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"subera\": \"B\",\n",
    "        \"channel\": \"El\",\n",
    "        \"isData\": True,\n",
    "        \"nEvents\": 60537490,\n",
    "        \"color\": leg_dict[\"Data\"],\n",
    "        \"source\": {\"Unknown\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/El_B_2017.root\",},\n",
    "        },\n",
    "    \"El_C\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"subera\": \"C\",\n",
    "        \"channel\": \"El\",\n",
    "        \"isData\": True,\n",
    "        \"nEvents\": 136637888,\n",
    "        \"color\": leg_dict[\"Data\"],\n",
    "        \"source\": {\"Unknown\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/El_C_2017.root\",},\n",
    "        },\n",
    "    \"El_D\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"subera\": \"D\",\n",
    "        \"channel\": \"El\",\n",
    "        \"isData\": True,\n",
    "        \"nEvents\": 51526710,\n",
    "        \"color\": leg_dict[\"Data\"],\n",
    "        \"source\": {\"Unknown\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/El_D_2017.root\",},\n",
    "        },\n",
    "    \"El_E\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"subera\": \"E\",\n",
    "        \"channel\": \"El\",\n",
    "        \"isData\": True,\n",
    "        \"nEvents\": 102121689,\n",
    "        \"color\": leg_dict[\"Data\"],\n",
    "        \"source\": {\"Unknown\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/El_E_2017.root\",},\n",
    "        },\n",
    "    \"El_F\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"subera\": \"F\",\n",
    "        \"channel\": \"El\",\n",
    "        \"isData\": True,\n",
    "        \"nEvents\": 128467223,\n",
    "        \"color\": leg_dict[\"Data\"],\n",
    "        \"source\": {\"Unknown\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/El_F_2017.root\",},\n",
    "        },\n",
    "    }\n",
    "minibooker = {\n",
    "    \"tttt\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"isData\": False,\n",
    "        \"nEvents\": 2273928,\n",
    "        \"nEventsPositive\": 1561946,\n",
    "        \"nEventsNegative\": 711982,\n",
    "        \"sumWeights\": 18645.487772,\n",
    "        \"sumWeights2\": 1094.209551,\n",
    "        \"isSignal\": True,\n",
    "        \"crossSection\": 0.012,\n",
    "        \"color\": leg_dict[\"tttt\"],\n",
    "        \"source\": {\"Unknown\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/tttt_2017_PUFix.root\",},\n",
    "        },\n",
    "    \"tt_SL-GF\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"isData\": False,\n",
    "        \"nEvents\": 8836856,\n",
    "        \"nEventsPositive\": 8794464,\n",
    "        \"nEventsNegative\": 42392,\n",
    "        \"sumWeights\": 2653328498.476976,\n",
    "        \"sumWeights2\": 812201885978.209229,\n",
    "        \"isSignal\": False,\n",
    "        \"crossSection\": 6,\n",
    "        \"color\": leg_dict[\"ttbar\"],\n",
    "        \"source\": {\"Unknown\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/tt_SL-GF_2017_PUFix.root\",},\n",
    "        },\n",
    "    \"tt_DL-GF\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"isData\": False,\n",
    "        \"nEvents\": 8510388,\n",
    "        \"nEventsPositive\": 8467543,\n",
    "        \"nEventsNegative\": 42845,\n",
    "        \"sumWeights\": 612101836.284397,\n",
    "        \"sumWeights2\": 44925503249.097206,\n",
    "        \"isSignal\": False,\n",
    "        \"crossSection\": 1.4705,\n",
    "        \"color\": leg_dict[\"ttbar\"],\n",
    "        \"source\": {\"Unknown\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/tt_DL-GF-*_2017_PUFix.root\",},\n",
    "        },\n",
    "    \"tt_SL\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"isData\": False,\n",
    "        \"nEvents\": 20122010,\n",
    "        \"nEventsPositive\": 20040607,\n",
    "        \"nEventsNegative\": 81403,\n",
    "        \"sumWeights\": 6052480345.748356,\n",
    "        \"sumWeights2\": 1850350248120.376221,\n",
    "        \"isSignal\": False,\n",
    "        \"crossSection\": 366.2073,\n",
    "        \"color\": leg_dict[\"ttbar\"],\n",
    "        \"source\": {\"Unknown\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/tt_SL_2017_PUFix.root\",},\n",
    "        },\n",
    "    \"tt_DL\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"isData\": False,\n",
    "        \"nEvents\": 69098644,\n",
    "        \"nEventsPositive\": 68818780,\n",
    "        \"nEventsNegative\": 279864,\n",
    "        \"sumWeights\": 4980769113.241218,\n",
    "        \"sumWeights2\": 364913493679.955078,\n",
    "        \"isSignal\": False,\n",
    "        \"crossSection\": 89.0482,\n",
    "        \"color\": leg_dict[\"ttbar\"],\n",
    "        \"source\": {\"Unknown\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/tt_DL_2017_PUFix.root\",},\n",
    "        },\n",
    "    \"ST_tW\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"isData\": False,\n",
    "        \"nEvents\": 7945242,\n",
    "        \"nEventsPositive\": 7914815,\n",
    "        \"nEventsNegative\": 30427,\n",
    "        \"sumWeights\": 277241050.840222,\n",
    "        \"sumWeights2\": 9823995766.508368,\n",
    "        \"isSignal\": False,\n",
    "        \"crossSection\": 35.8,\n",
    "        \"color\": leg_dict[\"singletop\"],\n",
    "        \"source\": {\"Unknown\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/ST_tW_2017_PUFix.root\",},\n",
    "        },\n",
    "    \"ST_tbarW\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"isData\": False,\n",
    "        \"nEvents\": 7745276,\n",
    "        \"nEventsPositive\": 7715654,\n",
    "        \"nEventsNegative\": 30427,\n",
    "        \"sumWeights\": 270762750.172525,\n",
    "        \"sumWeights2\": 9611964941.797768,\n",
    "        \"isSignal\": False,\n",
    "        \"crossSection\": 35.8,\n",
    "        \"color\": leg_dict[\"singletop\"],\n",
    "        \"source\": {\"Unknown\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/ST_tbarW_2017_PUFix.root\",},\n",
    "        },\n",
    "\n",
    "    \"DYJets_DL\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"isData\": False,\n",
    "        \"nEvents\": 49125561,\n",
    "        \"nEventsPositive\": 49103859,\n",
    "        \"nEventsNegative\": 21702,\n",
    "        \"sumWeights\": 49082157.000000,\n",
    "        \"sumWeights2\": 49125561.000000,\n",
    "        \"isSignal\": False,\n",
    "        \"crossSection\": 5075.6,\n",
    "        \"color\": leg_dict[\"DY\"],\n",
    "        \"source\": {\"Unknown\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/DYJets_DL_2017_PUFix.root\",},\n",
    "        },\n",
    "    \"MuMu\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"subera\": \"B\",\n",
    "        \"channel\": \"MuMu\",\n",
    "        \"isData\": True,\n",
    "        \"nEvents\": 14501767 + 49636525 + 23075733 + 51589091 + 79756560,\n",
    "        \"color\": leg_dict[\"Data\"],\n",
    "        \"source\": {\"Unknown\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/MuMu_*_2017.root\",},\n",
    "        },\n",
    "    \"ElEl\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"subera\": \"B\",\n",
    "        \"channel\": \"ElEl\",\n",
    "        \"isData\": True,\n",
    "        \"nEvents\": 58088760 + 65181125 + 25911432 + 56233597 + 74307066,\n",
    "        \"color\": leg_dict[\"Data\"],\n",
    "        \"source\": {\"Unknown\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/ElEl_*_2017.root\",},\n",
    "        },\n",
    "    \"ElMu\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"subera\": \"B\",\n",
    "        \"channel\": \"ElMu\",\n",
    "        \"isData\": True,\n",
    "        \"nEvents\": 4453465 + 15595214 + 9164365 + 19043421 + 25776363,\n",
    "        \"color\": leg_dict[\"Data\"],\n",
    "        \"source\": {\"Unknown\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/ElMu_*_2017.root\",},\n",
    "        },\n",
    "    \"Mu\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"subera\": \"B\",\n",
    "        \"channel\": \"Mu\",\n",
    "        \"isData\": True,\n",
    "        \"nEvents\": 136300266 + 165652756 + 70361660 + 154630534 + 242135500,\n",
    "        \"color\": leg_dict[\"Data\"],\n",
    "        \"source\": {\"Unknown\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/Mu_*_2017.root\",},\n",
    "        },\n",
    "    \"El\":{\n",
    "        \"era\": \"2017\",\n",
    "        \"subera\": \"B\",\n",
    "        \"channel\": \"El\",\n",
    "        \"isData\": True,\n",
    "        \"nEvents\": 60537490 + 136637888 + 51526710 + 102121689 + 128467223,\n",
    "        \"color\": leg_dict[\"Data\"],\n",
    "        \"source\": {\"Unknown\": \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/El_*_2017.root\",},\n",
    "        },\n",
    "    }\n",
    "\n",
    "#Set up channel bits for selection and baseline. Separation not necessary in this stage, but convenient for loops\n",
    "Chan = {}\n",
    "Chan[\"ElMu_selection\"] = 24576\n",
    "Chan[\"MuMu_selection\"] = 6144\n",
    "Chan[\"ElEl_selection\"] = 512\n",
    "Chan[\"Mu_selection\"] = 128\n",
    "Chan[\"El_selection\"] = 64\n",
    "Chan[\"selection\"] = Chan[\"ElMu_selection\"] + Chan[\"MuMu_selection\"] + Chan[\"ElEl_selection\"] + Chan[\"Mu_selection\"] + Chan[\"El_selection\"]\n",
    "Chan[\"ElMu_baseline\"] = 24576\n",
    "Chan[\"MuMu_baseline\"] = 6144\n",
    "Chan[\"ElEl_baseline\"] = 512\n",
    "Chan[\"Mu_baseline\"] = 128\n",
    "Chan[\"El_baseline\"] = 64\n",
    "Chan[\"baseline\"] = Chan[\"ElMu_baseline\"] + Chan[\"MuMu_baseline\"] + Chan[\"ElEl_baseline\"] + Chan[\"Mu_baseline\"] + Chan[\"El_baseline\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def METXYCorr(input_df, run_branch = \"run\", era = \"2017\", isData = True, npv_branch = \"PV_npvs\",\n",
    "               sysVariations={\"$NOMINAL\": {\"jet_mask\": \"jet_mask\", \n",
    "                                             \"wgt_prebTag\": \"wgt_SUMW_PU_LSF_L1PF\",\n",
    "                                             \"jet_pt_var\": \"Jet_pt\",\n",
    "                                             \"jet_mass_var\": \"Jet_mass\",\n",
    "                                             \"met_pt_var\": \"METFixEE2017_pt\",\n",
    "                                             \"met_phi_var\": \"METFixEE2017_phi\",\n",
    "                                             \"btagSF\": \"Jet_btagSF_deepcsv_shape\",\n",
    "                                             \"weightVariation\": False},\n",
    "                              \"_jesTotalUp\": {\"jet_mask\": \"jet_mask_jesTotalUp\", \n",
    "                                             \"wgt_prebTag\": \"wgt_SUMW_PU_LSF_L1PF\",\n",
    "                                             \"jet_pt_var\": \"Jet_pt_jesTotalUp\",\n",
    "                                             \"jet_mass_var\": \"Jet_mass_jesTotalUp\",\n",
    "                                             \"met_pt_var\": \"METFixEE2017_pt_jesTotalUp\",\n",
    "                                             \"met_phi_var\": \"METFixEE2017_phi_jesTotalUp\",\n",
    "                                             \"btagSF\": \"Jet_btagSF_deepcsv_shape\",\n",
    "                                             \"weightVariation\": False},\n",
    "                              \"_jesTotalDown\": {\"jet_mask\": \"jet_mask_jesTotalDown\", \n",
    "                                             \"wgt_prebTag\": \"wgt_SUMW_PU_LSF_L1PF\",\n",
    "                                             \"jet_pt_var\": \"Jet_pt_jesTotalDown\",\n",
    "                                             \"jet_mass_var\": \"Jet_mass_jesTotalDown\",\n",
    "                                             \"met_pt_var\": \"METFixEE2017_pt_jesTotalDown\",\n",
    "                                             \"met_phi_var\": \"METFixEE2017_phi_jesTotalDown\",\n",
    "                                             \"btagSF\": \"Jet_btagSF_deepcsv_shape\",\n",
    "                                             \"weightVariation\": False},\n",
    "                                     },\n",
    "                       verbose=False):\n",
    "    rdf = input_df\n",
    "    listOfDefinedColumns = input_df.GetColumnNames()\n",
    "    z = []\n",
    "    for sysVar, sysDict in sysVariations.items():\n",
    "        #skip systematic variations on data, only do the nominal\n",
    "        if isData and sysVar != \"$NOMINAL\": \n",
    "            continue\n",
    "        #skip making MET corrections unless it is: Nominal or a scale variation (i.e. JES up...)\n",
    "        isWeightVariation = sysDict.get(\"weightVariation\", False)\n",
    "        if isWeightVariation == True: continue\n",
    "        metPt = sysDict.get(\"met_pt_var\")\n",
    "        metPhi = sysDict.get(\"met_phi_var\")\n",
    "        metDoublet = \"MET_xycorr_doublet{pf}\".format(pf=sysVar.replace(\"$NOMINAL\", \"_nom\"))\n",
    "        metPtName = \"FTAMET{pf}_pt\".format(pf=sysVar.replace(\"$NOMINAL\", \"_nom\"))\n",
    "        metPhiName = \"FTAMET{pf}_phi\".format(pf=sysVar.replace(\"$NOMINAL\", \"_nom\"))\n",
    "        #XY correctors takes the: uncorrected pt and phi, run branch, year, C++ true/false for isData, and PV branch\n",
    "        def_fnc = \"FTA::METXYCorr({mpt}, {mph}, {run}, {era}, {isData}, {pv})\".format(mpt=metPt,\n",
    "                                                                        mph=metPhi,\n",
    "                                                                        run=run_branch,\n",
    "                                                                        era=era,\n",
    "                                                                        isData=str(isData).lower(),\n",
    "                                                                        pv=npv_branch\n",
    "                                                                        )\n",
    "        #append the definitions to the list in the order required\n",
    "        z.append((metDoublet, def_fnc))\n",
    "        z.append((metPtName, \"{}.first\".format(metDoublet)))\n",
    "        z.append((metPhiName, \"{}.second\".format(metDoublet)))\n",
    "\n",
    "    for defName, defFunc in z:\n",
    "        if defName in listOfDefinedColumns:\n",
    "            if verbose:\n",
    "                print(\"{} already defined, skipping\".format(defName))\n",
    "            continue\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(\"rdf = rdf.Define(\\\"{}\\\", \\\"{}\\\")\".format(defName, defFunc))\n",
    "            rdf = rdf.Define(defName, defFunc)\n",
    "            listOfDefinedColumns.push_back(defName)\n",
    "    return rdf\n",
    "        #if metDoublet not in listOfDefinedColumns:\n",
    "            #if verbose: \n",
    "            #    print(\"Doing MET XY correction:\\nrdf = rdf.Define(\\\"{0}\\\", \\\"{1}\\\")\".format(metDoublet, def_fnc))\n",
    "            #rdf = rdf.Define(metDoublet, def_fnc)\n",
    "            #listOfDefinedColumns.push_back(metDoublet)\n",
    "        #if metPt not in listOfDefinedColumns and metPhi not in listOfDefinedColumns:\n",
    "            #if verbose: \n",
    "            #    print(\"rdf = rdf.Define(\\\"{0}\\\", \\\"{1}\\\")\".format(metPt, metDoublet))\n",
    "            #    print(\"rdf = rdf.Define(\\\"{0}\\\", \\\"{1}\\\")\".format(metPhi, metDoublet))\n",
    "            #rdf = rdf.Define(metPt, \"{}.first\".format(metDoublet))\n",
    "            #rdf = rdf.Define(metPhi, \"{}.second\".format(metDoublet))\n",
    "            #listOfDefinedColumns.push_back(metPt)\n",
    "            #listOfDefinedColumns.push_back(metPhi)\n",
    "    #return rdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def defineLeptons(input_df, input_lvl_filter=None, isData=True, useBackupChannel=False, verbose=False,\n",
    "                 sysVariations={\"$NOMINAL\": {\"jet_mask\": \"jet_mask\", \n",
    "                                             \"wgt_prebTag\": \"wgt_SUMW_PU_LSF_L1PF\",\n",
    "                                             \"jet_pt_var\": \"Jet_pt\",\n",
    "                                             \"jet_mass_var\": \"Jet_mass\",\n",
    "                                             \"met_pt_var\": \"METFixEE2017_pt\",\n",
    "                                             \"met_phi_var\": \"METFixEE2017_phi\",\n",
    "                                             \"btagSF\": \"Jet_btagSF_deepcsv_shape\",\n",
    "                                             \"weightVariation\": False},\n",
    "                              \"_jesTotalUp\": {\"jet_mask\": \"jet_mask_jesTotalUp\", \n",
    "                                             \"wgt_prebTag\": \"wgt_SUMW_PU_LSF_L1PF\",\n",
    "                                             \"jet_pt_var\": \"Jet_pt_jesTotalUp\",\n",
    "                                             \"jet_mass_var\": \"Jet_mass_jesTotalUp\",\n",
    "                                             \"met_pt_var\": \"METFixEE2017_pt_jesTotalUp\",\n",
    "                                             \"met_phi_var\": \"METFixEE2017_phi_jesTotalUp\",\n",
    "                                             \"btagSF\": \"Jet_btagSF_deepcsv_shape\",\n",
    "                                             \"weightVariation\": False},\n",
    "                              \"_jesTotalDown\": {\"jet_mask\": \"jet_mask_jesTotalDown\", \n",
    "                                             \"wgt_prebTag\": \"wgt_SUMW_PU_LSF_L1PF\",\n",
    "                                             \"jet_pt_var\": \"Jet_pt_jesTotalDown\",\n",
    "                                             \"jet_mass_var\": \"Jet_mass_jesTotalDown\",\n",
    "                                             \"met_pt_var\": \"METFixEE2017_pt_jesTotalDown\",\n",
    "                                             \"met_phi_var\": \"METFixEE2017_phi_jesTotalDown\",\n",
    "                                             \"btagSF\": \"Jet_btagSF_deepcsv_shape\",\n",
    "                                             \"weightVariation\": False},\n",
    "                                },\n",
    "                 ):\n",
    "    \"\"\"Function to take in a dataframe and return one with new columns defined,\n",
    "    plus event filtering based on the criteria defined inside the function\"\"\"\n",
    "        \n",
    "    #Set up channel bits for selection and baseline. Separation not necessary in this stage, but convenient for loops\n",
    "    Chan = {}\n",
    "    Chan[\"ElMu_selection\"] = 24576\n",
    "    Chan[\"MuMu_selection\"] = 6144\n",
    "    Chan[\"ElEl_selection\"] = 512\n",
    "    Chan[\"Mu_selection\"] = 128\n",
    "    Chan[\"El_selection\"] = 64\n",
    "    Chan[\"selection\"] = Chan[\"ElMu_selection\"] + Chan[\"MuMu_selection\"] + Chan[\"ElEl_selection\"] + Chan[\"Mu_selection\"] + Chan[\"El_selection\"]\n",
    "    Chan[\"ElMu_baseline\"] = 24576\n",
    "    Chan[\"MuMu_baseline\"] = 6144\n",
    "    Chan[\"ElEl_baseline\"] = 512\n",
    "    Chan[\"Mu_baseline\"] = 128\n",
    "    Chan[\"El_baseline\"] = 64\n",
    "    Chan[\"baseline\"] = Chan[\"ElMu_baseline\"] + Chan[\"MuMu_baseline\"] + Chan[\"ElEl_baseline\"] + Chan[\"Mu_baseline\"] + Chan[\"El_baseline\"]\n",
    "    b = {}\n",
    "    b[\"baseline\"] = \"(ESV_TriggerAndLeptonLogic_baseline & {0}) > 0\".format(Chan[\"ElMu_baseline\"] + Chan[\"MuMu_baseline\"] + \n",
    "                                                                            Chan[\"ElEl_baseline\"] + Chan[\"Mu_baseline\"] + Chan[\"El_baseline\"])\n",
    "    \n",
    "    b[\"ElMu_baseline\"] = \"(ESV_TriggerAndLeptonLogic_baseline & {0}) > 0\".format(Chan[\"ElMu_baseline\"])\n",
    "    b[\"MuMu_baseline\"] = \"(ESV_TriggerAndLeptonLogic_baseline & {0}) == 0 && (ESV_TriggerAndLeptonLogic_baseline & {1}) > 0\".format(Chan[\"ElMu_baseline\"], \n",
    "                                                                                                                                Chan[\"MuMu_baseline\"])\n",
    "    b[\"ElEl_baseline\"] = \"(ESV_TriggerAndLeptonLogic_baseline & {0}) == 0 && (ESV_TriggerAndLeptonLogic_baseline & {1}) > 0\".format(Chan[\"ElMu_baseline\"] + Chan[\"MuMu_baseline\"], \n",
    "                                                                                                                                Chan[\"ElEl_baseline\"])\n",
    "    b[\"Mu_baseline\"] = \"(ESV_TriggerAndLeptonLogic_baseline & {0}) == 0 && (ESV_TriggerAndLeptonLogic_baseline & {1}) > 0\".format(Chan[\"ElMu_baseline\"] + Chan[\"MuMu_baseline\"] + Chan[\"ElEl_baseline\"], Chan[\"Mu_baseline\"])\n",
    "    b[\"El_baseline\"] = \"(ESV_TriggerAndLeptonLogic_baseline & {0}) == 0 && (ESV_TriggerAndLeptonLogic_baseline & {1}) > 0\".format(Chan[\"ElMu_baseline\"] + Chan[\"MuMu_baseline\"] + \n",
    "                                                                                                                    Chan[\"ElEl_baseline\"] + Chan[\"Mu_baseline\"], Chan[\"El_baseline\"])\n",
    "    b[\"selection\"] = \"ESV_TriggerAndLeptonLogic_selection > 0\"\n",
    "    b[\"ElMu_selection\"] = \"(ESV_TriggerAndLeptonLogic_selection & {0}) > 0\".format(Chan[\"ElMu_selection\"])\n",
    "    b[\"MuMu_selection\"] = \"(ESV_TriggerAndLeptonLogic_selection & {0}) == 0 && (ESV_TriggerAndLeptonLogic_selection & {1}) > 0\".format(Chan[\"ElMu_selection\"], Chan[\"MuMu_selection\"])\n",
    "    b[\"ElEl_selection\"] = \"(ESV_TriggerAndLeptonLogic_selection & {0}) == 0 && (ESV_TriggerAndLeptonLogic_selection & {1}) > 0\".format(Chan[\"ElMu_selection\"] + Chan[\"MuMu_selection\"], Chan[\"ElEl_selection\"])\n",
    "    b[\"Mu_selection\"] = \"(ESV_TriggerAndLeptonLogic_selection & {0}) == 0 && (ESV_TriggerAndLeptonLogic_selection & {1}) > 0\".format(Chan[\"ElMu_selection\"] + Chan[\"MuMu_selection\"] + Chan[\"ElEl_selection\"], Chan[\"Mu_selection\"])\n",
    "    b[\"El_selection\"] = \"(ESV_TriggerAndLeptonLogic_selection & {0}) == 0 && (ESV_TriggerAndLeptonLogic_selection & {1}) > 0\".format(Chan[\"ElMu_selection\"] + Chan[\"MuMu_selection\"] + Chan[\"ElEl_selection\"]\n",
    "                                                                                                                                 + Chan[\"Mu_selection\"], Chan[\"El_selection\"])\n",
    "    if input_lvl_filter == None:\n",
    "        rdf = input_df\\\n",
    "                .Define(\"mu_mask\", \"Muon_pt > 0\").Define(\"e_mask\", \"Electron_pt > 0\")\n",
    "    else:\n",
    "        if \"baseline\" in input_lvl_filter:\n",
    "            lvl_type = \"baseline\"\n",
    "        elif \"selection\" in input_lvl_filter:\n",
    "            lvl_type = \"selection\"\n",
    "        else:\n",
    "            raise RuntimeError(\"No such level permissable: must contain 'selection' or 'baseline'\")\n",
    "        rdf_input = input_df.Filter(b[input_lvl_filter], input_lvl_filter)\n",
    "        rdf = rdf_input\n",
    "        rdf = rdf.Define(\"mu_mask\", \"(Muon_OSV_{0} & {1}) > 0\".format(lvl_type, Chan[input_lvl_filter]))\n",
    "        rdf = rdf.Define(\"e_mask\", \"(Electron_OSV_{0} & {1}) > 0\".format(lvl_type, Chan[input_lvl_filter]))\n",
    "    transverseMassCode = '''auto MT2 = {m1}*{m1} + {m2}*{m2} + 2*(sqrt({m1}*{m1} + {pt1}*{pt1})*sqrt({m2}*{m2} + {pt2}*{pt2}) - {pt1}*{pt2}*cos(ROOT::VecOps::DeltaPhi({phi1}, {phi2})));\n",
    "                         return sqrt(MT2);'''\n",
    "    transverseMassCodeChannelSafe = '''\n",
    "                         if( {pt1}.size() != {pt2}.size()){{ROOT::VecOps::RVec<float> v; v.push_back(-9999); return v;}}\n",
    "                         else {{auto MT2 = {m1}*{m1} + {m2}*{m2} + 2*(sqrt({m1}*{m1} + {pt1}*{pt1})*sqrt({m2}*{m2} + {pt2}*{pt2}) - {pt1}*{pt2}*cos(ROOT::VecOps::DeltaPhi({phi1}, {phi2})));\n",
    "                         return sqrt(MT2);\n",
    "                         }}'''\n",
    "    transverseMassCodeChecker = '''auto V1 = ROOT::Math::PtEtaPhiMVector({pt1}, {eta1}, {phi1}, {m1});\n",
    "                                auto V2 = ROOT::Math::PtEtaPhiMVector({pt2}, {eta2}, {phi2}, {m2});\n",
    "                                auto V = V1 + V2;\n",
    "                                return V.Mt();'''\n",
    "    transverseMassLightCode = '''auto MT2 = 2*{pt1}*{pt2}*(1 - cos(ROOT::VecOps::DeltaPhi({phi1}, {phi2})));\n",
    "                              return sqrt(MT2);'''\n",
    "    z = []\n",
    "    #only valid postfix for leptons, excluding calculations involving MET, is \"_nom\"\n",
    "    leppostfix = \"_nom\"\n",
    "    \n",
    "    #MUONS\n",
    "    z.append((\"Muon_idx\", \"FTA::generateIndices(Muon_pt);\"))\n",
    "    z.append((\"nFTAMuon{lpf}\".format(lpf=leppostfix), \"Muon_pt[mu_mask].size()\"))\n",
    "    z.append((\"FTAMuon{lpf}_idx\".format(lpf=leppostfix), \"Muon_idx[mu_mask]\"))\n",
    "    z.append((\"FTAMuon{lpf}_pfIsoId\".format(lpf=leppostfix), \"Muon_pfIsoId[mu_mask]\"))\n",
    "    z.append((\"FTAMuon{lpf}_looseId\".format(lpf=leppostfix), \"Muon_looseId[mu_mask]\"))\n",
    "    z.append((\"FTAMuon{lpf}_pt\".format(lpf=leppostfix), \"Muon_pt[mu_mask]\"))\n",
    "    z.append((\"FTAMuon{lpf}_eta\".format(lpf=leppostfix), \"Muon_eta[mu_mask]\"))\n",
    "    z.append((\"FTAMuon{lpf}_phi\".format(lpf=leppostfix), \"Muon_phi[mu_mask]\"))\n",
    "    z.append((\"FTAMuon{lpf}_mass\".format(lpf=leppostfix), \"Muon_mass[mu_mask]\"))\n",
    "    z.append((\"FTAMuon{lpf}_charge\".format(lpf=leppostfix), \"Muon_charge[mu_mask]\"))\n",
    "    z.append((\"FTAMuon{lpf}_dz\".format(lpf=leppostfix), \"Muon_dz[mu_mask]\"))\n",
    "    z.append((\"FTAMuon{lpf}_dxy\".format(lpf=leppostfix), \"Muon_dxy[mu_mask]\"))\n",
    "    z.append((\"FTAMuon{lpf}_d0\".format(lpf=leppostfix), \"sqrt(Muon_dz*Muon_dz + Muon_dxy*Muon_dxy)[mu_mask]\"))\n",
    "    z.append((\"FTAMuon{lpf}_ip3d\".format(lpf=leppostfix), \"Muon_ip3d[mu_mask]\"))\n",
    "    z.append((\"FTAMuon{lpf}_pfRelIso03_all\".format(lpf=leppostfix), \"Muon_pfRelIso03_all[mu_mask]\"))\n",
    "    z.append((\"FTAMuon{lpf}_pfRelIso03_chg\".format(lpf=leppostfix), \"Muon_pfRelIso03_chg[mu_mask]\"))\n",
    "    z.append((\"FTAMuon{lpf}_pfRelIso04_all\".format(lpf=leppostfix), \"Muon_pfRelIso04_all[mu_mask]\"))\n",
    "    z.append((\"FTAMuon{lpf}_jetIdx\".format(lpf=leppostfix), \"Muon_jetIdx[mu_mask]\"))\n",
    "    #z.append((\"METofMETandMu2\", ) #FIXME: switch to MET_xycorr_pt{}\n",
    "    z.append((\"nFTAMuon{lpf}\".format(lpf=leppostfix), \"Muon{pf}_pt.size()\"))\n",
    "    z.append((\"nLooseFTAMuon{lpf}\".format(lpf=leppostfix), \"Muon_looseId[mu_mask && Muon_looseId == true].size()\"))\n",
    "    z.append((\"nMediumFTAMuon{lpf}\".format(lpf=leppostfix), \"Muon_mediumId[mu_mask && Muon_mediumId == true].size()\"))\n",
    "    z.append((\"nTightFTAMuon{lpf}\".format(lpf=leppostfix), \"Muon_tightId[mu_mask && Muon_tightId == true].size()\"))\n",
    "    z.append((\"FTAMuon{lpf}_InvariantMass\".format(lpf=leppostfix), \"nFTAMuon{lpf} == 2 ? InvariantMass(FTAMuon{lpf}_pt, FTAMuon{lpf}_eta, FTAMuon{lpf}_phi, FTAMuon{lpf}_mass) : -0.1\".format(lpf=leppostfix)))    \n",
    "    if isData == False:\n",
    "        z.append((\"FTAMuon{lpf}_SF_ID_nom\".format(lpf=leppostfix), \"Muon_SF_ID_nom[mu_mask]\"))\n",
    "        z.append((\"FTAMuon{lpf}_SF_ID_stat\".format(lpf=leppostfix), \"Muon_SF_ID_stat[mu_mask]\"))\n",
    "        z.append((\"FTAMuon{lpf}_SF_ID_syst\".format(lpf=leppostfix), \"Muon_SF_ID_syst[mu_mask]\"))\n",
    "        z.append((\"FTAMuon{lpf}_SF_ISO_nom\".format(lpf=leppostfix), \"Muon_SF_ISO_nom[mu_mask]\"))\n",
    "        z.append((\"FTAMuon{lpf}_SF_ISO_stat\".format(lpf=leppostfix), \"Muon_SF_ISO_stat[mu_mask]\"))\n",
    "        z.append((\"FTAMuon{lpf}_SF_ISO_syst\".format(lpf=leppostfix), \"Muon_SF_ISO_syst[mu_mask]\"))\n",
    "    #ELECTRONS\n",
    "    z.append((\"Electron_idx\", \"FTA::generateIndices(Electron_pt);\"))\n",
    "    z.append((\"nFTAElectron{lpf}\".format(lpf=leppostfix), \"Electron_pt[e_mask].size()\"))\n",
    "    z.append((\"FTAElectron{lpf}_idx\".format(lpf=leppostfix), \"Electron_idx[e_mask]\"))\n",
    "    z.append((\"FTAElectron{lpf}_cutBased\".format(lpf=leppostfix), \"Electron_cutBased[e_mask]\"))\n",
    "    z.append((\"FTAElectron{lpf}_pt\".format(lpf=leppostfix), \"Electron_pt[e_mask]\"))\n",
    "    z.append((\"FTAElectron{lpf}_eta\".format(lpf=leppostfix), \"Electron_eta[e_mask]\"))\n",
    "    z.append((\"FTAElectron{lpf}_phi\".format(lpf=leppostfix), \"Electron_phi[e_mask]\"))\n",
    "    z.append((\"FTAElectron{lpf}_mass\".format(lpf=leppostfix), \"Electron_mass[e_mask]\"))\n",
    "    z.append((\"FTAElectron{lpf}_charge\".format(lpf=leppostfix), \"Electron_charge[e_mask]\"))\n",
    "    z.append((\"FTAElectron{lpf}_dz\".format(lpf=leppostfix), \"Electron_dz[e_mask]\"))\n",
    "    z.append((\"FTAElectron{lpf}_dxy\".format(lpf=leppostfix), \"Electron_dxy[e_mask]\"))\n",
    "    z.append((\"FTAElectron{lpf}_d0\".format(lpf=leppostfix), \"sqrt(Electron_dz*Electron_dz + Electron_dxy*Electron_dxy)[e_mask]\"))\n",
    "    z.append((\"FTAElectron{lpf}_ip3d\".format(lpf=leppostfix), \"Electron_ip3d[e_mask]\"))\n",
    "    z.append((\"FTAElectron{lpf}_pfRelIso03_all\".format(lpf=leppostfix), \"Electron_pfRelIso03_all[e_mask]\"))\n",
    "    z.append((\"FTAElectron{lpf}_pfRelIso03_chg\".format(lpf=leppostfix), \"Electron_pfRelIso03_chg[e_mask]\"))\n",
    "    z.append((\"FTAElectron{lpf}_jetIdx\".format(lpf=leppostfix), \"Electron_jetIdx[e_mask]\"))\n",
    "    ##FIXME: This code above is broken for some reason, doesn't like it... why?\n",
    "    z.append((\"nLooseFTAElectron{lpf}\".format(lpf=leppostfix), \"Sum(FTAElectron{lpf}_cutBased >= 2)\".format(lpf=leppostfix)))\n",
    "    z.append((\"nMediumFTAElectron{lpf}\".format(lpf=leppostfix), \"Sum(FTAElectron{lpf}_cutBased >= 3)\".format(lpf=leppostfix)))\n",
    "    z.append((\"nTightFTAElectron{lpf}\".format(lpf=leppostfix), \"Sum(FTAElectron{lpf}_cutBased >= 4)\".format(lpf=leppostfix)))\n",
    "    z.append((\"FTAElectron{lpf}_InvariantMass\".format(lpf=leppostfix), \"nFTAElectron{lpf} == 2 ? InvariantMass(FTAElectron{lpf}_pt, FTAElectron{lpf}_eta, FTAElectron{lpf}_phi, FTAElectron{lpf}_mass) : -0.1\".format(lpf=leppostfix)))\n",
    "    if isData == False:\n",
    "        z.append((\"FTAElectron{lpf}_SF_EFF_nom\".format(lpf=leppostfix), \"Electron_SF_EFF_nom[e_mask]\"))\n",
    "        z.append((\"FTAElectron{lpf}_SF_EFF_unc\".format(lpf=leppostfix), \"Electron_SF_EFF_unc[e_mask]\"))\n",
    "        z.append((\"FTAElectron{lpf}_SF_ID_nom\".format(lpf=leppostfix), \"Electron_SF_ID_nom[e_mask]\"))\n",
    "        z.append((\"FTAElectron{lpf}_SF_ID_unc\".format(lpf=leppostfix), \"Electron_SF_ID_unc[e_mask]\"))\n",
    "    #LEPTONS\n",
    "    z.append((\"nLooseFTALepton{lpf}\".format(lpf=leppostfix), \"nLooseFTAMuon{lpf} + nLooseFTAElectron{lpf}\".format(lpf=leppostfix)))\n",
    "    z.append((\"nMediumFTALepton{lpf}\".format(lpf=leppostfix), \"nMediumFTAMuon{lpf} + nMediumFTAElectron{lpf}\".format(lpf=leppostfix)))\n",
    "    z.append((\"nTightFTALepton{lpf}\".format(lpf=leppostfix), \"nTightFTAMuon{lpf} + nTightFTAElectron{lpf}\".format(lpf=leppostfix)))\n",
    "    z.append((\"nFTALepton{lpf}\".format(lpf=leppostfix), \"nFTAMuon{lpf} + nFTAElectron{lpf}\".format(lpf=leppostfix)))\n",
    "    z.append((\"FTALepton{lpf}_argsort\".format(lpf=leppostfix), \"Reverse(Argsort(Concatenate(Muon_pt[mu_mask], Electron_pt[e_mask])))\"))\n",
    "    z.append((\"FTALepton{lpf}_pt\".format(lpf=leppostfix), \"Take(Concatenate(Muon_pt[mu_mask], Electron_pt[e_mask]), FTALepton{lpf}_argsort)\".format(lpf=leppostfix)))\n",
    "    z.append((\"FTALepton{lpf}_eta\".format(lpf=leppostfix), \"Take(Concatenate(Muon_eta[mu_mask], Electron_eta[e_mask]), FTALepton{lpf}_argsort)\".format(lpf=leppostfix)))\n",
    "    z.append((\"FTALepton{lpf}_phi\".format(lpf=leppostfix), \"Take(Concatenate(Muon_phi[mu_mask], Electron_phi[e_mask]), FTALepton{lpf}_argsort)\".format(lpf=leppostfix)))\n",
    "    z.append((\"FTALepton{lpf}_jetIdx\".format(lpf=leppostfix), \"Take(Concatenate(Muon_jetIdx[mu_mask], Electron_jetIdx[e_mask]), FTALepton{lpf}_argsort)\".format(lpf=leppostfix)))\n",
    "    z.append((\"FTALepton{lpf}_pdgId\".format(lpf=leppostfix), \"Take(Concatenate(Muon_pdgId[mu_mask], Electron_pdgId[e_mask]), FTALepton{lpf}_argsort)\".format(lpf=leppostfix)))\n",
    "    z.append((\"FTALepton{lpf}_dRll\".format(lpf=leppostfix), \"FTALepton{lpf}_pt.size() > 1 ? ROOT::VecOps::DeltaR(FTALepton{lpf}_eta.at(0), FTALepton{lpf}_eta.at(1), FTALepton{lpf}_phi.at(0), FTALepton{lpf}_phi.at(1)) : -0.1\".format(lpf=leppostfix)))\n",
    "    z.append((\"FTALepton{lpf}_dPhill\".format(lpf=leppostfix), \"FTALepton{lpf}_pt.size() > 1 ? ROOT::VecOps::DeltaPhi(FTALepton{lpf}_phi.at(0), FTALepton{lpf}_phi.at(1)) : -999\".format(lpf=leppostfix)))\n",
    "    z.append((\"FTALepton{lpf}_dEtall\".format(lpf=leppostfix), \"FTALepton{lpf}_pt.size() > 1 ? abs(FTALepton{lpf}_eta.at(0) - FTALepton{lpf}_eta.at(1)) : -999\".format(lpf=leppostfix)))\n",
    "    z.append((\"nFTALepton{lpf}\".format(lpf=leppostfix), \"FTALepton{lpf}_pt.size()\".format(lpf=leppostfix)))\n",
    "    z.append((\"FTALepton{lpf}_pt_LeadLep\".format(lpf=leppostfix), \"FTALepton{lpf}_pt.size() > 0 ? FTALepton{lpf}_pt.at(0) : -0.000000000001\".format(lpf=leppostfix)))\n",
    "    z.append((\"FTALepton{lpf}_pt_SubleadLep\".format(lpf=leppostfix), \"FTALepton{lpf}_pt.size() > 1 ? FTALepton{lpf}_pt.at(1) : -0.000000000001\".format(lpf=leppostfix)))\n",
    "    z.append((\"FTALepton{lpf}_eta_LeadLep\".format(lpf=leppostfix), \"FTALepton{lpf}_eta.size() > 0 ? FTALepton{lpf}_eta.at(0) : -9999\".format(lpf=leppostfix)))\n",
    "    z.append((\"FTALepton{lpf}_eta_SubleadLep\".format(lpf=leppostfix), \"FTALepton{lpf}_eta.size() > 1 ? FTALepton{lpf}_eta.at(1) : -0.9999\".format(lpf=leppostfix)))\n",
    "    z.append((\"FTALepton{lpf}_jetIdx_0\".format(lpf=leppostfix), \"FTALepton{lpf}_jetIdx.size() > 0 ? FTALepton{lpf}_jetIdx.at(0) : -1\".format(lpf=leppostfix)))\n",
    "    z.append((\"FTALepton{lpf}_jetIdx_1\".format(lpf=leppostfix), \"FTALepton{lpf}_jetIdx.size() > 1 ? FTALepton{lpf}_jetIdx.at(1) : -1\".format(lpf=leppostfix)))\n",
    "    if isData == False:\n",
    "        z.append((\"FTALepton{lpf}_SF_nom\".format(lpf=leppostfix), \"Take(Concatenate(FTAMuon{lpf}_SF_ID_nom*FTAMuon{lpf}_SF_ISO_nom, FTAElectron{lpf}_SF_ID_nom*FTAElectron{lpf}_SF_EFF_nom), FTALepton{lpf}_argsort)\".format(lpf=leppostfix)))\n",
    "\n",
    "    for sysVar, sysDict in sysVariations.items():\n",
    "        #skip systematic variations on data, only do the nominal\n",
    "        if isData and sysVar != \"$NOMINAL\": \n",
    "            continue\n",
    "        #skip making MET corrections unless it is: Nominal or a scale variation (i.e. JES up...)\n",
    "        isWeightVariation = sysDict.get(\"weightVariation\", False)\n",
    "        if isWeightVariation == True: continue\n",
    "        postfix = sysVar.replace(\"$NOMINAL\", \"_nom\")\n",
    "        #metPt = sysDict.get(\"met_pt_var\")\n",
    "        #metPhi = sysDict.get(\"met_phi_var\")\n",
    "        #These are the xy corrected MET values, to be used in the calculations\n",
    "        metPtName = \"FTAMET{pf}_pt\".format(pf=sysVar.replace(\"$NOMINAL\", \"_nom\"))\n",
    "        metPhiName = \"FTAMET{pf}_phi\".format(pf=sysVar.replace(\"$NOMINAL\", \"_nom\"))\n",
    "        z.append((\"MTofMETandMu{pf}\".format(pf=postfix), \n",
    "                         \"FTA::transverseMassMET(FTAMuon{lpf}_pt, FTAMuon{lpf}_phi, FTAMuon{lpf}_mass, {mpt}, {mph})\".format(lpf=leppostfix, mpt=metPtName, mph=metPhiName)))\n",
    "        z.append((\"MTofMETandEl{pf}\".format(pf=postfix),  \n",
    "                         \"FTA::transverseMassMET(FTAElectron{lpf}_pt, FTAElectron{lpf}_phi, FTAElectron{lpf}_mass, {mpt}, {mph})\".format(lpf=leppostfix, mpt=metPtName, mph=metPhiName)))\n",
    "        #There shouldn't be any variation on this quantity, but... easier looping\n",
    "        z.append((\"MTofElandMu{pf}\".format(pf=postfix), \n",
    "                         \"FTA::transverseMass(FTAMuon{lpf}_pt, FTAMuon{lpf}_phi, FTAMuon{lpf}_mass, FTAElectron{lpf}_pt, FTAElectron{lpf}_phi, FTAElectron{lpf}_mass)\".format(lpf=leppostfix)))\n",
    "    \n",
    "    listOfDefinedColumns = rdf.GetDefinedColumnNames()\n",
    "    #Add them to the dataframe...\n",
    "    for defName, defFunc in z:\n",
    "        if defName in listOfDefinedColumns:\n",
    "            if verbose:\n",
    "                print(\"{} already defined, skipping\".format(defName))\n",
    "            continue\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(\"rdf = rdf.Define(\\\"{}\\\", \\\"{}\\\")\".format(defName, defFunc))\n",
    "            rdf = rdf.Define(defName, defFunc)\n",
    "            listOfDefinedColumns.push_back(defName)\n",
    "    return rdf\n",
    "    #OLD CODE\n",
    "    #rdf = rdf.Define(\"Muon_idx\", \"FTA::generateIndices(Muon_pt);\")\n",
    "    #rdf = rdf.Define(\"GMuon_idx\", \"Muon_idx[mu_mask]\")\n",
    "    #rdf = rdf.Define(\"GMuon_pfIsoId\", \"Muon_pfIsoId[mu_mask]\")\n",
    "    #rdf = rdf.Define(\"GMuon_looseId\", \"Muon_looseId[mu_mask]\")\n",
    "    #rdf = rdf.Define(\"GMuon_pt\", \"Muon_pt[mu_mask]\")\n",
    "    #rdf = rdf.Define(\"GMuon_eta\", \"Muon_eta[mu_mask]\")\n",
    "    #rdf = rdf.Define(\"GMuon_phi\", \"Muon_phi[mu_mask]\")\n",
    "    #rdf = rdf.Define(\"GMuon_mass\", \"Muon_mass[mu_mask]\")\n",
    "    #rdf = rdf.Define(\"GMuon_charge\", \"Muon_charge[mu_mask]\")\n",
    "    #rdf = rdf.Define(\"GMuon_dz\", \"Muon_dz[mu_mask]\")\n",
    "    #rdf = rdf.Define(\"GMuon_dxy\", \"Muon_dxy[mu_mask]\")\n",
    "    #rdf = rdf.Define(\"GMuon_d0\", \"sqrt(Muon_dz*Muon_dz + Muon_dxy*Muon_dxy)[mu_mask]\")\n",
    "    #rdf = rdf.Define(\"GMuon_ip3d\", \"Muon_ip3d[mu_mask]\")\n",
    "    #rdf = rdf.Define(\"GMuon_pfRelIso03_all\", \"Muon_pfRelIso03_all[mu_mask]\")\n",
    "    #rdf = rdf.Define(\"GMuon_pfRelIso03_chg\", \"Muon_pfRelIso03_chg[mu_mask]\")\n",
    "    #rdf = rdf.Define(\"GMuon_pfRelIso04_all\", \"Muon_pfRelIso04_all[mu_mask]\")\n",
    "    #rdf = rdf.Define(\"GMuon_jetIdx\", \"Muon_jetIdx[mu_mask]\")\n",
    "    ##rdf = rdf.Define(\"METofMETandMu2\", ) #FIXME: switch to MET_xycorr_pt{}\n",
    "    #rdf = rdf.Define(\"nGMuon\", \"GMuon_pt.size()\")\n",
    "    #rdf = rdf.Define(\"nLooseGMuon\", \"Muon_looseId[mu_mask && Muon_looseId == true].size()\")\n",
    "    #rdf = rdf.Define(\"nMediumGMuon\", \"Muon_mediumId[mu_mask && Muon_mediumId == true].size()\")\n",
    "    #rdf = rdf.Define(\"nTightGMuon\", \"Muon_tightId[mu_mask && Muon_tightId == true].size()\")\n",
    "    #rdf = rdf.Define(\"GMuon_InvariantMass\", \"nGMuon == 2 ? InvariantMass(GMuon_pt, GMuon_eta, GMuon_phi, GMuon_mass) : -0.1\")\n",
    "    #rdf = rdf.Define(\"Electron_idx\", \"FTA::generateIndices(Electron_pt);\")\n",
    "    #rdf = rdf.Define(\"GElectron_idx\", \"Electron_idx[e_mask]\")\n",
    "    #rdf = rdf.Define(\"GElectron_cutBased\", \"Electron_cutBased[e_mask]\")\n",
    "    #rdf = rdf.Define(\"GElectron_pt\", \"Electron_pt[e_mask]\")\n",
    "    #rdf = rdf.Define(\"GElectron_eta\", \"Electron_eta[e_mask]\")\n",
    "    #rdf = rdf.Define(\"GElectron_phi\", \"Electron_phi[e_mask]\")\n",
    "    #rdf = rdf.Define(\"GElectron_mass\", \"Electron_mass[e_mask]\")\n",
    "    #rdf = rdf.Define(\"GElectron_charge\", \"Electron_charge[e_mask]\")\n",
    "    #rdf = rdf.Define(\"GElectron_dz\", \"Electron_dz[e_mask]\")\n",
    "    #rdf = rdf.Define(\"GElectron_dxy\", \"Electron_dxy[e_mask]\")\n",
    "    #rdf = rdf.Define(\"GElectron_d0\", \"sqrt(Electron_dz*Electron_dz + Electron_dxy*Electron_dxy)[e_mask]\")\n",
    "    #rdf = rdf.Define(\"GElectron_ip3d\", \"Electron_ip3d[e_mask]\")\n",
    "    #rdf = rdf.Define(\"GElectron_pfRelIso03_all\", \"Electron_pfRelIso03_all[e_mask]\")\n",
    "    #rdf = rdf.Define(\"GElectron_pfRelIso03_chg\", \"Electron_pfRelIso03_chg[e_mask]\")\n",
    "    #rdf = rdf.Define(\"GElectron_jetIdx\", \"Electron_jetIdx[e_mask]\")\n",
    "    ##FIXME: This code above is broken for some reason, doesn't like it... why?\n",
    "    #rdf = rdf.Define(\"nGElectron\", \"GElectron_pt.size()\")\n",
    "    #rdf = rdf.Define(\"nLooseGElectron\", \"Sum(GElectron_cutBased >= 2)\")\n",
    "    #rdf = rdf.Define(\"nMediumGElectron\", \"Sum(GElectron_cutBased >= 3)\")\n",
    "    #rdf = rdf.Define(\"nTightGElectron\", \"Sum(GElectron_cutBased >= 4)\")\n",
    "        \n",
    "    #for sysVar, sysDict in sysVariations.items():\n",
    "    #    #skip systematic variations on data, only do the nominal\n",
    "    #    if isData and sysVar != \"$NOMINAL\": \n",
    "    #        continue\n",
    "    #    #skip making MET corrections unless it is: Nominal or a scale variation (i.e. JES up...)\n",
    "    #    isWeightVariation = sysDict.get(\"weightVariation\", False)\n",
    "    #    if isWeightVariation == True: continue\n",
    "    #    postfix = sysVar.replace(\"$NOMINAL\", \"_nom\")\n",
    "    #    metPt = sysDict.get(\"met_pt_var\")\n",
    "    #    metPhi = sysDict.get(\"met_phi_var\")\n",
    "    #    metPtName = \"MET_xycorr_pt{}\".format(sysVar.replace(\"$NOMINAL\", \"_nom\"))\n",
    "    #    metPhiName = \"MET_xycorr_phi{}\".format(sysVar.replace(\"$NOMINAL\", \"_nom\"))\n",
    "    #    rdf = rdf.Define()\n",
    "    #    rdf = rdf.Define(\"MTofMETandMu{}\".format(postfix), \n",
    "    #                     \"FTA::transverseMassMET(\\\"GMuon_pt\\\", \\\"GMuon_phi\\\", \\\"GMuon_mass\\\", \\\"{0}\\\", \\\"{1}\\\")\".format(metPtName, metPhiName))\n",
    "    #    rdf = rdf.Define(\"MTofMETandEl{}\".format(postfix),  \n",
    "    #                     \"FTA::transverseMassMET(\\\"GElectron_pt\\\", \\\"GElectron_phi\\\", \\\"GElectron_mass\\\", \\\"{0}\\\", \\\"{1}\\\")\".format(metPtName, metPhiName))\n",
    "    #    #There shouldn't be any variation on this quantity, but... easier looping\n",
    "    #    rdf = rdf.Define(\"MTofElandMu{}\".format(postfix), \n",
    "    #                     \"FTA::transverseMassMET(\\\"GMuon_pt\\\", \\\"GMuon_phi\\\", \\\"GMuon_mass\\\", \\\"GElectron_pt\\\", \\\"GElectron_phi\\\", \\\"GElectron_mass\\\")\")\n",
    "    \n",
    "    #rdf = rdf.Define(\"nLooseGLepton\", \"nLooseGMuon + nLooseGElectron\")\n",
    "    #rdf = rdf.Define(\"nMediumGLepton\", \"nMediumGMuon + nMediumGElectron\")\n",
    "    #rdf = rdf.Define(\"nTightGLepton\", \"nTightGMuon + nTightGElectron\")\n",
    "    #rdf = rdf.Define(\"GElectron_InvariantMass\", \"nGElectron == 2 ? InvariantMass(GElectron_pt, GElectron_eta, GElectron_phi, GElectron_mass) : -0.1\")\n",
    "    #rdf = rdf.Define(\"GLepton_argsort\", \"Reverse(Argsort(Concatenate(Muon_pt[mu_mask], Electron_pt[e_mask])))\")\n",
    "    #rdf = rdf.Define(\"GLepton_pt\", \"Take(Concatenate(Muon_pt[mu_mask], Electron_pt[e_mask]), GLepton_argsort)\")\n",
    "    #rdf = rdf.Define(\"GLepton_eta\", \"Take(Concatenate(Muon_eta[mu_mask], Electron_eta[e_mask]), GLepton_argsort)\")\n",
    "    #rdf = rdf.Define(\"GLepton_phi\", \"Take(Concatenate(Muon_phi[mu_mask], Electron_phi[e_mask]), GLepton_argsort)\")\n",
    "    #rdf = rdf.Define(\"GLepton_jetIdx\", \"Take(Concatenate(Muon_jetIdx[mu_mask], Electron_jetIdx[e_mask]), GLepton_argsort)\")\n",
    "    #rdf = rdf.Define(\"GLepton_pdgId\", \"Take(Concatenate(Muon_pdgId[mu_mask], Electron_pdgId[e_mask]), GLepton_argsort)\")\n",
    "    #rdf = rdf.Define(\"GLepton_dRll\", \"GLepton_pt.size() > 1 ? ROOT::VecOps::DeltaR(GLepton_eta.at(0), GLepton_eta.at(1), GLepton_phi.at(0), GLepton_phi.at(1)) : -0.1\")\n",
    "    #rdf = rdf.Define(\"GLepton_dPhill\", \"GLepton_pt.size() > 1 ? ROOT::VecOps::DeltaPhi(GLepton_phi.at(0), GLepton_phi.at(1)) : -999\")\n",
    "    #rdf = rdf.Define(\"GLepton_dEtall\", \"GLepton_pt.size() > 1 ? abs(GLepton_eta.at(0) - GLepton_eta.at(1)) : -999\")\n",
    "    #rdf = rdf.Define(\"nGLepton\", \"GLepton_pt.size()\")\n",
    "    #rdf = rdf.Define(\"GLepton_pt_LeadLep\", \"GLepton_pt.size() > 0 ? GLepton_pt.at(0) : -0.000000000001\")\n",
    "    #rdf = rdf.Define(\"GLepton_pt_SubleadLep\", \"GLepton_pt.size() > 1 ? GLepton_pt.at(1) : -0.000000000001\")\n",
    "    #rdf = rdf.Define(\"GLepton_eta_LeadLep\", \"GLepton_eta.size() > 0 ? GLepton_eta.at(0) : -9999\")\n",
    "    #rdf = rdf.Define(\"GLepton_eta_SubleadLep\", \"GLepton_eta.size() > 1 ? GLepton_eta.at(1) : -0.9999\")\n",
    "    #rdf = rdf.Define(\"GLepton_jetIdx_0\", \"GLepton_jetIdx.size() > 0 ? GLepton_jetIdx.at(0) : -1\")\n",
    "    #rdf = rdf.Define(\"GLepton_jetIdx_1\", \"GLepton_jetIdx.size() > 1 ? GLepton_jetIdx.at(1) : -1\")\n",
    "    #if isData == False:\n",
    "    #    rdf = rdf.Define(\"GMuon_SF_ID_nom\", \"Muon_SF_ID_nom[mu_mask]\")\n",
    "    #    rdf = rdf.Define(\"GMuon_SF_ID_stat\", \"Muon_SF_ID_stat[mu_mask]\")\n",
    "    #    rdf = rdf.Define(\"GMuon_SF_ID_syst\", \"Muon_SF_ID_syst[mu_mask]\")\n",
    "    #    rdf = rdf.Define(\"GMuon_SF_ISO_nom\", \"Muon_SF_ISO_nom[mu_mask]\")\n",
    "    #    rdf = rdf.Define(\"GMuon_SF_ISO_stat\", \"Muon_SF_ISO_stat[mu_mask]\")\n",
    "    #    rdf = rdf.Define(\"GMuon_SF_ISO_syst\", \"Muon_SF_ISO_syst[mu_mask]\")\n",
    "    #    rdf = rdf.Define(\"GElectron_SF_EFF_nom\", \"Electron_SF_EFF_nom[e_mask]\")\n",
    "    #    rdf = rdf.Define(\"GElectron_SF_EFF_unc\", \"Electron_SF_EFF_unc[e_mask]\")\n",
    "    #    rdf = rdf.Define(\"GElectron_SF_ID_nom\", \"Electron_SF_ID_nom[e_mask]\")\n",
    "    #    rdf = rdf.Define(\"GElectron_SF_ID_unc\", \"Electron_SF_ID_unc[e_mask]\")\n",
    "    #    rdf = rdf.Define(\"GLepton_SF_nom\", \"Take(Concatenate(Muon_SF_ID_nom[mu_mask]*Muon_SF_ISO_nom[mu_mask], Electron_SF_ID_nom[e_mask]*Electron_SF_EFF_nom[e_mask]), GLepton_argsort)\")\n",
    "#    else:\n",
    "#        rdf = rdf.Define(\"GMuon_SF_ID_nom\", \"1\")\n",
    "#        rdf = rdf.Define(\"GMuon_SF_ID_stat\", \"0\")\n",
    "#        rdf = rdf.Define(\"GMuon_SF_ID_syst\", \"0\")\n",
    "#        rdf = rdf.Define(\"GMuon_SF_ISO_nom\", \"1\")\n",
    "#        rdf = rdf.Define(\"GMuon_SF_ISO_stat\", \"0\")\n",
    "#        rdf = rdf.Define(\"GMuon_SF_ISO_syst\", \"0\")\n",
    "#        rdf = rdf.Define(\"GElectron_SF_EFF_nom\", \"1\")\n",
    "#        rdf = rdf.Define(\"GElectron_SF_EFF_unc\", \"0\")\n",
    "#        rdf = rdf.Define(\"GElectron_SF_ID_nom\", \"1\")\n",
    "#        rdf = rdf.Define(\"GElectron_SF_ID_unc\", \"0\")\n",
    "#        rdf = rdf.Define(\"GLepton_SF_nom\", \"Take(Concatenate(Muon_SF_ID_nom[mu_mask]*Muon_SF_ISO_nom[mu_mask], Electron_SF_ID_nom[e_mask]*Electron_SF_EFF_nom[e_mask]), GLepton_argsort)\")\n",
    "    \n",
    "                \n",
    "    #Things that don't work...\n",
    "    #NOPE doesn't work .Define(\"nLooseGMuon\", \"Sum(Muon_looseId[mu_mask])\")\\\n",
    "#    return rdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def defineInitWeights(input_df, crossSection=0, sumWeights=-1, lumi=0,\n",
    "                  nEvents=-1, nEventsPositive=2, nEventsNegative=1,\n",
    "                  isData=True, verbose=False):\n",
    "    leppostfix = \"_nom\"\n",
    "    \n",
    "    mc_def = collections.OrderedDict()\n",
    "    data_def = collections.OrderedDict()\n",
    "    mc_def[\"wgt_NUMW\"] = \"({xs:s} * {lumi:s} * 1000 * genWeight) / (abs(genWeight) * ( {nevtp:s} - {nevtn:s} ) )\"\\\n",
    "            .format(xs=str(crossSection), lumi=str(lumi), nevt=str(nEvents),\n",
    "                    nevtp=str(nEventsPositive), nevtn=str(nEventsNegative))\n",
    "    mc_def[\"wgt_SUMW\"] = \"({xs:s} * {lumi:s} * 1000 * genWeight) / {sumw:s}\"\\\n",
    "            .format(xs=str(crossSection), lumi=str(lumi), sumw=str(sumWeights))\n",
    "    mc_def[\"wgt_LSF\"] = \"(FTALepton{lpf}_SF_nom.size() > 1 ? FTALepton{lpf}_SF_nom.at(0) * FTALepton{lpf}_SF_nom.at(1) : FTALepton{lpf}_SF_nom.at(0))\".format(lpf=leppostfix)\n",
    "    mc_def[\"wgt_SUMW_PU\"] = \"wgt_SUMW * puWeight\"\n",
    "    mc_def[\"wgt_SUMW_LSF\"] = \"wgt_SUMW * wgt_LSF\"\n",
    "    mc_def[\"wgt_SUMW_L1PF\"] = \"wgt_SUMW * L1PreFiringWeight_Nom\"\n",
    "    mc_def[\"wgt_SUMW_PU_LSF\"] = \"wgt_SUMW * puWeight * wgt_LSF\"\n",
    "    mc_def[\"wgt_SUMW_PU_L1PF\"] = \"wgt_SUMW * puWeight * L1PreFiringWeight_Nom\"\n",
    "    mc_def[\"wgt_SUMW_PU_LSF_L1PF\"] = \"wgt_SUMW * puWeight * wgt_LSF * L1PreFiringWeight_Nom\"\n",
    "    mc_def[\"wgt_SUMW_LSF_L1PF\"] = \"wgt_SUMW * wgt_LSF * L1PreFiringWeight_Nom\"\n",
    "    mc_def[\"wgt_NUMW_LSF_L1PF\"] = \"wgt_NUMW * wgt_LSF * L1PreFiringWeight_Nom\"\n",
    "    #mc_def[\"wgt_SUMW_PU_LSF\"] = \"wgt_SUMW * puWeight * GLepton_SF_nom.at(0) * GLepton_SF_nom.at(1)\"\n",
    "    mc_def[\"SPL_SP\"] = \"wgt_SUMW_PU_LSF/wgt_SUMW_PU\"\n",
    "    mc_def[\"wgt_diff\"] = \"abs(wgt_NUMW - wgt_SUMW)/max(abs(wgt_SUMW), abs(wgt_NUMW))\"\n",
    "    for k in mc_def.keys():\n",
    "        data_def[k] = \"1\"\n",
    "    if verbose == True:\n",
    "        print(\"===data and mc weight definitions===\")\n",
    "        print(data_def)\n",
    "        print(mc_def)\n",
    "        \n",
    "    rdf = input_df\n",
    "    if isData:\n",
    "        for k, v in data_def.items():\n",
    "            rdf = rdf.Define(k, v)\n",
    "    else:\n",
    "        for k, v in mc_def.items():\n",
    "            rdf = rdf.Define(k, v)\n",
    "    return rdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def defineJets(input_df, era=\"2017\", doAK8Jets=False, isData=True, debugInfo = True, \n",
    "               nJetsToHisto=10, useDeepCSV=True, verbose=False,\n",
    "               sysVariations={\"$NOMINAL\": {\"jet_mask\": \"jet_mask\", \n",
    "                                             \"wgt_prebTag\": \"wgt_SUMW_PU_LSF_L1PF\",\n",
    "                                             \"jet_pt_var\": \"Jet_pt\",\n",
    "                                             \"jet_mass_var\": \"Jet_mass\",\n",
    "                                             \"met_pt_var\": \"METFixEE2017_pt\",\n",
    "                                             \"met_phi_var\": \"METFixEE2017_phi\",\n",
    "                                             \"btagSF\": \"Jet_btagSF_deepcsv_shape\",\n",
    "                                             \"weightVariation\": False},\n",
    "                              \"_jesTotalUp\": {\"jet_mask\": \"jet_mask_jesTotalUp\", \n",
    "                                             \"wgt_prebTag\": \"wgt_SUMW_PU_LSF_L1PF\",\n",
    "                                             \"jet_pt_var\": \"Jet_pt_jesTotalUp\",\n",
    "                                             \"jet_mass_var\": \"Jet_mass_jesTotalUp\",\n",
    "                                             \"met_pt_var\": \"METFixEE2017_pt_jesTotalUp\",\n",
    "                                             \"met_phi_var\": \"METFixEE2017_phi_jesTotalUp\",\n",
    "                                             \"btagSF\": \"Jet_btagSF_deepcsv_shape\",\n",
    "                                             \"weightVariation\": False},\n",
    "                              \"_jesTotalDown\": {\"jet_mask\": \"jet_mask_jesTotalDown\", \n",
    "                                             \"wgt_prebTag\": \"wgt_SUMW_PU_LSF_L1PF\",\n",
    "                                             \"jet_pt_var\": \"Jet_pt_jesTotalDown\",\n",
    "                                             \"jet_mass_var\": \"Jet_mass_jesTotalDown\",\n",
    "                                             \"met_pt_var\": \"METFixEE2017_pt_jesTotalDown\",\n",
    "                                             \"met_phi_var\": \"METFixEE2017_phi_jesTotalDown\",\n",
    "                                             \"btagSF\": \"Jet_btagSF_deepcsv_shape\",\n",
    "                                             \"weightVariation\": False},\n",
    "                                },\n",
    "              ):\n",
    "    \"\"\"Function to take in a dataframe and return one with new columns defined,\n",
    "    plus event filtering based on the criteria defined inside the function\"\"\"\n",
    "    bTagWorkingPointDict = {\n",
    "        '2016':{\n",
    "            'DeepCSV':{'L': 0.2217, 'M': 0.6321, 'T': 0.8953, 'Var': 'btagDeepB'},\n",
    "            'DeepJet':{ 'L': 0.0614, 'M': 0.3093, 'T': 0.7221, 'Var': 'btagDeepFlavB'}\n",
    "        },\n",
    "        '2017':{\n",
    "            'CSVv2':{'L': 0.5803, 'M': 0.8838, 'T': 0.9693, 'Var': 'btagCSVV2'},\n",
    "            'DeepCSV':{'L': 0.1522, 'M': 0.4941, 'T': 0.8001, 'Var': 'btagDeepB'},\n",
    "            'DeepJet':{'L': 0.0521, 'M': 0.3033, 'T': 0.7489, 'Var': 'btagDeepFlavB'}\n",
    "        },\n",
    "        '2018':{\n",
    "            'DeepCSV':{'L': 0.1241, 'M': 0.4184, 'T': 0.7527, 'Var': 'btagDeepB'},\n",
    "            'DeepJet':{'L': 0.0494, 'M': 0.2770, 'T': 0.7264, 'Var': 'btagDeepFlavB'}\n",
    "        }\n",
    "    }\n",
    "    print(\"FIXMEFIXME: Setting Jet_pt min to 30GeV! Must fix!\")\n",
    "    leppostfix = \"_nom\"\n",
    "    #z will be a list of tuples to define, so that we can do cleaner error handling and checks\n",
    "    z = []\n",
    "    for sysVar, sysDict in sysVariations.items():\n",
    "        #skip systematic variations on data, only do the nominal\n",
    "        if isData and sysVar != \"$NOMINAL\": \n",
    "            continue\n",
    "        #skip making MET corrections unless it is: Nominal or a scale variation (i.e. JES up...)\n",
    "        isWeightVariation = sysDict.get(\"weightVariation\", False)\n",
    "        if isWeightVariation == True: continue\n",
    "        jetMask = sysDict.get(\"jet_mask\")\n",
    "        jetPt = sysDict.get(\"jet_pt_var\")\n",
    "        jetMass = sysDict.get(\"jet_mass_var\")\n",
    "        postfix = sysVar.replace(\"$NOMINAL\", \"_nom\")\n",
    "        \n",
    "        #Fill lists\n",
    "        z.append((\"Jet_idx\", \"FTA::generateIndices(Jet_pt)\"))\n",
    "        z.append((\"{jm}\".format(jm=jetMask), \"({jpt} >= 30 && abs(Jet_eta) <= 2.5 && Jet_jetId > 2 && Jet_idx != FTALepton{lpf}_jetIdx_0 && Jet_idx != FTALepton{lpf}_jetIdx_1)\".format(lpf=leppostfix, jpt=jetPt)))\n",
    "        z.append((\"nFTAJet{pf}\".format(pf=postfix), \"{jm}[{jm}].size()\".format(jm=jetMask)))\n",
    "        z.append((\"FTAJet{pf}_ptsort\".format(pf=postfix), \"Reverse(Argsort({jpt}[{jm}]))\".format(jpt=jetPt, jm=jetMask)))\n",
    "        z.append((\"FTAJet{pf}_deepcsvsort\".format(pf=postfix), \"Reverse(Argsort(Jet_{btv}[{jm}]))\".format(btv=bTagWorkingPointDict[era][\"DeepCSV\"][\"Var\"], jm=jetMask)))\n",
    "        z.append((\"FTAJet{pf}_deepjetsort\".format(pf=postfix), \"Reverse(Argsort(Jet_{btv}[{jm}]))\".format(btv=bTagWorkingPointDict[era][\"DeepJet\"][\"Var\"], jm=jetMask)))\n",
    "        z.append((\"FTAJet{pf}_idx\".format(pf=postfix), \"Jet_idx[{jm}]\".format(jm=jetMask)))\n",
    "        z.append((\"FTAJet{pf}_pt\".format(pf=postfix), \"{jpt}[{jm}]\".format(jpt=jetPt, jm=jetMask)))\n",
    "        z.append((\"FTAJet{pf}_eta\".format(pf=postfix), \"Jet_eta[{jm}]\".format(jm=jetMask)))\n",
    "        z.append((\"FTAJet{pf}_phi\".format(pf=postfix), \"Jet_phi[{jm}]\".format(jm=jetMask)))\n",
    "        z.append((\"FTAJet{pf}_mass\".format(pf=postfix), \"{jms}[{jm}]\".format(jms=jetMass, jm=jetMask)))\n",
    "        z.append((\"FTAJet{pf}_jetId\".format(pf=postfix), \"Jet_jetId[{jm}]\".format(jm=jetMask)))\n",
    "        z.append((\"FTAJet{pf}_puId\".format(pf=postfix), \"Jet_puId[{jm}]\".format(jm=jetMask)))\n",
    "        if isData is False:\n",
    "            z.append((\"FTAJet{pf}_genJetIdx\".format(pf=postfix), \"Jet_genJetIdx[{jm}]\".format(jm=jetMask)))\n",
    "            z.append((\"nFTAJet{pf}_genMatched\".format(pf=postfix), \"FTAJet{pf}_genJetIdx[FTAJet{pf}_genJetIdx >= 0].size()\".format(pf=postfix)))\n",
    "            z.append((\"nFTAJet{pf}_puIdLoose\".format(pf=postfix), \"FTAJet{pf}_genJetIdx[(FTAJet{pf}_puId >= 4 || FTAJet{pf}_pt >= 50)].size()\".format(pf=postfix)))\n",
    "            z.append((\"nFTAJet{pf}_genMatched_puIdLoose\".format(pf=postfix), \"FTAJet{pf}_genJetIdx[FTAJet{pf}_genJetIdx >= 0 && (FTAJet{pf}_puId >= 4 || FTAJet{pf}_pt >= 50)].size()\".format(pf=postfix)))\n",
    "        z.append((\"FTAJet{pf}_DeepCSVB\".format(pf=postfix), \"Jet_btagDeepB[{jm}]\".format(jm=jetMask)))\n",
    "        z.append((\"FTAJet{pf}_DeepCSVB_sorted\".format(pf=postfix), \"Take(FTAJet{pf}_DeepCSVB, FTAJet{pf}_deepcsvsort)\".format(pf=postfix)))\n",
    "        z.append((\"FTAJet{pf}_DeepCSVB_sorted_LeadtagJet\".format(pf=postfix), \"FTAJet{pf}_DeepCSVB_sorted.size() > 0 ? FTAJet{pf}_DeepCSVB_sorted.at(0) : -9999\".format(pf=postfix)))\n",
    "        z.append((\"FTAJet{pf}_DeepCSVB_sorted_SubleadtagJet\".format(pf=postfix), \"FTAJet{pf}_DeepCSVB_sorted.size() > 1 ? FTAJet{pf}_DeepCSVB_sorted.at(1) : -9999\".format(pf=postfix)))\n",
    "        z.append((\"FTAJet{pf}_DeepJetB\".format(pf=postfix), \"Jet_btagDeepFlavB[{jm}]\".format(jm=jetMask)))\n",
    "        z.append((\"FTAJet{pf}_DeepJetB_sorted\".format(pf=postfix), \"Take(FTAJet{pf}_DeepJetB, FTAJet{pf}_deepjetsort)\".format(pf=postfix)))\n",
    "        z.append((\"FTAJet{pf}_DeepJetB_sorted_LeadtagJet\".format(pf=postfix), \"FTAJet{pf}_DeepJetB_sorted.size() > 0 ? FTAJet{pf}_DeepJetB_sorted.at(0) : -9999\".format(pf=postfix)))\n",
    "        z.append((\"FTAJet{pf}_DeepJetB_sorted_SubleadtagJet\".format(pf=postfix), \"FTAJet{pf}_DeepJetB_sorted.size() > 1 ? FTAJet{pf}_DeepJetB_sorted.at(1) : -9999\".format(pf=postfix)))\n",
    "        for x in xrange(nJetsToHisto):\n",
    "            z.append((\"FTAJet{pf}_pt_jet{n}\".format(pf=postfix, n=x+1), \"FTAJet{pf}_pt.size() > {n} ? FTAJet{pf}_pt.at({n}) : -9999\".format(pf=postfix, n=x)))\n",
    "            z.append((\"FTAJet{pf}_eta_jet{n}\".format(pf=postfix, n=x+1), \"FTAJet{pf}_eta.size() > {n} ? FTAJet{pf}_phi.at({n}) : -9999\".format(pf=postfix, n=x)))\n",
    "            z.append((\"FTAJet{pf}_phi_jet{n}\".format(pf=postfix, n=x+1), \"FTAJet{pf}_phi.size() > {n} ? FTAJet{pf}_phi.at({n}) : -9999\".format(pf=postfix, n=x)))\n",
    "            z.append((\"FTAJet{pf}_DeepCSVB_jet{n}\".format(pf=postfix, n=x+1), \"FTAJet{pf}_DeepCSVB.size() > {n} ? FTAJet{pf}_DeepCSVB.at({n}) : -9999\".format(pf=postfix, n=x)))\n",
    "            z.append((\"FTAJet{pf}_DeepJetB_jet{n}\".format(pf=postfix, n=x+1), \"FTAJet{pf}_DeepJetB.size() > {n} ? FTAJet{pf}_DeepJetB.at({n}) : -9999\".format(pf=postfix, n=x)))\n",
    "            z.append((\"FTAJet{pf}_DeepCSVB_sortedjet{n}\".format(pf=postfix, n=x+1), \"FTAJet{pf}_DeepCSVB_sorted.size() > {n} ? FTAJet{pf}_DeepCSVB_sorted.at({n}) : -9999\".format(pf=postfix, n=x)))\n",
    "            z.append((\"FTAJet{pf}_DeepJetB_sortedjet{n}\".format(pf=postfix, n=x+1), \"FTAJet{pf}_DeepJetB_sorted.size() > {n} ? FTAJet{pf}_DeepJetB_sorted.at({n}) : -9999\".format(pf=postfix, n=x)))\n",
    "        z.append((\"FTAJet{pf}_LooseDeepCSVB\".format(pf=postfix), \"FTAJet{pf}_DeepCSVB[FTAJet{pf}_DeepCSVB > {wpv}]\".format(pf=postfix, wpv=bTagWorkingPointDict[era][\"DeepCSV\"][\"L\"])))\n",
    "        z.append((\"FTAJet{pf}_MediumDeepCSVB\".format(pf=postfix), \"FTAJet{pf}_DeepCSVB[FTAJet{pf}_DeepCSVB > {wpv}]\".format(pf=postfix, wpv=bTagWorkingPointDict[era][\"DeepCSV\"][\"M\"])))\n",
    "        z.append((\"FTAJet{pf}_TightDeepCSVB\".format(pf=postfix), \"FTAJet{pf}_DeepCSVB[FTAJet{pf}_DeepCSVB > {wpv}]\".format(pf=postfix, wpv=bTagWorkingPointDict[era][\"DeepCSV\"][\"T\"])))\n",
    "        z.append((\"nLooseDeepCSVB{pf}\".format(pf=postfix), \"FTAJet{pf}_LooseDeepCSVB.size()\".format(pf=postfix)))\n",
    "        z.append((\"nMediumDeepCSVB{pf}\".format(pf=postfix), \"FTAJet{pf}_MediumDeepCSVB.size()\".format(pf=postfix)))\n",
    "        z.append((\"nTightDeepCSVB{pf}\".format(pf=postfix), \"FTAJet{pf}_TightDeepCSVB.size()\".format(pf=postfix)))\n",
    "        z.append((\"FTAJet{pf}_LooseDeepJetB\".format(pf=postfix), \"FTAJet{pf}_DeepJetB[FTAJet{pf}_DeepJetB > {wpv}]\".format(pf=postfix, wpv=bTagWorkingPointDict[era][\"DeepJet\"][\"L\"])))\n",
    "        z.append((\"FTAJet{pf}_MediumDeepJetB\".format(pf=postfix), \"FTAJet{pf}_DeepJetB[FTAJet{pf}_DeepJetB > {wpv}]\".format(pf=postfix, wpv=bTagWorkingPointDict[era][\"DeepJet\"][\"M\"])))\n",
    "        z.append((\"FTAJet{pf}_TightDeepJetB\".format(pf=postfix), \"FTAJet{pf}_DeepJetB[FTAJet{pf}_DeepJetB > {wpv}]\".format(pf=postfix, wpv=bTagWorkingPointDict[era][\"DeepJet\"][\"T\"])))\n",
    "        z.append((\"nLooseDeepJetB{pf}\".format(pf=postfix), \"FTAJet{pf}_LooseDeepJetB.size()\".format(pf=postfix)))\n",
    "        z.append((\"nMediumDeepJetB{pf}\".format(pf=postfix), \"FTAJet{pf}_MediumDeepJetB.size()\".format(pf=postfix)))\n",
    "        z.append((\"nTightDeepJetB{pf}\".format(pf=postfix), \"FTAJet{pf}_TightDeepJetB.size()\".format(pf=postfix)))\n",
    "        #These might be more efficiently calculated with my own custom code, instead of this... well, lets try for the sake of experimentation\n",
    "        #HT is just the sum of good jet pts\n",
    "        # HT2M is the sum of jet pt's for all but the two highest-b-tagged jets (2016 analysis requires 4+ jets to define this quantity), so here Take() is used twice.\n",
    "        # The first call acquires the good jet pt's sorted by b-tagging, the senond Take() gets the last n-2 elements, thereby excluding the two highest b-tagged jet's pt\n",
    "        # HTRat = HT(two highest b-tagged) / HT, so it's useful to define this similarly to HT2M (and crosscheck that HTNum + HT2M = HT!)\n",
    "        # H and H2M are defined similarly for the overall momentum magnitude...\n",
    "        # P = pt/sin(theta) = pt * (1/sin(theta)) = pt * cosh(eta)\n",
    "        if useDeepCSV:\n",
    "            z.append((\"FTAJet{pf}_pt_bsrt\".format(pf=postfix), \"Take(FTAJet{pf}_pt, FTAJet{pf}_deepcsvsort)\".format(pf=postfix)))\n",
    "            z.append((\"FTAJet{pf}_eta_bsrt\".format(pf=postfix), \"Take(FTAJet{pf}_eta, FTAJet{pf}_deepcsvsort)\".format(pf=postfix)))\n",
    "            z.append((\"FTAJet{pf}_phi_bsrt\".format(pf=postfix), \"Take(FTAJet{pf}_phi, FTAJet{pf}_deepcsvsort)\".format(pf=postfix)))\n",
    "            z.append((\"FTAJet{pf}_mass_bsrt\".format(pf=postfix), \"Take(FTAJet{pf}_mass, FTAJet{pf}_deepcsvsort)\".format(pf=postfix)))\n",
    "        else:\n",
    "            z.append((\"FTAJet{pf}_pt_bsrt\".format(pf=postfix), \"Take(FTAJet{pf}_pt, FTAJet{pf}_deepjetsort)\".format(pf=postfix)))\n",
    "            z.append((\"FTAJet{pf}_eta_bsrt\".format(pf=postfix), \"Take(FTAJet{pf}_eta, FTAJet{pf}_deepjetsort)\".format(pf=postfix)))\n",
    "            z.append((\"FTAJet{pf}_phi_bsrt\".format(pf=postfix), \"Take(FTAJet{pf}_phi, FTAJet{pf}_deepjetsort)\".format(pf=postfix)))\n",
    "            z.append((\"FTAJet{pf}_mass_bsrt\".format(pf=postfix), \"Take(FTAJet{pf}_mass, FTAJet{pf}_deepjetsort)\".format(pf=postfix)))\n",
    "        z.append((\"FTAJet{pf}_P_bsrt\".format(pf=postfix), \"FTAJet{pf}_pt_bsrt * ROOT::VecOps::cosh(FTAJet{pf}_eta_bsrt)\".format(pf=postfix)))\n",
    "        z.append((\"HT{pf}\".format(pf=postfix), \"Sum(FTAJet{pf}_pt)\".format(pf=postfix)))\n",
    "        z.append((\"HT2M{pf}\".format(pf=postfix), \"FTAJet{pf}_pt_bsrt.size() > 2 ? Sum(Take(FTAJet{pf}_pt_bsrt, (2 - FTAJet{pf}_pt_bsrt.size()))) : -0.1\".format(pf=postfix)))\n",
    "        z.append((\"HTNum{pf}\".format(pf=postfix), \"FTAJet{pf}_pt_bsrt.size() > 2 ? Sum(Take(FTAJet{pf}_pt_bsrt, 2)) : -0.1\".format(pf=postfix)))\n",
    "        z.append((\"HTRat{pf}\".format(pf=postfix), \"FTAJet{pf}_pt_bsrt.size() > 2 ? (HT2M{pf} / HT{pf}) : -0.1\".format(pf=postfix)))\n",
    "        z.append((\"dRbb{pf}\".format(pf=postfix), \"FTAJet{pf}_pt_bsrt.size() > 2 ? ROOT::VecOps::DeltaR(FTAJet{pf}_eta_bsrt.at(0), FTAJet{pf}_eta_bsrt.at(1), FTAJet{pf}_phi_bsrt.at(0), FTAJet{pf}_phi_bsrt.at(1)) : -0.1\".format(pf=postfix)))\n",
    "        z.append((\"dPhibb{pf}\".format(pf=postfix), \"FTAJet{pf}_pt_bsrt.size() > 2 ? ROOT::VecOps::DeltaPhi(FTAJet{pf}_phi_bsrt.at(0), FTAJet{pf}_phi_bsrt.at(1)) : -999\".format(pf=postfix)))\n",
    "        z.append((\"dEtabb{pf}\".format(pf=postfix), \"FTAJet{pf}_pt_bsrt.size() > 2 ? abs(FTAJet{pf}_eta_bsrt.at(0) - FTAJet{pf}_eta_bsrt.at(1)) : -999\".format(pf=postfix)))\n",
    "        z.append((\"H{pf}\".format(pf=postfix), \"Sum(FTAJet{pf}_P_bsrt)\".format(pf=postfix)))\n",
    "        z.append((\"H2M{pf}\".format(pf=postfix), \"FTAJet{pf}_pt_bsrt.size() > 2 ? Sum(Take(FTAJet{pf}_P_bsrt, (2 - FTAJet{pf}_pt_bsrt.size()))) : -0.1\".format(pf=postfix)))\n",
    "        z.append((\"HTH{pf}\".format(pf=postfix), \"HT{pf}/H{pf}\".format(pf=postfix)))\n",
    "        if useDeepCSV:\n",
    "            z.append((\"HTb{pf}\".format(pf=postfix), \"Sum(FTAJet{pf}_pt[FTAJet{pf}_DeepCSVB > {wpv}])\".format(pf=postfix, wpv=bTagWorkingPointDict[era][\"DeepCSV\"][\"M\"])))\n",
    "        else:\n",
    "            z.append((\"HTb{pf}\".format(pf=postfix), \"Sum(FTAJet{pf}_pt[FTAJet{pf}_DeepJetB > {wpv}])\".format(pf=postfix, wpv=bTagWorkingPointDict[era][\"DeepJet\"][\"M\"])))\n",
    "        \n",
    "    rdf = input_df\n",
    "    listOfDefinedColumns = rdf.GetDefinedColumnNames()\n",
    "    for defName, defFunc in z:\n",
    "        if defName in listOfDefinedColumns:\n",
    "            if verbose:\n",
    "                print(\"{} already defined, skipping\".format(defName))\n",
    "            continue\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(\"rdf = rdf.Define(\\\"{}\\\", \\\"{}\\\")\".format(defName, defFunc))\n",
    "            rdf = rdf.Define(defName, defFunc)\n",
    "            listOfDefinedColumns.push_back(defName)\n",
    "                \n",
    "    return rdf\n",
    "        #rdf = rdf.Define(\"Jet_idx\", \"FTA::generateIndices(Jet_pt)\")#G\n",
    "        #rdf = rdf.Define(\"{}\".format(jetMask), \"({jpt} >= 30 && abs(Jet_eta) <= 2.5 && Jet_jetId > 2 && Jet_idx != GLepton_jetIdx_0 && Jet_idx != GLepton_jetIdx_1)\".format(jpt=jetPt))#G\n",
    "        #rdf = rdf.Define(\"jet{}_ptsort\".format(postfix), \"Reverse(Argsort({jpt}[{jm}]))\".format(jpt=jetPt, jm=jetMask))#G\n",
    "        #rdf = rdf.Define(\"jet{}_deepcsvsort\".format(postfix), \"Reverse(Argsort(Jet_{btv}[{jm}]))\".format(btv=bTagWorkingPointDict[era][\"DeepCSV\"][\"Var\"], jetMask))#G\n",
    "        #rdf = rdf.Define(\"jet{}_deepjetsort\".format(postfix), \"Reverse(Argsort(Jet_{btv}[{jm}]))\".format(btv=bTagWorkingPointDict[era][\"DeepJet\"][\"Var\"], jetMask))#G\n",
    "        #rdf = rdf.Define(\"Jet{}_idx\".format(postfix), \"Jet_idx[{jm}]\".format(jm=jetMask))#G\n",
    "        #rdf = rdf.Define(\"Jet{}_pt\".format(postfix), \"{jpt}[{jm}]\".format(jpt=jetPt, jm=jetMask))#G\n",
    "        #rdf = rdf.Define(\"Jet{}_eta\".format(postfix), \"Jet_eta[{jm}]\".format(jm=jetMask))#G\n",
    "        #rdf = rdf.Define(\"Jet{}_phi\".format(postfix), \"Jet_phi[{jm}]\".format(jm=jetMask))#G\n",
    "        #rdf = rdf.Define(\"Jet{}_mass\".format(postfix), \"{jms}[{jm}]\".format(jms=jetMass, jm=jetMask))#G\n",
    "        #rdf = rdf.Define(\"Jet{}_jetId\".format(postfix), \"Jet_jetId[{jm}]\".format(jm=jetMask))#G\n",
    "        #rdf = rdf.Define(\"Jet{}_puId\".format(postfix), \"Jet_puId[{jm}]\".format(jm=jetMask))#G\n",
    "        #if isData is False:\n",
    "        #    rdf = rdf.Define(\"Jet{}_genJetIdx\".format(postfix), \"Jet_genJetIdx[jet_mask]\")\n",
    "        #    rdf = rdf.Define(\"nJet{}_genMatched\".format(postfix), \"Jet{}_genJetIdx[Jet{}_genJetIdx >= 0].size()\")\n",
    "        #    rdf = rdf.Define(\"nJet{}_puIdLoose\".format(postfix), \"Jet{}_genJetIdx[(Jet{}_puId >= 4 || Jet{}_pt >= 50)].size()\")\n",
    "        #    rdf = rdf.Define(\"nJet{}_genMatched_puIdLoose\".format(postfix), \"Jet{}_genJetIdx[Jet{}_genJetIdx >= 0 && (Jet{}_puId >= 4 || Jet{}_pt >= 50)].size()\")\n",
    "        #rdf = rdf.Define(\"nJet{}\".format(postfix), \"Jet{}_pt.size()\".format(postfix))#G, don't want normal jetPt input here!\n",
    "        #rdf = rdf.Define(\"Jet{}_btagDeepB\", \"Jet_btagDeepB[{jm}]\".format(jm=jetMask))#G\n",
    "        #rdf = rdf.Define(\"Jet{}_btagDeepB_sorted\", \"Take(Jet_btagDeepB[jet_mask], jet_deepcsvsort)\")\n",
    "        #rdf = rdf.Define(\"Jet{}_btagDeepFlavB_sorted_LeadtagJet\", \"Jet{}_btagDeepB_sorted.size() > 0 ? Jet{}_btagDeepB_sorted.at(0) : -9999\")\n",
    "        #rdf = rdf.Define(\"Jet{}_btagDeepFlavB_sorted_SubleadtagJet\", \"Jet{}_btagDeepB_sorted.size() > 1 ? Jet{}_btagDeepB_sorted.at(1) : -9999\")\n",
    "        #rdf = rdf.Define(\"Jet{}_btagDeepB_LeadtagJet\", \"Jet{}_btagDeepB.size() > 0 ? Reverse(Sort(Jet{}_btagDeepB)).at(0) : -0.000000000001\")\n",
    "        #rdf = rdf.Define(\"Jet{}_btagDeepB_SubleadtagJet\", \"Jet{}_btagDeepB.size() > 1 ? Reverse(Sort(Jet{}_btagDeepB)).at(1) : -0.000000000001\")\n",
    "        #rdf = rdf.Define(\"Jet{}_btagDeepFlavB\", \"Jet_btagDeepFlavB[jet_mask]\")\n",
    "        #rdf = rdf.Define(\"Jet{}_btagDeepFlavB_sorted\", \"Take(Jet_btagDeepFlavB[jet_mask], jet_deepjetsort)\")\n",
    "        #rdf = rdf.Define(\"Jet{}_btagDeepFlavB_sorted_LeadtagJet\", \"Jet{}_btagDeepFlavB_sorted.size() > 0 ? Jet{}_btagDeepFlavB_sorted.at(0) : -9999\")\n",
    "        #rdf = rdf.Define(\"Jet{}_btagDeepFlavB_sorted_SubleadtagJet\", \"Jet{}_btagDeepFlavB_sorted.size() > 1 ? Jet{}_btagDeepFlavB_sorted.at(1) : -9999\")\n",
    "        #for x in xrange(nJetsToHisto):\n",
    "        #    rdf = rdf.Define(\"Jet{}_pt_jet{}\".format(x+1), \"Jet{}_pt.size() > {} ? Jet{}_pt.at({}) : -9999\".format(x, x))\n",
    "        #    rdf = rdf.Define(\"Jet{}_eta_jet{}\".format(x+1), \"Jet{}_eta.size() > {} ? Jet{}_phi.at({}) : -9999\".format(x, x))\n",
    "        #    rdf = rdf.Define(\"Jet{}_phi_jet{}\".format(x+1), \"Jet{}_phi.size() > {} ? Jet{}_phi.at({}) : -9999\".format(x, x))\n",
    "        #    rdf = rdf.Define(\"Jet{}_DeepCSV_jet{}\".format(x+1), \"Jet{}_btagDeepB.size() > {} ? Jet{}_btagDeepB.at({}) : -9999\".format(x, x))\n",
    "        #    rdf = rdf.Define(\"Jet{}_DeepJet_jet{}\".format(x+1), \"Jet{}_btagDeepFlavB.size() > {} ? Jet{}_btagDeepFlavB.at({}) : -9999\".format(x, x))\n",
    "        #    rdf = rdf.Define(\"Jet{}_DeepCSV_sortedjet{}\".format(x+1), \"Jet{}_btagDeepB_sorted.size() > {} ? Jet{}_btagDeepB_sorted.at({}) : -9999\".format(x, x))\n",
    "        #    rdf = rdf.Define(\"Jet{}_DeepJet_sortedjet{}\".format(x+1), \"Jet{}_btagDeepFlavB_sorted.size() > {} ? Jet{}_btagDeepFlavB_sorted.at({}) : -9999\".format(x, x))\n",
    "        #rdf = rdf.Define(\"Jet{}_LooseDeepCSV\", \"Jet{}_{0}[Jet{}_{0} > {1}]\".format(bTagWorkingPointDict[era][\"DeepCSV\"][\"Var\"],\n",
    "        #                                                                        bTagWorkingPointDict[era][\"DeepCSV\"][\"L\"]))\n",
    "        #rdf = rdf.Define(\"Jet{}_MediumDeepCSV\", \"Jet{}_{0}[Jet{}_{0} > {1}]\".format(bTagWorkingPointDict[era][\"DeepCSV\"][\"Var\"],\n",
    "        #                                                                        bTagWorkingPointDict[era][\"DeepCSV\"][\"M\"]))\n",
    "        #rdf = rdf.Define(\"Jet{}_TightDeepCSV\", \"Jet{}_{0}[Jet{}_{0} > {1}]\".format(bTagWorkingPointDict[era][\"DeepCSV\"][\"Var\"],\n",
    "        #                                                                        bTagWorkingPointDict[era][\"DeepCSV\"][\"T\"]))\n",
    "        #rdf = rdf.Define(\"nJet{}_LooseDeepCSV\", \"Jet{}_LooseDeepCSV.size()\")\n",
    "        #rdf = rdf.Define(\"nJet{}_MediumDeepCSV\", \"Jet{}_MediumDeepCSV.size()\")\n",
    "        #rdf = rdf.Define(\"nJet{}_TightDeepCSV\", \"Jet{}_TightDeepCSV.size()\")\n",
    "        #rdf = rdf.Define(\"Jet{}_LooseDeepJet\", \"Jet{}_{0}[Jet{}_{0} > {1}]\".format(bTagWorkingPointDict[era][\"DeepJet\"][\"Var\"],\n",
    "        #                                                                        bTagWorkingPointDict[era][\"DeepJet\"][\"L\"]))\n",
    "        #rdf = rdf.Define(\"Jet{}_MediumDeepJet\", \"Jet{}_{0}[Jet{}_{0} > {1}]\".format(bTagWorkingPointDict[era][\"DeepJet\"][\"Var\"],\n",
    "        #                                                                        bTagWorkingPointDict[era][\"DeepJet\"][\"M\"]))\n",
    "        #rdf = rdf.Define(\"Jet{}_TightDeepJet\", \"Jet{}_{0}[Jet{}_{0} > {1}]\".format(bTagWorkingPointDict[era][\"DeepJet\"][\"Var\"],\n",
    "        #                                                                        bTagWorkingPointDict[era][\"DeepJet\"][\"T\"]))\n",
    "        #rdf = rdf.Define(\"nJet{}_LooseDeepJet\", \"Jet{}_LooseDeepJet.size()\")\n",
    "        #rdf = rdf.Define(\"nJet{}_MediumDeepJet\", \"Jet{}_MediumDeepJet.size()\")\n",
    "        #rdf = rdf.Define(\"nJet{}_TightDeepJet\", \"Jet{}_TightDeepJet.size()\")\n",
    "        #These might be more efficiently calculated with my own custom code, instead of this... well, lets try for the sake of experimentation\n",
    "        #HT is just the sum of good jet pts\n",
    "        # HT2M is the sum of jet pt's for all but the two highest-b-tagged jets (2016 analysis requires 4+ jets to define this quantity), so here Take() is used twice.\n",
    "        # The first call acquires the good jet pt's sorted by b-tagging, the senond Take() gets the last n-2 elements, thereby excluding the two highest b-tagged jet's pt\n",
    "        # HTRat = HT(two highest b-tagged) / HT, so it's useful to define this similarly to HT2M (and crosscheck that HTNum + HT2M = HT!)\n",
    "        # H and H2M are defined similarly for the overall momentum magnitude...\n",
    "        # P = pt/sin(theta) = pt * (1/sin(theta)) = pt * cosh(eta)\n",
    "        #if useDeepCSV:\n",
    "        #    rdf = rdf.Define(\"Jet{}_pt_bsrt\", \"Take(Jet{}_pt, jet_deepcsvsort)\")\n",
    "        #    rdf = rdf.Define(\"Jet{}_eta_bsrt\", \"Take(Jet{}_eta, jet_deepcsvsort)\")\n",
    "        #    rdf = rdf.Define(\"Jet{}_phi_bsrt\", \"Take(Jet{}_phi, jet_deepcsvsort)\")\n",
    "        #else:\n",
    "        #    rdf = rdf.Define(\"Jet{}_pt_bsrt\", \"Take(Jet{}_pt, jet_deepjetsort)\")\n",
    "        #    rdf = rdf.Define(\"Jet{}_eta_bsrt\", \"Take(Jet{}_eta, jet_deepjetsort)\")\n",
    "        #    rdf = rdf.Define(\"Jet{}_phi_bsrt\", \"Take(Jet{}_phi, jet_deepjetsort)\")\n",
    "        #rdf = rdf.Define(\"Jet{}_P_bsrt\", \"Jet{}_pt_bsrt * ROOT::VecOps::cosh(Jet{}_eta_bsrt)\")\n",
    "        #rdf = rdf.Define(\"Jet{}_HT\", \"Sum(Jet{}_pt)\")\n",
    "        #rdf = rdf.Define(\"Jet{}_HT2M\", \"Jet{}_pt_bsrt.size() > 2 ? Sum(Take(Jet{}_pt_bsrt, (2 - Jet{}_pt_bsrt.size()))) : -0.1\")\n",
    "        #rdf = rdf.Define(\"Jet{}_HTNum\", \"Jet{}_pt_bsrt.size() > 2 ? Sum(Take(Jet{}_pt_bsrt, 2)) : -0.1\")\n",
    "        #rdf = rdf.Define(\"Jet{}_HTRat\", \"Jet{}_pt_bsrt.size() > 2 ? (Jet{}_HT2M / Jet{}_HT) : -0.1\")\n",
    "        #rdf = rdf.Define(\"Jet{}_dRbb\", \"Jet{}_pt_bsrt.size() > 2 ? ROOT::VecOps::DeltaR(Jet{}_eta_bsrt.at(0), Jet{}_eta_bsrt.at(1), Jet{}_phi_bsrt.at(0), Jet{}_phi_bsrt.at(1)) : -0.1\")\n",
    "        #rdf = rdf.Define(\"Jet{}_dPhibb\", \"Jet{}_pt_bsrt.size() > 2 ? ROOT::VecOps::DeltaPhi(Jet{}_phi_bsrt.at(0), Jet{}_phi_bsrt.at(1)) : -999\")\n",
    "        #rdf = rdf.Define(\"Jet{}_dEtabb\", \"Jet{}_pt_bsrt.size() > 2 ? abs(Jet{}_eta_bsrt.at(0) - Jet{}_eta_bsrt.at(1)) : -999\")\n",
    "        #rdf = rdf.Define(\"Jet{}_H\", \"Sum(Jet{}_P_bsrt)\")\n",
    "        #rdf = rdf.Define(\"Jet{}_H2M\", \"Jet{}_pt_bsrt.size() > 2 ? Sum(Take(Jet{}_P_bsrt, (2 - Jet{}_pt_bsrt.size()))) : -0.1\")\n",
    "        #rdf = rdf.Define(\"Jet{}_HTH\", \"Jet{}_HT/Jet{}_H\")\n",
    "        #rdf = rdf.Define(\"Jet{}_HTb\", \"Sum(Jet{}_pt[Jet{}_{0} > {1}])\".format(bTagWorkingPointDict[era][\"DeepJet\"][\"Var\"],\n",
    "        #                                                                        bTagWorkingPointDict[era][\"DeepJet\"][\"M\"]))\n",
    "        #if debugInfo == True:\n",
    "        #    rdf = rdf.Define(\"Jet{}_ptALT\", \"Jet_pt[jet_maskALT]\")\n",
    "        #    rdf = rdf.Define(\"Jet{}_etaALT\", \"Jet_eta[jet_maskALT]\")\n",
    "        #    rdf = rdf.Define(\"Jet{}_phiALT\", \"Jet_phi[jet_maskALT]\")\n",
    "        #    rdf = rdf.Define(\"Jet{}_massALT\", \"Jet_mass[jet_maskALT]\")\n",
    "        #    rdf = rdf.Define(\"Jet{}_jetIdALT\", \"Jet_jetId[jet_maskALT]\")\n",
    "        #    rdf = rdf.Define(\"DiffMaskVsALT\", \"Jet{}_ptALT.size() - Jet{}_pt.size()\")\n",
    "        #    rdf = rdf.Define(\"DiffnJet\", \"nGJet - ESV_JetMETLogic_nJet_selection\")\n",
    "        #    rdf = rdf.Define(\"dR_Jet_Mu_leading\", \"GLepton_jetIdx_0 > -1 && abs(GLepton_pdgId.at(0)) == 13 ? ROOT::VecOps::DeltaR(Jet_eta.at(GLepton_jetIdx_0), GLepton_eta.at(0), Jet_phi.at(GLepton_jetIdx_0), GLepton_phi.at(0)) : -0.01\")\n",
    "        #    rdf = rdf.Define(\"dR_Jet_Mu_sublead\", \"GLepton_jetIdx_1 > -1 && abs(GLepton_pdgId.at(1)) == 13 ? ROOT::VecOps::DeltaR(Jet_eta.at(GLepton_jetIdx_1), GLepton_eta.at(1), Jet_phi.at(GLepton_jetIdx_1), GLepton_phi.at(1)) : -0.01\")\n",
    "        #    rdf = rdf.Define(\"dR_Jet_El_leading\", \"GLepton_jetIdx_0 > -1 && abs(GLepton_pdgId.at(1)) == 11 ? ROOT::VecOps::DeltaR(Jet_eta.at(GLepton_jetIdx_0), GLepton_eta.at(0), Jet_phi.at(GLepton_jetIdx_0), GLepton_phi.at(0)) : -0.01\")\n",
    "        #    rdf = rdf.Define(\"dR_Jet_El_sublead\", \"GLepton_jetIdx_1 > -1 && abs(GLepton_pdgId.at(1)) == 11 ? ROOT::VecOps::DeltaR(Jet_eta.at(GLepton_jetIdx_1), GLepton_eta.at(1), Jet_phi.at(GLepton_jetIdx_1), GLepton_phi.at(1)) : -0.01\")\n",
    "        #    #rdf = rdf.Define(\"dR_Jet_lep0\", \"GLepton_jetIdx_0 > -1 ? ROOT::VecOps::DeltaR(Jet_eta.at(GLepton_jetIdx_0), GLepton_eta.at(0), Jet_phi.at(GLepton_jetIdx_0), GLepton_phi.at(0)) : -0.01\")\n",
    "        #    #rdf = rdf.Define(\"dR_Jet_lep1\", \"GLepton_jetIdx_1 > -1 ? ROOT::VecOps::DeltaR(Jet_eta.at(GLepton_jetIdx_1), GLepton_eta.at(1), Jet_phi.at(GLepton_jetIdx_1), GLepton_phi.at(1)) : -0.01\")\n",
    "        #    rdf = rdf.Define(\"Jet{}_btagDeepFlavB_jet0Med\", \"Jet{}_MediumDeepJet.size() > 0 ? Reverse(Sort(Jet{}_MediumDeepJet)).at(0) : -0.000000000001\")\n",
    "        #    rdf = rdf.Define(\"Jet{}_btagDeepFlavB_jet1Med\", \"Jet{}_MediumDeepJet.size() > 1 ? Reverse(Sort(Jet{}_MediumDeepJet)).at(1) : -0.000000000001\")\n",
    "        #    rdf = rdf.Define(\"DeepJetSorted\", \"Jet{}_btagDeepFlavB_sorted.size() > 1 ? (Jet{}_btagDeepFlavB_sorted.at(0) >= Jet{}_btagDeepFlavB_sorted.at(1)) : true\")\n",
    "        #    rdf = rdf.Define(\"DeepJet0Minus1\", \"Jet{}_btagDeepFlavB_sorted.size() > 1 ? (Jet{}_btagDeepFlavB_sorted.at(0) - Jet{}_btagDeepFlavB_sorted.at(1)) : 0\")\n",
    "        #    rdf = rdf.Define(\"MediumDeepJetSorted\", \"Jet{}_MediumDeepJet.size() > 1 ? (Reverse(Sort(Jet{}_MediumDeepJet)).at(0) >= Reverse(Sort(Jet{}_MediumDeepJet)).at(1)) : true\")\n",
    "        #    rdf = rdf.Define(\"MediumDeepJet0Minus1\", \"Jet{}_MediumDeepJet.size() > 1 ? (Reverse(Sort(Jet{}_MediumDeepJet)).at(0) - Reverse(Sort(Jet{}_MediumDeepJet)).at(1)) : 0\")\n",
    "        #return rdf\n",
    "        #Code taht doesn't work...\n",
    "        #Can see that the jets are in fact not sorted when calling Reverse(Jet{}_MediumDeepJet).at(0), for example, as the one .at(1) can sometimes not be smaller\n",
    "        #Looking at the definition makes it obvious, because Reverse is not short for \"ReverseSort\" but is literally just std::reverse. Must call (Arg)sort first...\n",
    "        #Definig a functor in the string like this doesn't work either:\n",
    "        #.Define(\"Jet{}_btagDeepB_jet0\", \"Jet{}_btagDeepB.size() > 0 ? Sort(Jet{}_btagDeepB, [](double x, double y) {return x > y;}).at(0) : -0.000000000001\")\\\n",
    "        #Cannot use ternary operator with RVec and double return types (Take(...) : -0.0000000001)\n",
    "        #.Define(\"Jet{}_btagDeepFlavB_jet0\", \"Jet{}_btagDeepFlavB.size() > 0 ? Take(Reverse(Jet{}_btagDeepFlavB), {0}) : -0.000000000001\")\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def defineFinalWeights(input_df, isData=False, verbose=False):\n",
    "    \"\"\"Define all the final weights and the variations, to be referened by the sysVariations dictionaries as wgt_final\"\"\"\n",
    "    rdf = input_df\n",
    "    leppostfix = \"_nom\"\n",
    "    z = []\n",
    "    z.append((\"pwgt_XS\", \"wgt_SUMW\")) #alias this until it's better defined here or elsewhere\n",
    "    z.append((\"pwgt_LSF_nom\", \"(FTALepton{lpf}_SF_nom.size() > 1 ? FTALepton{lpf}_SF_nom.at(0) * FTALepton{lpf}_SF_nom.at(1) : FTALepton{lpf}_SF_nom.at(0))\".format(lpf=leppostfix)))\n",
    "    \n",
    "    #WARNING: on btag weights, it can ALWAYS be 'varied' to match the systematic, so that the event weight from\n",
    "    #the correct jet collection, btag SFs, and yields is used. Always match! This duplicates some calculations uselessly\n",
    "    #in the BtaggingYields function, but it should help avoid mistakes at the level of final calculations\n",
    "    \n",
    "    #Nominal weight\n",
    "    z.append((\"wgt_nom\", \"pwgt_XS * puWeight * L1PreFiringWeight_Nom * pwgt_LSF_nom * pwgt_btag_nom\"))\n",
    "    \n",
    "    #JES Up and Down - effectively the nominal weight, but with the CORRECT btag weight for those jets!\n",
    "    z.append((\"wgt_jesTotalDown\", \"pwgt_XS * puWeight * L1PreFiringWeight_Nom * pwgt_LSF_nom * pwgt_btag_jesTotalDown\"))\n",
    "    z.append((\"wgt_jesTotalUp\", \"pwgt_XS * puWeight * L1PreFiringWeight_Nom * pwgt_LSF_nom * pwgt_btag_jesTotalUp\"))\n",
    "    \n",
    "    #Pileup variations \n",
    "    print(\"FIXME: Using temporary definition of weights for PU variations (change pwgt_btag_VARIATION)\")\n",
    "    z.append((\"wgt_puWeightDown\", \"pwgt_XS * puWeightDown * L1PreFiringWeight_Nom * pwgt_LSF_nom * pwgt_btag_nom\"))\n",
    "    z.append((\"wgt_puWeightUp\", \"pwgt_XS * puWeightUp * L1PreFiringWeight_Nom * pwgt_LSF_nom * pwgt_btag_nom\"))\n",
    "    #z.append((\"wgt_puWeightDown\", \"pwgt_XS * puWeightDown * L1PreFiringWeight_Nom * pwgt_LSF_nom * pwgt_btag_puWeightDown\"))\n",
    "    #z.append((\"wgt_puWeightUp\", \"pwgt_XS * puWeightUp * L1PreFiringWeight_Nom * pwgt_LSF_nom * pwgt_btag_puWeightUp\"))\n",
    "    \n",
    "    #L1 PreFiring variations\n",
    "    print(\"FIXME: Using temporary definition of weights for L1PreFire variations (change pwgt_btag_VARIATION)\")\n",
    "    z.append((\"wgt_L1PreFireDown\", \"pwgt_XS * puWeight * L1PreFiringWeight_Dn * pwgt_LSF_nom * pwgt_btag_nom\"))\n",
    "    z.append((\"wgt_L1PreFireUp\", \"pwgt_XS * puWeight * L1PreFiringWeight_Up * pwgt_LSF_nom * pwgt_btag_nom\"))\n",
    "    #z.append((\"wgt_L1PreFireDown\", \"pwgt_XS * puWeight * L1PreFiringWeight_Dn * pwgt_LSF_nom * pwgt_btag_L1PreFireDown\"))\n",
    "    #z.append((\"wgt_L1PreFireUp\", \"pwgt_XS * puWeight * L1PreFiringWeight_Up * pwgt_LSF_nom * pwgt_btag_L1PreFireUp\"))\n",
    "    \n",
    "    #Pure BTagging variations, no other variations necessary\n",
    "    z.append((\"wgt_btagSF_deepcsv_shape_down_hf\", \"pwgt_XS * puWeight * L1PreFiringWeight_Nom * pwgt_LSF_nom * pwgt_btag_btagSF_deepcsv_shape_down_hf\"))\n",
    "    z.append((\"wgt_btagSF_deepcsv_shape_up_hf\", \"pwgt_XS * puWeight * L1PreFiringWeight_Nom * pwgt_LSF_nom * pwgt_btag_btagSF_deepcsv_shape_up_hf\"))\n",
    "    z.append((\"wgt_btagSF_deepcsv_shape_down_lf\", \"pwgt_XS * puWeight * L1PreFiringWeight_Nom * pwgt_LSF_nom * pwgt_btag_btagSF_deepcsv_shape_down_lf\"))\n",
    "    z.append((\"wgt_btagSF_deepcsv_shape_up_lf\", \"pwgt_XS * puWeight * L1PreFiringWeight_Nom * pwgt_LSF_nom * pwgt_btag_btagSF_deepcsv_shape_up_lf\"))\n",
    "    \n",
    "    #Factorization/Renormalization weights... depend on dividing genWeight back out?\n",
    "    #TBD x4 for top samples, MAYBE NOT FOR OTHERS! (Until \"Run II Legacy\" samples are being used)\n",
    "    \n",
    "    \n",
    "    if isData:\n",
    "        rdf = rdf.Define(\"wgt_nom\", \"1\")\n",
    "    else:\n",
    "        listOfDefinedColumns = rdf.GetDefinedColumnNames()\n",
    "        for defName, defFunc in z:\n",
    "            if defName in listOfDefinedColumns:\n",
    "                if verbose:\n",
    "                    print(\"{} already defined, skipping\".format(defName))\n",
    "                continue\n",
    "            else:\n",
    "                if verbose:\n",
    "                    print(\"rdf = rdf.Define(\\\"{}\\\", \\\"{}\\\")\".format(defName, defFunc))\n",
    "                rdf = rdf.Define(defName, defFunc)\n",
    "                listOfDefinedColumns.push_back(defName) \n",
    "    return rdf\n",
    "    #cartesian_product_list(name_format=\"$NUM_$LET_$SYM\", \n",
    "    #                       name_tuples=[(\"$XS\", [\"1\", \"2\"]),\n",
    "    #                                    (\"$LEP_SF\", [\"A\", \"B\", \"C\"]),\n",
    "    #                                    (\"$BTAG_SF\", [\"*\", \"@\"])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BtaggingYields(input_df, sampleName, isData = True, histos_dict=None, verbose=False,\n",
    "                   sysVariations={\"$NOMINAL\": {\"jet_mask\": \"jet_mask\", \n",
    "                                             \"wgt_prebTag\": \"wgt_SUMW_PU_LSF_L1PF\",\n",
    "                                             \"jet_pt_var\": \"Jet_pt\",\n",
    "                                             \"jet_mass_var\": \"Jet_mass\",\n",
    "                                             \"met_pt_var\": \"METFixEE2017_pt\",\n",
    "                                             \"met_phi_var\": \"METFixEE2017_phi\",\n",
    "                                             \"btagSF\": \"Jet_btagSF_deepcsv_shape\",\n",
    "                                             \"weightVariation\": False},\n",
    "                              \"_jesTotalUp\": {\"jet_mask\": \"jet_mask_jesTotalUp\", \n",
    "                                             \"wgt_prebTag\": \"wgt_SUMW_PU_LSF_L1PF\",\n",
    "                                             \"jet_pt_var\": \"Jet_pt_jesTotalUp\",\n",
    "                                             \"jet_mass_var\": \"Jet_mass_jesTotalUp\",\n",
    "                                             \"met_pt_var\": \"METFixEE2017_pt_jesTotalUp\",\n",
    "                                             \"met_phi_var\": \"METFixEE2017_phi_jesTotalUp\",\n",
    "                                             \"btagSF\": \"Jet_btagSF_deepcsv_shape\",\n",
    "                                             \"weightVariation\": False},\n",
    "                              \"_jesTotalDown\": {\"jet_mask\": \"jet_mask_jesTotalDown\", \n",
    "                                             \"wgt_prebTag\": \"wgt_SUMW_PU_LSF_L1PF\",\n",
    "                                             \"jet_pt_var\": \"Jet_pt_jesTotalDown\",\n",
    "                                             \"jet_mass_var\": \"Jet_mass_jesTotalDown\",\n",
    "                                             \"met_pt_var\": \"METFixEE2017_pt_jesTotalDown\",\n",
    "                                             \"met_phi_var\": \"METFixEE2017_phi_jesTotalDown\",\n",
    "                                             \"btagSF\": \"Jet_btagSF_deepcsv_shape\",\n",
    "                                             \"weightVariation\": False},\n",
    "                                  \"_btagSF_deepcsv_shape_up_hf\": {\"jet_mask\": \"jet_mask\", \n",
    "                                                                 \"wgt_prebTag\": \"wgt_SUMW_PU_LSF_L1PF\", \n",
    "                                                                 \"jet_pt_var\": \"Jet_pt\",\n",
    "                                                                 \"btagSF\": \"Jet_btagSF_deepcsv_shape_up_hf\",\n",
    "                                                                 \"weightVariation\": True},\n",
    "                                  \"_btagSF_deepcsv_shape_down_hf\": {\"jet_mask\": \"jet_mask\", \n",
    "                                                                   \"wgt_prebTag\": \"wgt_SUMW_PU_LSF_L1PF\", \n",
    "                                                                   \"jet_pt_var\": \"Jet_pt\",\n",
    "                                                                   \"btagSF\": \"Jet_btagSF_deepcsv_shape_down_hf\",\n",
    "                                                                   \"weightVariation\": True},\n",
    "                                  \"_btagSF_deepcsv_shape_up_lf\": {\"jet_mask\": \"jet_mask\", \n",
    "                                                                 \"wgt_prebTag\": \"wgt_SUMW_PU_LSF_L1PF\", \n",
    "                                                                 \"jet_pt_var\": \"Jet_pt\",\n",
    "                                                                 \"btagSF\": \"Jet_btagSF_deepcsv_shape_up_lf\",\n",
    "                                                                 \"weightVariation\": True},\n",
    "                                  \"_btagSF_deepcsv_shape_down_lf\": {\"jet_mask\": \"jet_mask\", \n",
    "                                                                   \"wgt_prebTag\": \"wgt_SUMW_PU_LSF_L1PF\", \n",
    "                                                                   \"jet_pt_var\": \"Jet_pt\",\n",
    "                                                                   \"btagSF\": \"Jet_btagSF_deepcsv_shape_down_lf\",\n",
    "                                                                   \"weightVariation\": True},\n",
    "                                              },\n",
    "                   loadYields=None,\n",
    "                   lookupMap=\"LUM\",\n",
    "                   useAggregate=True,\n",
    "                   calculateYields=True,\n",
    "                   HTBinWidth=10, HTMin=200, HTMax=3200,\n",
    "                   nJetBinWidth=1, nJetMin=4, nJetMax=20):\n",
    "    \"\"\"Calculate or load the event yields in various categories for the purpose of renormalizing btagging shape correction weights.\n",
    "    \n",
    "    A btagging preweight (event level) must be calculated using the product of all SF(discriminant, pt, eta) for\n",
    "    all selected jets. Then a ratio of the sum(weights before)/sum(weights after) for application of this btagging \n",
    "    preweight serves as a renormalization, and this phase space extrapolation can be a function of multiple variables.\n",
    "    Minimally, for high-jet multiplicity analyses, it can be expected to depend on nJet. The final btagging event weight\n",
    "    is then the product of this phase space ratio and the btagging preweight. This should be computed PRIOR to ANY\n",
    "    btagging requirements; after, the event yields and shapes are expected to shift.\n",
    "    \n",
    "    The final and preweight are computed separately from the input weight (that is, it must be multiplied with the non-btagging event weight)\n",
    "    The yields are calculated as the sum of weights before and after multiplying the preweight with the input weight\n",
    "    https://twiki.cern.ch/twiki/bin/viewauth/CMS/BTagShapeCalibration\"\"\"\n",
    "                        \n",
    "    JetBins = int((nJetMax - nJetMin)/nJetBinWidth)\n",
    "    HTBins = int((HTMax-HTMin)/HTBinWidth)\n",
    "    \n",
    "    #internal variable/pointer to the TH2 lookup map, and the sample-specific one\n",
    "    iLUM = None\n",
    "    if loadYields != None:\n",
    "        calculateYields = False\n",
    "        #We need a lookupMap to store the TH2Lookup class with the yields loaded in them,\n",
    "        #With key1 based on the sample name and key2 based on slot number in the RDataFrame\n",
    "        #We need the string name for the object in the ROOT space, creating it if necessary\n",
    "        if type(lookupMap) == str:\n",
    "            #It's a string name, see if it's been declared in the ROOT instance and if not create it\n",
    "            try:\n",
    "                if str(type(getattr(ROOT, lookupMap))) == \"<class 'ROOT.map<string,vector<TH2Lookup*> >'>\":\n",
    "                    #We'll pick it up after the except statement\n",
    "                    pass\n",
    "            except:\n",
    "                ROOT.gInterpreter.Declare(\"std::map<std::string, std::vector<TH2Lookup*>> {0};\".format(lookupMap))\n",
    "            iLUM = getattr(ROOT, lookupMap)\n",
    "        else:\n",
    "            raise RuntimeError(\"lookupMap (used in BtaggingYields function) must either be a string name \"\\\n",
    "                               \"for the declared function's (C++ variable) name, used to find or declare one of type \"\\\n",
    "                               \"std::map<std::string, std::vector<TH2Lookup*>>\")\n",
    "        nSlots = input_df.GetNSlots()\n",
    "        while iLUM[sampleName].size() < nSlots:\n",
    "            if type(loadYields) == str:\n",
    "                iLUM[sampleName].push_back(ROOT.TH2Lookup(loadYields))\n",
    "            elif loadYields == True:\n",
    "                iLUM[sampleName].push_back(ROOT.TH2Lookup(\"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/BTaggingYields.root\"))\n",
    "    \n",
    "    #column guards\n",
    "    listOfColumns = input_df.GetDefinedColumnNames()\n",
    "    listOfDefinedColumns = input_df.GetColumnNames() #This is a superset, containing non-Define'd columns as well\n",
    "\n",
    "    if isData == True:\n",
    "        return input_df\n",
    "    else:\n",
    "        rdf = input_df\n",
    "        #List of defines to do\n",
    "        z = {}\n",
    "        #Create list of the variations to be histogrammed (2D yields)\n",
    "        yieldList = []\n",
    "        #Add key to histos dictionary, if calculating the yields\n",
    "        if calculateYields and \"BtaggingYields\" not in histos_dict.keys():\n",
    "            histos_dict[\"BtaggingYields\"] = {}\n",
    "        for sysVar, sysDict in sysVariations.items():\n",
    "            z[sysVar] = []\n",
    "            isWeightVariation = sysDict.get(\"weightVariation\")\n",
    "            branchpostfix = \"_nom\" if isWeightVariation else sysVar.replace(\"$NOMINAL\", \"_nom\") #branch postfix for \n",
    "            syspostfix = sysVar.replace(\"$NOMINAL\", \"_nom\")\n",
    "            jetMask = sysDict.get(\"jet_mask\") #mask as defined for the jet collection under this systematic variation\n",
    "            jetPt = sysDict.get(\"jet_pt_var\") #colum name of jet pt collection for this systematic\n",
    "            jetSF = sysDict.get(\"btagSF\") #colum name of per-jet shape SFs\n",
    "            #We must get or calculate various weights, defined below\n",
    "            #This btagSFProduct is the product of the SFs for the selected jets from collection jetPt with mask jetMask\n",
    "            btagSFProduct = \"btagSFProduct{spf}\".format(spf=syspostfix)\n",
    "            #input weight, should include all corrections for this systematic variation except Btagging SF and yield ratio\n",
    "            calculationWeightBefore = \"prewgt{spf}\".format(spf=syspostfix)\n",
    "            #For calculating the yeild ratio, we need this weight, which will be the product of calculationWeightBefore and the product of btag SFs (no yield ratio!)\n",
    "            calculationWeightAfter = \"calcBtagYields_after{spf}\".format(spf=syspostfix)\n",
    "            #Define the form of the final name of the btagSFProduct * YieldRatio(HT, nJet)\n",
    "            #This needs to match what will be picked up in the final weight definitions!\n",
    "            btagFinalWeight = \"pwgt_btag{spf}\".format(spf=syspostfix)\n",
    "            \n",
    "            #Lets be really obvious about missing jet_masks... exception it\n",
    "            if jetMask not in listOfDefinedColumns:\n",
    "                raise RuntimeError(\"Could not find {} column in method BtaggingYields\".format(jetMask))\n",
    "            \n",
    "            #Skip SFs for which the requisite per-jet SFs are not present...\n",
    "            if jetSF not in listOfDefinedColumns:\n",
    "                if verbose: print(\"Skipping {} in BtaggingYields as it is not a valid column name\".format(jetSF))\n",
    "                continue\n",
    "                \n",
    "            #Check we have the input weight for before btagSF and yield ratio multiplication\n",
    "            if calculationWeightBefore not in listOfDefinedColumns:\n",
    "                raise RuntimeError(\"{} is not defined, cannot continue with calculating BTaggingYields\")\n",
    "            \n",
    "            #Now check if the event preweight SF is in the list of columns, and if not, define it (common to calculating yields and loading them...)\n",
    "            #We might want to call this function twice to calculate yields for a future iteration and use an older iteration at the same time\n",
    "            if btagSFProduct not in listOfDefinedColumns:\n",
    "                if calculateYields and btagSFProduct not in histos_dict[\"BtaggingYields\"].keys():\n",
    "                    histos_dict[\"BtaggingYields\"][btagSFProduct] = {}\n",
    "                z[sysVar].append((\"{}\".format(btagSFProduct), \"FTA::btagEventWeight_shape({}, {})\".format(jetSF, jetMask)))\n",
    "            if calculationWeightAfter not in listOfDefinedColumns:\n",
    "                z[sysVar].append((\"{}\".format(calculationWeightAfter), \"{} * {}\".format(calculationWeightBefore, \n",
    "                                                                                         btagSFProduct)))\n",
    "                \n",
    "            #Check that the HT and nJet numbers are available to us, and if not, define them based on the available masks    \n",
    "            #if isScaleVariation:\n",
    "            nJetName = \"nFTAJet{bpf}\".format(bpf=branchpostfix)\n",
    "            HTName = \"HT{bpf}\".format(bpf=branchpostfix)\n",
    "            if nJetName not in listOfDefinedColumns:\n",
    "                z[sysVar].append((nJetName, \"{0}[{1}].size()\".format(jetPt, jetMask)))\n",
    "            if HTName not in listOfDefinedColumns:\n",
    "                z[sysVar].append((HTName, \"Sum({0}[{1}])\".format(jetPt, jetMask)))\n",
    "            \n",
    "            #loadYields path...\n",
    "            if loadYields:\n",
    "                if useAggregate:\n",
    "                    z[sysVar].append((btagFinalWeight, \"{bsf} * {lm}[\\\"{sn}\\\"][rdfslot_]->getEventYieldRatio(\\\"{yk}\\\", \\\"{vk}\\\", {nj}, {ht});\"\\\n",
    "                                     .format(bsf=btagSFProduct, lm=lookupMap, sn=sampleName, yk=\"Aggregate\", vk=btagSFProduct.replace(\"nonorm_\",\"\"), nj=nJetName, ht=HTName)))\n",
    "                else:\n",
    "                    z[sysVar].append((btagFinalWeight, \"{bsf} * {lm}[\\\"{sn}\\\"][rdfslot_]->getEventYieldRatio(\\\"{yk}\\\", \\\"{vk}\\\", {nj}, {ht});\"\\\n",
    "                                     .format(bsf=btagSFProduct, lm=lookupMap, sn=sampleName, yk=sampleName, vk=btagSFProduct.replace(\"nonorm_\",\"\"), nj=nJetName, ht=HTName)))\n",
    "\n",
    "            for defName, defFunc in z[sysVar]:\n",
    "                if defName in listOfDefinedColumns:\n",
    "                    if verbose:\n",
    "                        print(\"{} already defined, skipping\".format(defName))\n",
    "                    continue\n",
    "                else:\n",
    "                    if verbose:\n",
    "                        print(\"rdf = rdf.Define(\\\"{}\\\", \\\"{}\\\")\".format(defName, defFunc))\n",
    "                    rdf = rdf.Define(defName, defFunc)\n",
    "                    listOfDefinedColumns.push_back(defName)        \n",
    "            #calculate Yields path\n",
    "            if calculateYields:\n",
    "                k = btagSFProduct\n",
    "                histos_dict[\"BtaggingYields\"][k] = {}\n",
    "                histos_dict[\"BtaggingYields\"][k][\"sumW_before\"] = rdf.Histo2D((\"{}_BtaggingYield_{}_sumW_before\".format(sampleName, btagSFProduct.replace(\"btagSFProduct_\",\"\")), \n",
    "                                                                               \"BTaggingYield #Sigma#omega_{before}; HT; nJet\",\n",
    "                                                                               HTBins, HTMin, HTMax,\n",
    "                                                                               JetBins, nJetMin, nJetMax),\n",
    "                                                                               HTName,\n",
    "                                                                               nJetName,\n",
    "                                                                               calculationWeightBefore)\n",
    "                histos_dict[\"BtaggingYields\"][k][\"sumW_after\"] = rdf.Histo2D((\"{}_BtaggingYield_{}_sumW_after\".format(sampleName, btagSFProduct.replace(\"btagSFProduct_\",\"\")),\n",
    "                                                                              \"BTaggingYield #Sigma#omega_{after}; HT; nJet\",\n",
    "                                                                              HTBins, HTMin, HTMax,\n",
    "                                                                              JetBins, nJetMin, nJetMax),\n",
    "                                                                              HTName,\n",
    "                                                                              nJetName,\n",
    "                                                                              calculationWeightAfter)\n",
    "                #For Unified JetBinning calculation\n",
    "                histos_dict[\"BtaggingYields1DX\"][k][\"sumW_before\"] = rdf.Histo2D((\"{}_BtaggingYield1DX_{}_sumW_before\".format(sampleName, btagSFProduct.replace(\"btagSFProduct_\",\"\")), \n",
    "                                                                               \"BTaggingYield #Sigma#omega_{before}; HT; nJet\",\n",
    "                                                                               HTBins, HTMin, HTMax,\n",
    "                                                                               1, nJetMin, nJetMax),\n",
    "                                                                               HTName,\n",
    "                                                                               nJetName,\n",
    "                                                                               calculationWeightBefore)\n",
    "                histos_dict[\"BtaggingYields1DX\"][k][\"sumW_after\"] = rdf.Histo2D((\"{}_BtaggingYield1DX_{}_sumW_after\".format(sampleName, btagSFProduct.replace(\"btagSFProduct_\",\"\")),\n",
    "                                                                              \"BTaggingYield #Sigma#omega_{after}; HT; nJet\",\n",
    "                                                                              HTBins, HTMin, HTMax,\n",
    "                                                                              1, nJetMin, nJetMax),\n",
    "                                                                              HTName,\n",
    "                                                                              nJetName,\n",
    "                                                                              calculationWeightAfter)\n",
    "                #For Unified HTBinning calculation\n",
    "                histos_dict[\"BtaggingYields1DY\"][k][\"sumW_before\"] = rdf.Histo2D((\"{}_BtaggingYield1DY_{}_sumW_before\".format(sampleName, btagSFProduct.replace(\"btagSFProduct_\",\"\")), \n",
    "                                                                               \"BTaggingYield #Sigma#omega_{before}; HT; nJet\",\n",
    "                                                                               1, HTMin, HTMax,\n",
    "                                                                               JetBins, nJetMin, nJetMax),\n",
    "                                                                               HTName,\n",
    "                                                                               nJetName,\n",
    "                                                                               calculationWeightBefore)\n",
    "                histos_dict[\"BtaggingYields1DY\"][k][\"sumW_after\"] = rdf.Histo2D((\"{}_BtaggingYield1DY_{}_sumW_after\".format(sampleName, btagSFProduct.replace(\"btagSFProduct_\",\"\")),\n",
    "                                                                              \"BTaggingYield #Sigma#omega_{after}; HT; nJet\",\n",
    "                                                                              1, HTMin, HTMax,\n",
    "                                                                              JetBins, nJetMin, nJetMax),\n",
    "                                                                              HTName,\n",
    "                                                                              nJetName,\n",
    "                                                                              calculationWeightAfter)\n",
    "        return rdf\n",
    "\n",
    "\n",
    "def BtaggingEfficiencies(input_df, sampleName=None, era=\"2017\", wgtVar=\"wgt_SUMW_PU_L1PF\", isData = True, histos_dict=None, \n",
    "               doDeepCSV=True, doDeepJet=True, debugInfo=False):\n",
    "    validAlgos = []\n",
    "    if doDeepCSV == True: validAlgos.append(\"DeepCSV\")\n",
    "    if doDeepJet == True: validAlgos.append(\"DeepJet\")\n",
    "    bTagWorkingPointDict = {\n",
    "        '2016':{\n",
    "            'DeepCSV':{'L': 0.2217, 'M': 0.6321, 'T': 0.8953, 'Var': 'btagDeepB'},\n",
    "            'DeepJet':{ 'L': 0.0614, 'M': 0.3093, 'T': 0.7221, 'Var': 'btagDeepFlavB'}\n",
    "        },\n",
    "        '2017':{\n",
    "            'CSVv2':{'L': 0.5803, 'M': 0.8838, 'T': 0.9693, 'Var': 'btagCSVV2'},\n",
    "            'DeepCSV':{'L': 0.1522, 'M': 0.4941, 'T': 0.8001, 'Var': 'btagDeepB'},\n",
    "            'DeepJet':{'L': 0.0521, 'M': 0.3033, 'T': 0.7489, 'Var': 'btagDeepFlavB'}\n",
    "        },\n",
    "        '2018':{\n",
    "            'DeepCSV':{'L': 0.1241, 'M': 0.4184, 'T': 0.7527, 'Var': 'btagDeepB'},\n",
    "            'DeepJet':{'L': 0.0494, 'M': 0.2770, 'T': 0.7264, 'Var': 'btagDeepFlavB'}\n",
    "        }\n",
    "    }\n",
    "    if isData == True:\n",
    "        pass\n",
    "    else:\n",
    "        theCats = collections.OrderedDict()\n",
    "        theCats[\"Inclusive\"] = \"nGJet >= 4\"\n",
    "        theCats[\"nJet4\"] = \"nGJet == 4\"\n",
    "        theCats[\"nJet5\"] = \"nGJet == 5\"\n",
    "        theCats[\"nJet6\"] = \"nGJet == 6\"\n",
    "        theCats[\"nJet7\"] = \"nGJet == 7\"\n",
    "        theCats[\"nJet8+\"] = \"nGJet >= 8\"\n",
    "        \n",
    "        input_df_defined = input_df.Define(\"GJet_hadronFlavour\", \"Jet_hadronFlavour[jet_mask]\")\n",
    "        input_df_defined = input_df_defined.Define(\"GJet_abseta\", \"abs(Jet_eta[jet_mask])\")\n",
    "        input_df_defined = input_df_defined.Define(\"GJet_bjet_mask\", \"GJet_hadronFlavour == 5\")\n",
    "        input_df_defined = input_df_defined.Define(\"GJet_cjet_mask\", \"GJet_hadronFlavour == 4\")\n",
    "        input_df_defined = input_df_defined.Define(\"GJet_udsgjet_mask\", \"GJet_hadronFlavour < 4\")\n",
    "        for algo in [\"DeepJet\", \"DeepCSV\"]:\n",
    "            for wp in [\"L\", \"M\", \"T\"]:\n",
    "                input_df_defined = input_df_defined.Define(\"GJet_{0}_{1}_mask\".format(algo, wp),\n",
    "                                                           \"GJet_{0} > {1}\".format(bTagWorkingPointDict[era][algo][\"Var\"],\n",
    "                                                                                bTagWorkingPointDict[era][algo][wp]))\n",
    "        \n",
    "        for jettype in [\"bjet\", \"cjet\", \"udsgjet\"]:\n",
    "            input_df_defined = input_df_defined.Define(\"GJet_{}_untagged_pt\".format(jettype), \"GJet_pt[GJet_{}_mask]\".format(jettype))\n",
    "            input_df_defined = input_df_defined.Define(\"GJet_{}_untagged_abseta\".format(jettype), \"GJet_abseta[GJet_{}_mask]\".format(jettype))\n",
    "            for algo in validAlgos:\n",
    "                for wp in [\"L\", \"M\", \"T\"]:\n",
    "                    input_df_defined = input_df_defined.Define(\"GJet_{0}_{1}_{2}_pt\".format(jettype, algo, wp), \"GJet_pt[GJet_{0}_mask && GJet_{1}_{2}_mask]\".format(jettype, algo, wp))\n",
    "                    input_df_defined = input_df_defined.Define(\"GJet_{0}_{1}_{2}_abseta\".format(jettype, algo, wp), \"GJet_abseta[GJet_{0}_mask && GJet_{1}_{2}_mask]\".format(jettype, algo, wp))\n",
    "                    \n",
    "        cat_df = collections.OrderedDict()\n",
    "        for ck, cs in theCats.items():\n",
    "            cat_df[ck] = input_df_defined.Filter(cs, \"btagging \" + cs)\n",
    "        if histos_dict != None:\n",
    "            if \"Btagging\" not in histos_dict:\n",
    "                histos_dict[\"Btagging\"] = {}\n",
    "            for tc in theCats.keys(): \n",
    "                if tc not in histos_dict[\"Btagging\"]: \n",
    "                    histos_dict[\"Btagging\"][tc] = {}\n",
    "            for tc, cut in theCats.items():\n",
    "                tcn = tc.replace(\"blind_\", \"\")\n",
    "                for jettype in [\"bjet\", \"cjet\", \"udsgjet\"]:\n",
    "                    histos_dict[\"Btagging\"][tc][\"{}s_untagged\".format(jettype)] = cat_df[tc].Histo2D((\"{0}s_untagged_[{0}]({1})\".format(tcn, wgtVar), \";jet p_{T}; jet |#eta|\", 248, 20, 2500, 25, 0, 2.5), \n",
    "                                                                                                     \"GJet_{}_untagged_pt\".format(jettype), \"GJet_{}_untagged_abseta\".format(jettype), wgtVar)\n",
    "                    for algo in validAlgos:\n",
    "                        for wp in [\"L\", \"M\", \"T\"]:\n",
    "                            histos_dict[\"Btagging\"][tc][\"{0}s_{1}_{2}\".format(jettype, algo, wp)] = cat_df[tc].Histo2D((\"{0}s_{1}_{2}_[{0}]({3})\".format(tcn, algo, wp, wgtVar), \";jet p_{T}; jet |#eta|\", 248, 20, 2500, 25, 0, 2.5), \n",
    "                                                                                                             \"GJet_{0}_{1}_{2}_pt\".format(jettype, algo, wp), \"GJet_{0}_{1}_{2}_abseta\".format(jettype, algo, wp), wgtVar)\n",
    "                            \n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cutPVandMETFilters(input_df, level, isData=False):\n",
    "    if \"selection\" in level: \n",
    "        lvl = \"selection\"\n",
    "    else:\n",
    "        lvl = \"baseline\"\n",
    "    PVbits = 0b00000000000000000111\n",
    "    METbits_MC = 0b00000000001111110000\n",
    "    METbits_Data = 0b00000000000000001000\n",
    "    if isData:\n",
    "        METbits = METbits_MC + METbits_Data \n",
    "    else:\n",
    "        METbits = METbits_MC\n",
    "    rdf = input_df.Filter(\"(ESV_JetMETLogic_{lvl} & {bits}) >= {bits}\".format(lvl=lvl, bits=METbits), \"PV, MET Filters\")\n",
    "    return rdf\n",
    "\n",
    "def insertPVandMETFilters(input_df, level, era=\"2017\", isData=False):\n",
    "    rdf = input_df\n",
    "    #flags for MET filters\n",
    "    FlagsDict = {\"2016\" :  \n",
    "                 { \"isData\" : [\"globalSuperTightHalo2016Filter\"],\n",
    "                   \"Common\" :  [\"goodVertices\",\n",
    "                                \"HBHENoiseFilter\",\n",
    "                                \"HBHENoiseIsoFilter\",\n",
    "                                \"EcalDeadCellTriggerPrimitiveFilter\",\n",
    "                                \"BadPFMuonFilter\"\n",
    "                               ],\n",
    "                   \"NotRecommended\" : [\"BadChargedCandidateFilter\",\n",
    "                                       \"eeBadScFilter\"\n",
    "                                      ],\n",
    "                 },\n",
    "                 \"2017\" :  \n",
    "                 { \"isData\" : [\"globalSuperTightHalo2016Filter\"],\n",
    "                   \"Common\" :  [\"goodVertices\",\n",
    "                                \"HBHENoiseFilter\",\n",
    "                                \"HBHENoiseIsoFilter\",\n",
    "                                \"EcalDeadCellTriggerPrimitiveFilter\",\n",
    "                                \"BadPFMuonFilter\",\n",
    "                                \"ecalBadCalibFilterV2\"\n",
    "                               ],\n",
    "                  \"NotRecommended\" : [\"BadChargedCandidateFilter\",\n",
    "                                      \"eeBadScFilter\"\n",
    "                                     ],\n",
    "                 },\n",
    "                 \"2018\" :  { \"isData\" : [\"globalSuperTightHalo2016Filter\"],\n",
    "                            \"Common\" :  [\"goodVertices\",\n",
    "                                         \"HBHENoiseFilter\",\n",
    "                                         \"HBHENoiseIsoFilter\",\n",
    "                                         \"EcalDeadCellTriggerPrimitiveFilter\",\n",
    "                                         \"BadPFMuonFilter\",\n",
    "                                         \"ecalBadCalibFilterV2\"\n",
    "                                        ],\n",
    "                            \"NotRecommended\" : [\"BadChargedCandidateFilter\",\n",
    "                                                \"eeBadScFilter\"\n",
    "                                               ],\n",
    "                           },\n",
    "                } \n",
    "    Flags = FlagsDict[era]\n",
    "\n",
    "    #2016selection required !isFake(), nDegreesOfFreedom> 4 (strictly),|z| < 24 (in cm? fractions of acentimeter?), and rho =sqrt(PV.x**2 + PV.y**2)< 2\n",
    "    #Cuts are to use strictly less than and greater than, i.e. PV.ndof > minNDoF, not >=\n",
    "    PVCutDict = {\n",
    "            '2016':{\n",
    "                'minNDoF': 4,\n",
    "                'maxAbsZ': 24.0,\n",
    "                'maxRho': 2\n",
    "                },\n",
    "            '2017':{\n",
    "                'minNDoF': 4,\n",
    "                'maxAbsZ': 24.0,\n",
    "                'maxRho': 2\n",
    "                },\n",
    "            '2018':{\n",
    "                'minNDoF': 4,\n",
    "                'maxAbsZ': 24.0,\n",
    "                'maxRho': 2\n",
    "                }\n",
    "        }\n",
    "    PVCut = PVCutDict[era]\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "#    if \"selection\" in level:\n",
    "#        rdf = rdf.Filter(\"(ESV_JetMETLogic_selection & {0}) >= {0}\".format(0b00000000000000000001), \"PV NDoF > 4\")\n",
    "#        rdf = rdf.Filter(\"(ESV_JetMETLogic_selection & {0}) >= {0}\".format(0b00000000000000000010), \"PV |z| < 24.0\")\n",
    "#        rdf = rdf.Filter(\"(ESV_JetMETLogic_selection & {0}) >= {0}\".format(0b00000000000000000100), \"PV rho < 2\")\n",
    "#        if isData == True:\n",
    "#            rdf = rdf.Filter(\"(ESV_JetMETLogic_selection & {0}) >= {0}\".format(0b00000000000000001000), \"MET globalSuperTightHalo2016Filter\")\n",
    "#        rdf = rdf.Filter(\"(ESV_JetMETLogic_selection & {0}) >= {0}\".format(0b00000000000000010000), \"MET goodVertices\")\n",
    "#        rdf = rdf.Filter(\"(ESV_JetMETLogic_selection & {0}) >= {0}\".format(0b00000000000000100000), \"MET HBHENoiseFilter\")\n",
    "#        rdf = rdf.Filter(\"(ESV_JetMETLogic_selection & {0}) >= {0}\".format(0b00000000000001000000), \"MET HBHENoiseIsoFilter\")\n",
    "#        rdf = rdf.Filter(\"(ESV_JetMETLogic_selection & {0}) >= {0}\".format(0b00000000000010000000), \"MET EcalDeadCellTriggerPrimitiveFilter\")\n",
    "#        rdf = rdf.Filter(\"(ESV_JetMETLogic_selection & {0}) >= {0}\".format(0b00000000000100000000), \"MET BadPFMuonFilter\")\n",
    "#        rdf = rdf.Filter(\"(ESV_JetMETLogic_selection & {0}) >= {0}\".format(0b00000000001000000000), \"MET ecalBadCalibFilterV2\")\n",
    "#    elif \"baseline\" in level:\n",
    "#        rdf = rdf.Filter(\"(ESV_JetMETLogic_baseline & {0}) >= {0}\".format(0b00000000000000000001), \"PV NDoF > 4\")\n",
    "#        rdf = rdf.Filter(\"(ESV_JetMETLogic_baseline & {0}) >= {0}\".format(0b00000000000000000010), \"PV |z| < 24.0\")\n",
    "#        rdf = rdf.Filter(\"(ESV_JetMETLogic_baseline & {0}) >= {0}\".format(0b00000000000000000100), \"PV rho < 2\")\n",
    "#        if isData == True:\n",
    "#            rdf = rdf.Filter(\"(ESV_JetMETLogic_baseline & {0}) >= {0}\".format(0b00000000000000001000), \"MET globalSuperTightHalo2016Filter\")\n",
    "#        rdf = rdf.Filter(\"(ESV_JetMETLogic_baseline & {0}) >= {0}\".format(0b00000000000000010000), \"MET goodVertices\")\n",
    "#        rdf = rdf.Filter(\"(ESV_JetMETLogic_baseline & {0}) >= {0}\".format(0b00000000000000100000), \"MET HBHENoiseFilter\")\n",
    "#        rdf = rdf.Filter(\"(ESV_JetMETLogic_baseline & {0}) >= {0}\".format(0b00000000000001000000), \"MET HBHENoiseIsoFilter\")\n",
    "#        rdf = rdf.Filter(\"(ESV_JetMETLogic_baseline & {0}) >= {0}\".format(0b00000000000010000000), \"MET EcalDeadCellTriggerPrimitiveFilter\")\n",
    "#        rdf = rdf.Filter(\"(ESV_JetMETLogic_baseline & {0}) >= {0}\".format(0b00000000000100000000), \"MET BadPFMuonFilter\")\n",
    "#        rdf = rdf.Filter(\"(ESV_JetMETLogic_baseline & {0}) >= {0}\".format(0b00000000001000000000), \"MET ecalBadCalibFilterV2\")\n",
    "#    return rdf\n",
    "    \n",
    "#def defineEventVars(input_df):\n",
    "#    rdf = input_df\n",
    "#    #rdf = rdf.Define(\"JML_baseline_pass\", \"(ESV_JetMETLogic_baseline & {0}) >= {0}\".format(0b00001100011111111111))#Cut on MET pt, nJet, HT\n",
    "#    rdf = rdf.Define(\"JML_baseline_pass\", \"(ESV_JetMETLogic_baseline & {0}) >= {0}\".format(0b00000000001111111111))#Only PV and MET filters required to pass\n",
    "#    #rdf = rdf.Define(\"JML_selection_pass\", \"(ESV_JetMETLogic_selection & {0}) >= {0}\".format(0b00001100011111111111))#Cut on MET pt, nJet, HT\n",
    "#    rdf = rdf.Define(\"JML_selection_pass\", \"(ESV_JetMETLogic_selection & {0}) >= {0}\".format(0b00000000001111111111))#Only PV and MET filters required to pass\n",
    "#    return rdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fillHistos(input_df, sampleName=None, wgtVar=\"wgt_SUMW\", isData = True, histos1D_dict=None, histos2D_dict=None, histosNS_dict=None, \n",
    "               doMuons=False, doElectrons=False, doLeptons=False, doJets=False, doWeights=False, doEventVars=False, \n",
    "               makeMountains=False, debugInfo=True, nJetsToHisto=10, useDeepCSV=False, verbose=False,\n",
    "               HTCut=500, METCut=None, ZMassMETWindow=None,\n",
    "               sysVariations={\"$NOMINAL\": {\"jet_mask\": \"jet_mask\",\n",
    "                                           \"wgt_final\": \"wgt_nom\",\n",
    "                                             \"wgt_prebTag\": \"wgt_SUMW_PU_LSF_L1PF\",\n",
    "                                             \"jet_pt_var\": \"Jet_pt\",\n",
    "                                             \"jet_mass_var\": \"Jet_mass\",\n",
    "                                             \"met_pt_var\": \"METFixEE2017_pt\",\n",
    "                                             \"met_phi_var\": \"METFixEE2017_phi\",\n",
    "                                             \"btagSF\": \"Jet_btagSF_deepcsv_shape\",\n",
    "                                             \"weightVariation\": False},\n",
    "                              \"_jesTotalUp\": {\"jet_mask\": \"jet_mask_jesTotalUp\",\n",
    "                                             \"wgt_final\": \"wgt_jesTotalUp\",\n",
    "                                             \"wgt_prebTag\": \"wgt_SUMW_PU_LSF_L1PF\",\n",
    "                                             \"jet_pt_var\": \"Jet_pt_jesTotalUp\",\n",
    "                                             \"jet_mass_var\": \"Jet_mass_jesTotalUp\",\n",
    "                                             \"met_pt_var\": \"METFixEE2017_pt_jesTotalUp\",\n",
    "                                             \"met_phi_var\": \"METFixEE2017_phi_jesTotalUp\",\n",
    "                                             \"btagSF\": \"Jet_btagSF_deepcsv_shape\",\n",
    "                                             \"weightVariation\": False},\n",
    "                              \"_jesTotalDown\": {\"jet_mask\": \"jet_mask_jesTotalDown\",\n",
    "                                             \"wgt_final\": \"wgt_jesTotalDown\", \n",
    "                                             \"wgt_prebTag\": \"wgt_SUMW_PU_LSF_L1PF\",\n",
    "                                             \"jet_pt_var\": \"Jet_pt_jesTotalDown\",\n",
    "                                             \"jet_mass_var\": \"Jet_mass_jesTotalDown\",\n",
    "                                             \"met_pt_var\": \"METFixEE2017_pt_jesTotalDown\",\n",
    "                                             \"met_phi_var\": \"METFixEE2017_phi_jesTotalDown\",\n",
    "                                             \"btagSF\": \"Jet_btagSF_deepcsv_shape\",\n",
    "                                             \"weightVariation\": False},\n",
    "                                     },):\n",
    "    \"\"\"Method to fill histograms given an input RDataFrame, input sample/dataset name, input histogram dictionaries.\n",
    "    Has several options of which histograms to fill, such as Leptons, Jets, Weights, EventVars, etc.\n",
    "    Types of histograms (1D, 2D, those which will not be stacked(NS - histosNS)) are filled by passing non-None\n",
    "    value to that histosXX_dict variable. Internally stored with structure separating the categories of histos,\n",
    "    with 'Muons,' 'Electrons,' 'Leptons,' 'Jets,' 'EventVars,' 'Weights' subcategories.\n",
    "    \n",
    "    ZMassMETWindow = [<invariant mass halfwidth>, <METCut>] - If in the same-flavor dilepton channel, require \n",
    "    abs(DileptonInvMass - ZMass) < ZWindowHalfWidth and MET >= METCut\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    if doMuons == False and doElectrons == False and doLeptons == False\\\n",
    "                and doJets == False and doWeights == False and doEventVars == False\\\n",
    "                and makeMountains == False:\n",
    "        raise RuntimeError(\"Must select something to plot:\"\\\n",
    "                               \"Set do{Muons,Electrons,Leptons,Jets,Weights,EventVars,etc} = True in init method\")\n",
    "    \n",
    "    pi = ROOT.TMath.Pi()\n",
    "    #Get the list of defined columns for checks\n",
    "    listOfDefinedColumns = input_df.GetDefinedColumnNames()\n",
    "    #Dictionary to hold all the categorization nodes\n",
    "    cat_df = collections.OrderedDict()\n",
    "    #Make sure the nominal is done first so that categorization is successful\n",
    "    for sysVar, sysDict in sorted(sysVariations.items(), key=lambda x: \"$NOMINAL\" in x[0], reverse=True):\n",
    "        #skip systematic variations on data, only do the nominal\n",
    "        if isData and sysVar != \"$NOMINAL\": \n",
    "            continue\n",
    "        #skip making MET corrections unless it is: Nominal or a scale variation (i.e. JES up...)\n",
    "        isWeightVariation = sysDict.get(\"weightVariation\", False)\n",
    "        #jetMask = sysDict.get(\"jet_mask\")\n",
    "        #jetPt = sysDict.get(\"jet_pt_var\")\n",
    "        #jetMass = sysDict.get(\"jet_mass_var\")\n",
    "        #Name histograms with their actual systematic variation postfix, using the convention that HISTO_NAME__nom is\n",
    "        # the nominal and HISTO_NAME__$SYSTEMATIC is the variation, like so:\n",
    "        syspostfix = \"__nom\" if sysVar == \"$NOMINAL\" else \"_{}\".format(sysVar)\n",
    "        #name branches for filling with the nominal postfix if weight variations, and systematic postfix if scale variation (jes_up, etc.)\n",
    "        branchpostfix = None\n",
    "        if isWeightVariation:\n",
    "            branchpostfix = \"_nom\"\n",
    "        else:\n",
    "            branchpostfix = sysVar.replace(\"$NOMINAL\", \"_nom\")\n",
    "        leppostfix = \"_nom\" #No variation on this yet, but just in case\n",
    "        \n",
    "        fillJet = \"FTAJet{bpf}\".format(bpf=branchpostfix)\n",
    "        fillJet_pt = \"FTAJet{bpf}_pt\".format(bpf=branchpostfix)\n",
    "        fillJet_phi = \"FTAJet{bpf}_phi\".format(bpf=branchpostfix)\n",
    "        fillJet_eta = \"FTAJet{bpf}_eta\".format(bpf=branchpostfix)\n",
    "        fillJet_mass = \"FTAJet{bpf}_mass\".format(bpf=branchpostfix)\n",
    "        fillMET_pt = \"FTAMET{bpf}_pt\".format(bpf=branchpostfix)\n",
    "        fillMET_phi = \"FTAMET{bpf}_phi\".format(bpf=branchpostfix)\n",
    "        \n",
    "        #Get the appropriate weight defined in defineFinalWeights function\n",
    "        wgtVar = sysDict.get(\"wgt_final\", \"wgt_nom\")\n",
    "        if wgtVar not in listOfDefinedColumns:\n",
    "            print(\"{} not found as a valid weight variation, trying something else as backup\".format(wgtVar))\n",
    "            if \"wgt_SUMW_PU_LSF_L1PF\" in listOfDefinedColumns:\n",
    "                wgtVar = \"wgt_SUMW_PU_LSF_L1PF\"\n",
    "            elif \"wgt_SUMW\" in listOfDefinedColumns:\n",
    "                wgtVar = \"wgt_SUMW\"\n",
    "            else:\n",
    "                raise RuntimeError(\"Couldn't find a valid fallback weight variation in fillHistos()\")\n",
    "            print(\"{} chosen as the weight for {} variation\".format(wgtVar, syspostfix))\n",
    "\n",
    "    \n",
    "        if doWeights == True:\n",
    "            if histosNS_dict != None:\n",
    "                if \"EventVars\" not in histosNS_dict:\n",
    "                    histosNS_dict[\"EventVars\"] = {}\n",
    "                histosNS[name][lvl][\"EventVars\"][\"wgt_NUMW\"] = input_df.Histo1D(\"wgt_NUMW\")\n",
    "                histosNS[name][lvl][\"EventVars\"][wgtVar] = input_df.Histo1D(wgtVar)\n",
    "            if histos1D_dict != None:\n",
    "                if \"EventVars\" not in histos1D_dict:\n",
    "                    histos1D_dict[\"EventVars\"] = {}\n",
    "                histos1D_dict[\"EventVars\"][\"wgt_diff\"] = input_df.Histo1D((\"wgt_diff\", \"(wgt_NUMW - wgt_SUMW)/wgt_SUMW\", 2000, -1, 1), \"wgt_diff\", \"1\")\n",
    "                histos1D_dict[\"EventVars\"][\"wgt_PU\"] = input_df.Histo1D((\"wgt_PU\", \"\", 2000, 0, 5), \"puWeight\", \"wgt_SUMW\")\n",
    "                histos1D_dict[\"EventVars\"][\"wgt_LSF\"] = input_df.Histo1D((\"wgt_LSF\", \"\", 2000, 0, 5), \"wgt_LSF\", \"wgt_SUMW\")\n",
    "                histos1D_dict[\"EventVars\"][\"wgt_L1PF\"] = input_df.Histo1D((\"wgt_L1PF\", \"\", 2000, 0, 5), \"L1PreFiringWeight_Nom\", \"wgt_SUMW\")\n",
    "                histos1D_dict[\"EventVars\"][\"wgt_PU_LSF_L1PF\"] = input_df.Histo1D((\"wgt_PU_LSF_L1PF\", \"\", 2000, 0, 5), \"wgt_PU_LSF_L1PF\", \"wgt_SUMW\")\n",
    "        if doMuons == True:\n",
    "            if histos1D_dict != None:\n",
    "                if \"Muons\" not in histos1D_dict: \n",
    "                    histos1D_dict[\"Muons\"] = {}\n",
    "                histos1D_dict[\"Muons\"][\"idx\"] = input_df.Histo1D((\"idx{}\".format(postfix), \"\", 5, 0, 5), \"Muon_idx\", wgtVar)\n",
    "                histos1D_dict[\"Muons\"][\"Gidx\"] = input_df.Histo1D((\"Gidx{}\".format(postfix), \"\", 5, 0, 5), \"FTAMuon{lpf}_idx\", wgtVar)\n",
    "                histos1D_dict[\"Muons\"][\"nMu\"] = input_df.Histo1D((\"nMuon{}\".format(postfix), \"\", 5, 0, 5), \"nFTAMuon{lpf}\", wgtVar)\n",
    "                histos1D_dict[\"Muons\"][\"nLooseMu\"] = input_df.Histo1D((\"nLooseMuon{}\".format(postfix), \"\", 5, 0, 5), \"nLooseFTAMuon{lpf}\", wgtVar)\n",
    "                histos1D_dict[\"Muons\"][\"nMediumMu\"] = input_df.Histo1D((\"nMediumMuon{}\".format(postfix), \"\", 5, 0, 5), \"nMediumFTAMuon{lpf}\", wgtVar)\n",
    "                histos1D_dict[\"Muons\"][\"pt\"] = input_df.Histo1D((\"Muon_pt{}\".format(postfix), \"\", 100, 0, 500), \"FTAMuon{lpf}_pt\", wgtVar)\n",
    "                histos1D_dict[\"Muons\"][\"eta\"] = input_df.Histo1D((\"Muon_eta{}\".format(postfix), \"\", 104, -2.6, 2.6), \"FTAMuon{lpf}_eta\", wgtVar)\n",
    "                histos1D_dict[\"Muons\"][\"phi\"] = input_df.Histo1D((\"Muon_phi{}\".format(postfix), \"\", 64, -pi, pi), \"FTAMuon{lpf}_phi\", wgtVar)\n",
    "                #histos1D_dict[\"Muons\"][\"mass\"] = input_df.Histo1D((\"Muon_mass{}\".format(postfix), \"\", 50, 0, 1), \"FTAMuon{lpf}_mass\", wgtVar)\n",
    "                histos1D_dict[\"Muons\"][\"iso\"] = input_df.Histo1D((\"Muon_iso{}\".format(postfix), \"\", 8, 0, 8), \"FTAMuon{lpf}_pfIsoId\", wgtVar)\n",
    "                histos1D_dict[\"Muons\"][\"dz\"] = input_df.Histo1D((\"Muon_dz{}\".format(postfix), \"\", 100, -0.01, 0.01), \"FTAMuon{lpf}_dz\", wgtVar)\n",
    "                histos1D_dict[\"Muons\"][\"dxy\"] = input_df.Histo1D((\"Muon_dxy{}\".format(postfix), \"\", 100, -0.1, 0.1), \"FTAMuon{lpf}_dxy\", wgtVar)\n",
    "                #histos1D_dict[\"Muons\"][\"d0\"] = input_df.Histo1D((\"Muon_d0{}\".format(postfix), \"\", 100, -0.01, 0.01), \"FTAMuon{lpf}_d0\", wgtVar)\n",
    "                histos1D_dict[\"Muons\"][\"ip3d\"] = input_df.Histo1D((\"Muon_ip3d{}\".format(postfix), \"\", 100, 0, 0.01), \"FTAMuon{lpf}_ip3d\", wgtVar)\n",
    "                histos1D_dict[\"Muons\"][\"pfRelIso03_all\"] = input_df.Histo1D((\"Muon_pfRelIso03_all{}\".format(postfix), \"\", 100, 0, 0.2), \"FTAMuon{lpf}_pfRelIso03_all\", wgtVar)\n",
    "                histos1D_dict[\"Muons\"][\"pfRelIso03_chg\"] = input_df.Histo1D((\"Muon_pfRelIso03_chg{}\".format(postfix), \"\", 100, 0, 0.2), \"FTAMuon{lpf}_pfRelIso03_chg\", wgtVar)\n",
    "                histos1D_dict[\"Muons\"][\"pfRelIso04_all\"] = input_df.Histo1D((\"Muon_pfRelIso04_all{}\".format(postfix), \"\", 100, 0, 0.2), \"FTAMuon{lpf}_pfRelIso04_all\", wgtVar)\n",
    "            if histos2D_dict != None:\n",
    "                if \"Muons\" not in histos2D_dict:\n",
    "                    histos2D_dict[\"Muons\"] = {}\n",
    "                histos2D_dict[\"Muons\"][\"eta_phi\"] = input_df.Histo2D((\"Muon_eta_phi{}\".format(postfix), \"\",\n",
    "                                                                      104, -2.6, 2.6,\n",
    "                                                                      64, -pi, pi),\n",
    "                                                                     \"FTAMuon{lpf}_eta\", \"FTAMuon{lpf}_phi\", wgtVar)\n",
    "                histos2D_dict[\"Muons\"][\"dz_ip3d\"] = input_df.Histo2D((\"Muon_dz_ip3d{}\".format(postfix), \"\",\n",
    "                                                                      100, -0.01, 0.01,\n",
    "                                                                      100, 0, 0.01),\n",
    "                                                                     \"FTAMuon{lpf}_dz\", \"FTAMuon{lpf}_ip3d\", wgtVar)\n",
    "        if doElectrons == True:\n",
    "            if histos1D_dict != None:\n",
    "                if \"Electrons\" not in histos1D_dict: \n",
    "                    histos1D_dict[\"Electrons\"] = {}\n",
    "                histos1D_dict[\"Electrons\"][\"nEl\"] = input_df.Histo1D((\"nElectron{}\".format(postfix), \"\", 5, 0, 5), \"nFTAElectron{lpf}\", wgtVar)\n",
    "                histos1D_dict[\"Electrons\"][\"nLooseEl\"] = input_df.Histo1D((\"nLooseElectron{}\".format(postfix), \"\", 5, 0, 5), \"nLooseFTAElectron{lpf}\", wgtVar)\n",
    "                histos1D_dict[\"Electrons\"][\"nMediumEl\"] = input_df.Histo1D((\"nMediumElectron{}\".format(postfix), \"\", 5, 0, 5), \"nMediumFTAElectron{lpf}\", wgtVar)\n",
    "                histos1D_dict[\"Electrons\"][\"pt\"] = input_df.Histo1D((\"Electron_pt{}\".format(postfix), \"\", 100, 0, 500), \"FTAElectron{lpf}_pt\", wgtVar)\n",
    "                histos1D_dict[\"Electrons\"][\"eta\"] = input_df.Histo1D((\"Electron_eta{}\".format(postfix), \"\", 104, -2.6, 2.6), \"FTAElectron{lpf}_eta\", wgtVar)\n",
    "                histos1D_dict[\"Electrons\"][\"phi\"] = input_df.Histo1D((\"Electron_phi{}\".format(postfix), \"\", 64, -pi, pi), \"FTAElectron{lpf}_phi\", wgtVar)\n",
    "                #histos1D_dict[\"Electrons\"][\"mass\"] = input_df.Histo1D((\"Electron_mass{}\".format(postfix), \"\", 50, 0, 1), \"FTAElectron{lpf}_mass\", wgtVar)\n",
    "                histos1D_dict[\"Electrons\"][\"dz\"] = input_df.Histo1D((\"Electron_dz{}\".format(postfix), \"\", 100, -0.01, 0.01), \"FTAElectron{lpf}_dz\", wgtVar)\n",
    "                #histos1D_dict[\"Electrons\"][\"d0\"] = input_df.Histo1D((\"Electron_d0{}\".format(postfix), \"\", 100, 0, 0.01), \"FTAElectron{lpf}_d0\", wgtVar)\n",
    "                histos1D_dict[\"Electrons\"][\"ip3d\"] = input_df.Histo1D((\"Electron_ip3d{}\".format(postfix), \"\", 100, 0, 0.01), \"FTAElectron{lpf}_ip3d\", wgtVar)\n",
    "                histos1D_dict[\"Electrons\"][\"pfRelIso03_all\"] = input_df.Histo1D((\"Electron_pfRelIso03_all{}\".format(postfix), \"\", 100, 0, 0.2), \"FTAElectron{lpf}_pfRelIso03_all\", wgtVar)\n",
    "                histos1D_dict[\"Electrons\"][\"pfRelIso03_chg\"] = input_df.Histo1D((\"Electron_pfRelIso03_chg{}\".format(postfix), \"\", 100, 0, 0.2), \"FTAElectron{lpf}_pfRelIso03_chg\", wgtVar)\n",
    "                histos1D_dict[\"Electrons\"][\"cutBased\"] = input_df.Histo1D((\"Electron_cutBased{}\".format(postfix), \"\", 5, 0, 5), \"FTAElectron{lpf}_cutBased\", wgtVar)\n",
    "            if histos2D_dict != None:\n",
    "                if \"Electrons\" not in histos2D_dict: \n",
    "                    histos2D_dict[\"Electrons\"] = {}\n",
    "                histos2D_dict[\"Electrons\"][\"eta_phi\"] = input_df.Histo2D((\"Electron_eta_phi{}\".format(postfix), \"\",\n",
    "                                                                          104, -2.6, 2.6,\n",
    "                                                                          64, -pi, pi),\n",
    "                                                                         \"FTAElectron{lpf}_eta\", \"FTAElectron{lpf}_phi\", wgtVar)\n",
    "                histos2D_dict[\"Electrons\"][\"dz_ip3d\"] = input_df.Histo2D((\"Electron_dz_ip3d{}\".format(postfix), \"\",\n",
    "                                                                          100, -0.01, 0.01,\n",
    "                                                                          100, 0, 0.01),\n",
    "                                                                         \"FTAElectron{lpf}_dz\", \"FTAElectron{lpf}_ip3d\", wgtVar)\n",
    "        if doLeptons == True:\n",
    "            if histos1D_dict != None:\n",
    "                if \"Leptons\" not in histos1D_dict: \n",
    "                    histos1D_dict[\"Leptons\"] = {}\n",
    "                histos1D_dict[\"Leptons\"][\"pt_LeadLep\"] = input_df\\\n",
    "                        .Histo1D((\"FTALepton{lpf}_pt_LeadLep{}\".format(postfix), \"\", 100, 0, 500),\"FTALepton{lpf}_pt_LeadLep\", wgtVar)\n",
    "                histos1D_dict[\"Leptons\"][\"pt_SubleadLep\"] = input_df\\\n",
    "                        .Histo1D((\"FTALepton{lpf}_pt_SubleadLep{}\".format(postfix), \"\", 100, 0, 500),\"FTALepton{lpf}_pt_SubleadLep\", wgtVar)\n",
    "                histos1D_dict[\"Leptons\"][\"eta\"] = input_df\\\n",
    "                        .Histo1D((\"FTALepton{lpf}_eta{}\".format(postfix), \"\", 104, -2.6, 2.6),\"FTALepton{lpf}_eta\", wgtVar)\n",
    "                histos1D_dict[\"Leptons\"][\"phi\"] = input_df\\\n",
    "                        .Histo1D((\"FTALepton{lpf}_phi{}\".format(postfix), \"\", 64, -pi, pi),\"FTALepton{lpf}_phi\", wgtVar)\n",
    "                histos1D_dict[\"Leptons\"][\"nLepton\"] = input_df\\\n",
    "                        .Histo1D((\"nLepton{}\".format(postfix), \"\", 5, 0, 5), \"nFTALepton{lpf}\", wgtVar)\n",
    "                histos1D_dict[\"Leptons\"][\"pdgId\"] = input_df\\\n",
    "                        .Histo1D((\"Lepton_pdgId{}\".format(postfix), \"\", 32, -16, 16), \"FTALepton{lpf}_pdgId\", wgtVar)\n",
    "                histos1D_dict[\"Leptons\"][\"jetIdx\"] = input_df\\\n",
    "                        .Histo1D((\"Lepton_jetIdx{}\".format(postfix), \"\", 20, 0, 20), \"FTALepton{lpf}_jetIdx\", wgtVar)\n",
    "                #histos1D_dict[\"Leptons\"][\"LepSF\"] = input_df\\\n",
    "                #        .Histo1D((\"Lepton_SF_({})\".format(\"wgt_SUMW_PU:HARDCODED\"), \"\", 100, 0.93, 1.03), \"FTALepton{lpf}_SF_nom\", \"wgt_SUMW_PU\")\n",
    "                #histos1D_dict[\"Leptons\"][\"LSF\"] = input_df\\\n",
    "                #        .Histo1D((\"LSF_({})\".format(\"wgt_SUMW_PU:HARDCODED\"), \"\", 200, 0.80, 1.1), \"wgt_LSF\", \"wgt_SUMW_PU\")\n",
    "                #histos1D_dict[\"Leptons\"][\"SPL_SP\"] = input_df\\\n",
    "                #        .Histo1D((\"SPL_SP_({})\".format(\"wgt_SUMW_PU:HARDCODED\"), \"\", 200, 0.80, 1.1), \"SPL_SP\", \"wgt_SUMW_PU\")\n",
    "                #histos1D_dict[\"Leptons\"][\"LepSF\"] = input_df.Histo1D(\"FTALepton{lpf}_SF_nom\")#, \"wgt_SUMW_PU\")\n",
    "                #histos1D_dict[\"Leptons\"][\"LSF\"] = input_df.Histo1D(\"wgt_LSF\")#, \"wgt_SUMW_PU\")\n",
    "                #histos1D_dict[\"Leptons\"][\"SPL_SP\"] = input_df.Histo1D(\"SPL_SP\")#, \"wgt_SUMW_PU\")\n",
    "                #histos1D_dict[\"Leptons\"][\"SUMW_PU\"] = input_df.Histo1D(\"wgt_SUMW_PU\")#, \"wgt_SUMW_PU\")\n",
    "                #histos1D_dict[\"Leptons\"][\"SUMW_PU_LSF\"] = input_df.Histo1D(\"wgt_SUMW_PU_LSF\")#, \"wgt_SUMW_PU\")\n",
    "                #histos1D_dict[\"Leptons\"][\"PU\"] = input_df.Histo1D(\"puWeight\")#, \"wgt_SUMW_PU\")\n",
    "        if doJets == True:\n",
    "            if histos1D_dict != None:\n",
    "                if \"Jets\" not in histos1D_dict:\n",
    "                    histos1D_dict[\"Jets\"] = {}\n",
    "                histos1D_dict[\"Jets\"][\"pt\"] = input_df.Histo1D((\"Jet_pt{}\".format(postfix), \"\", 100, 0, 500), fillJet_pt, wgtVar)\n",
    "                for x in xrange(nJetsToHisto):\n",
    "                    histos1D_dict[\"Jets\"][\"pt_jet{}\".format(x+1)] = input_df.Histo1D((\"Jet_pt_jet{}({})\".format(x+1, wgtVar), \"\", 100, 0, 500), \"Jet{bpf}_pt_jet{}\".format(x+1), wgtVar)\n",
    "                    histos1D_dict[\"Jets\"][\"eta_jet{}\".format(x+1)] = input_df.Histo1D((\"Jet_eta_jet{}({})\".format(x+1, wgtVar), \"\", 104, -2.6, 2.6), \"Jet{bpf}_eta_jet{}\".format(x+1), wgtVar)\n",
    "                    histos1D_dict[\"Jets\"][\"phi_jet{}\".format(x+1)] = input_df.Histo1D((\"Jet_phi_jet{}({})\".format(x+1, wgtVar), \"\", 64, -pi, pi), \"Jet{bpf}_phi_jet{}\".format(x+1), wgtVar)\n",
    "                histos1D_dict[\"Jets\"][\"eta\"] = input_df.Histo1D((\"Jet_eta{}\".format(postfix), \"\", 104, -2.6, 2.6), fillJet_eta, wgtVar)\n",
    "                histos1D_dict[\"Jets\"][\"phi\"] = input_df.Histo1D((\"Jet_phi{}\".format(postfix), \"\", 64, -pi, pi), fillJet_phi, wgtVar)\n",
    "                histos1D_dict[\"Jets\"][\"mass\"] = input_df.Histo1D((\"Jet_mass{}\".format(postfix), \"\", 100, 0, 500), fillJet_mass, wgtVar)\n",
    "                histos1D_dict[\"Jets\"][\"jetId\"] = input_df.Histo1D((\"Jet_jetId{}\".format(postfix), \"\", 8, 0, 8), \"Jet{bpf}_jetId\", wgtVar) #FIXME: not based on variation... okay? maybe not with masks...\n",
    "                #histos1D_dict[\"Jets\"][\"btagDeepB_LeadtagJet\"] = input_df.Histo1D((\"Jet_btagDeepB_LeadtagJet{}\".format(postfix), \"\", 101, -0.01, 1), \"Jet{bpf}_btagDeepB_LeadtagJet\", wgtVar)\n",
    "                #histos1D_dict[\"Jets\"][\"btagDeepB_SubleadtagJet\"] = input_df.Histo1D((\"Jet_btagDeepB_SubleadtagJet{}\".format(postfix), \"\", 101, -0.01, 1), \"Jet{bpf}_btagDeepB_SubleadtagJet\", wgtVar)\n",
    "                #histos1D_dict[\"Jets\"][\"btagDeepJet_LeadtagJet\"] = input_df.Histo1D((\"Jet_btagDeepJetB_LeadtagJet{}\".format(postfix), \"\", 101, -0.01, 1), \"Jet{bpf}_btagDeepFlavB_sorted_LeadtagJet\", wgtVar)\n",
    "                #histos1D_dict[\"Jets\"][\"btagDeepJet_SubleadtagJet\"] = input_df.Histo1D((\"Jet_btagDeepJetB_SubleadtagJet{}\".format(postfix), \"\", 101, -0.01, 1), \"Jet{bpf}_btagDeepFlavB_sorted_SubleadtagJet\", wgtVar)\n",
    "                #histos1D_dict[\"Jets\"][\"nMediumCSVv2\"] = input_df.Histo1D((\"nJet_MediumCSVv2{}\".format(postfix), \"\", 10, 0, 10), \"nJet{bpf}_MediumCSVv2\", wgtVar)\n",
    "                histos1D_dict[\"Jets\"][\"nMediumDeepCSV\"] = input_df.Histo1D((\"nJet_MediumDeepCSV{}\".format(postfix), \"\", 10, 0, 10), \"nJet{bpf}_MediumDeepCSV\", wgtVar)\n",
    "                histos1D_dict[\"Jets\"][\"nMediumDeepJet\"] = input_df.Histo1D((\"nJet_MediumDeepJet{}\".format(postfix), \"\", 10, 0, 10), \"nJet{bpf}_MediumDeepJet\", wgtVar)\n",
    "                histos1D_dict[\"Jets\"][\"nJet\"] = input_df.Histo1D((\"nJet{}\".format(postfix), \"\", 15, 0, 15), \"nJet{bpf}\", wgtVar)\n",
    "                histos1D_dict[\"Jets\"][\"dR_Jet_Mu_leading\"] = input_df.Histo1D((\"dR_Jet_Mu_leading{}\".format(postfix), \"dR(Jet, #mu_{leading}); dR; Events)\", 40, 0, 0.8), \"dR_Jet_Mu_leading\", wgtVar)\n",
    "                histos1D_dict[\"Jets\"][\"dR_Jet_Mu_sublead\"] = input_df.Histo1D((\"dR_Jet_Mu_sublead{}\".format(postfix), \"dR(Jet, #mu_{subleading}); dR; Events)\", 40, 0, 0.8), \"dR_Jet_Mu_sublead\", wgtVar)\n",
    "                histos1D_dict[\"Jets\"][\"dR_Jet_El_leading\"] = input_df.Histo1D((\"dR_Jet_El_leading{}\".format(postfix), \"dR(Jet, #e_{leading}); dR; Events)\", 40, 0, 0.8), \"dR_Jet_El_leading\", wgtVar)\n",
    "                histos1D_dict[\"Jets\"][\"dR_Jet_El_sublead\"] = input_df.Histo1D((\"dR_Jet_El_sublead{}\".format(postfix), \"dR(Jet, #e_{subleading}); dR; Events)\", 40, 0, 0.8), \"dR_Jet_El_sublead\", wgtVar)\n",
    "            \n",
    "                if debugInfo == True:\n",
    "                    #histos1D_dict[\"Jets\"][\"DiffMaskVsALT\"] = input_df.Histo1D((\"DiffMaskVsALT\", \"\", 10, -10, 10), \"DiffMaskVsALT\", wgtVar)\n",
    "                    #histos1D_dict[\"Jets\"][\"DiffnJet\"] = input_df.Histo1D((\"DiffnJet\", \"\", 10, -10, 10), \"DiffnJet\", wgtVar)\n",
    "                    histos1D_dict[\"Jets\"][\"DeepJetSorted\"] = input_df.Histo1D(\"DeepJetSorted\", wgtVar)\n",
    "                    histos1D_dict[\"Jets\"][\"DeepJetLeadtagMinusSubleadtag\"] = input_df.Histo1D((\"DeepJetLeadtagMinusSubleadtag\", \"DeepJet(Leadtag - Subleadtag);;Events\", 100, -1, 1), \"DeepJet0Minus1\", wgtVar)\n",
    "                    histos1D_dict[\"Jets\"][\"MediumDeepJetSorted\"] = input_df.Histo1D(\"MediumDeepJetSorted\", wgtVar)\n",
    "                    #histos1D_dict[\"Jets\"][\"MediumDeepJet0Minus1\"] = input_df.Histo1D((\"MediumDeepJet0Minus1\", \"\", 100, -1, 1), \"MediumDeepJet0Minus1\", wgtVar)\n",
    "                    #histos1D_dict[\"Jets\"][\"btagDeepJet_jet0Med\"] = input_df.Histo1D((\"Jet_btagDeepJetB_jet0Med{}\".format(postfix), \"\", 102, -0.02, 1), \"Jet{bpf}_btagDeepFlavB_jet0Med\", wgtVar)\n",
    "                    #histos1D_dict[\"Jets\"][\"btagDeepJet_jet1Med\"] = input_df.Histo1D((\"Jet_btagDeepJetB_jet1Med{}\".format(postfix), \"\", 102, -0.02, 1), \"Jet{bpf}_btagDeepFlavB_jet1Med\", wgtVar)\n",
    "                    #histos1D_dict[\"Jets\"][\"nJetNUMW\"] = input_df.Histo1D((\"nJet_NUMW\", \"\", 15, 0, 15), \"nJet{bpf}\", \"wgt_NUMW_V2\")\n",
    "                    #histos1D_dict[\"Jets\"][\"nJetSUMW_PU\"] = input_df.Histo1D((\"nJet_SUMW_PU\", \"\", 15, 0, 15), \"nJet{bpf}\", \"wgt_SUMW_PU\")\n",
    "                    #histos1D_dict[\"Jets\"][\"nJetSUMW_LSF\"] = input_df.Histo1D((\"nJet_SUMW_LSF\", \"\", 15, 0, 15), \"nJet{bpf}\", \"wgt_SUMW_LSF\")\n",
    "                    #histos1D_dict[\"Jets\"][\"ptALT\"] = input_df.Histo1D((\"Jet_ptALT{}\".format(postfix), \"\", 100, 0, 500), \"Jet{bpf}_ptALT\", wgtVar)\n",
    "                    #histos1D_dict[\"Jets\"][\"etaALT\"] = input_df.Histo1D((\"Jet_etaALT{}\".format(postfix), \"\", 104, -2.6, 2.6), \"Jet{bpf}_etaALT\", wgtVar)\n",
    "                    #histos1D_dict[\"Jets\"][\"phiALT\"] = input_df.Histo1D((\"Jet_phiALT{}\".format(postfix), \"\", 64, -pi, pi), \"Jet{bpf}_phiALT\", wgtVar)\n",
    "                    #histos1D_dict[\"Jets\"][\"massALT\"] = input_df.Histo1D((\"Jet_massALT{}\".format(postfix), \"\", 100, 0, 500), \"Jet{bpf}_massALT\", wgtVar)\n",
    "                    #histos1D_dict[\"Jets\"][\"jetIdALT\"] = input_df.Histo1D((\"Jet_jetIdALT{}\".format(postfix), \"\", 8, 0, 8), \"Jet{bpf}_jetIdALT\", wgtVar)\n",
    "        \n",
    "            if histos2D_dict != None:\n",
    "                if \"Jets\" not in histos2D_dict:\n",
    "                    histos2D_dict[\"Jets\"] = {}\n",
    "                histos2D_dict[\"Jets\"][\"eta_phi\"] = input_df.Histo2D((\"Jet_eta_phi{}\".format(postfix), \"\",\n",
    "                                                                     104, -2.6, 2.6,\n",
    "                                                                     64, -pi, pi),\n",
    "                                                                    fillJet_eta, fillJet_phi, wgtVar)\n",
    "        if doEventVars == True:\n",
    "            if histos1D_dict != None:\n",
    "                if \"EventVars\" not in histos1D_dict:\n",
    "                    histos1D_dict[\"EventVars\"] = {}\n",
    "                #histos1D_dict[\"EventVars\"][\"JML_baseline\"] = input_df.Histo1D((\"JML_baseline{}\".format(postfix), \"\", 2,0,2), \"JML_baseline_pass\", wgtVar)\n",
    "                #histos1D_dict[\"EventVars\"][\"JML_selection\"] = input_df.Histo1D((\"JML_selection{}\".format(postfix), \"\", 2,0,2), \"JML_selection_pass\", wgtVar)\n",
    "                #histos1D_dict[\"EventVars\"][\"HT_baseline\"] = input_df.Histo1D((\"ESV_JetMETLogic_HT_baseline{}\".format(postfix), \"\", 100,400,1400), \"ESV_JetMETLogic_HT_baseline\", wgtVar)\n",
    "                #histos1D_dict[\"EventVars\"][\"H_baseline\"] = input_df.Histo1D((\"ESV_JetMETLogic_H_baseline{}\".format(postfix), \"\", 100,400,1400), \"ESV_JetMETLogic_H_baseline\", wgtVar)\n",
    "                #histos1D_dict[\"EventVars\"][\"HT2M_baseline\"] = input_df.Histo1D((\"ESV_JetMETLogic_HT2M_baseline{}\".format(postfix), \"\", 100,400,900), \"ESV_JetMETLogic_HT2M_baseline\", wgtVar)\n",
    "                #histos1D_dict[\"EventVars\"][\"H2M_baseline\"] = input_df.Histo1D((\"ESV_JetMETLogic_H2M_baseline{}\".format(postfix), \"\", 100,400,900), \"ESV_JetMETLogic_H2M_baseline\", wgtVar)\n",
    "                #histos1D_dict[\"EventVars\"][\"HTb_baseline\"] = input_df.Histo1D((\"ESV_JetMETLogic_HTb_baseline{}\".format(postfix), \"\", 100,400,900), \"ESV_JetMETLogic_HTb_baseline\", wgtVar)\n",
    "                #histos1D_dict[\"EventVars\"][\"HTH_baseline\"] = input_df.Histo1D((\"ESV_JetMETLogic_HTH_baseline{}\".format(postfix), \"\", 100,0,1), \"ESV_JetMETLogic_HTH_baseline\", wgtVar)\n",
    "                #histos1D_dict[\"EventVars\"][\"HTRat_baseline\"] = input_df.Histo1D((\"ESV_JetMETLogic_HTRat_baseline{}\".format(postfix), \"\", 100,0,1), \"ESV_JetMETLogic_HTRat_baseline\", wgtVar)\n",
    "                #histos1D_dict[\"EventVars\"][\"dRbb_baseline\"] = input_df.Histo1D((\"ESV_JetMETLogic_dRbb_baseline{}\".format(postfix), \"\", 64,0,2*pi), \"ESV_JetMETLogic_dRbb_baseline\", wgtVar)\n",
    "                #histos1D_dict[\"EventVars\"][\"DiLepMass_baseline\"] = input_df.Histo1D((\"ESV_JetMETLogic_DiLepMass_baseline{}\".format(postfix), \"\", 100,0,500), \"ESV_JetMETLogic_DiLepMass_baseline\", wgtVar)\n",
    "                #histos1D_dict[\"EventVars\"][\"HT_selection\"] = input_df.Histo1D((\"ESV_JetMETLogic_HT_selection{}\".format(postfix), \"\", 100,400,1400), \"ESV_JetMETLogic_HT_selection\", wgtVar)\n",
    "                #histos1D_dict[\"EventVars\"][\"H_selection\"] = input_df.Histo1D((\"ESV_JetMETLogic_H_selection{}\".format(postfix), \"\", 100,400,1400), \"ESV_JetMETLogic_H_selection\", wgtVar)\n",
    "                #histos1D_dict[\"EventVars\"][\"HT2M_selection\"] = input_df.Histo1D((\"ESV_JetMETLogic_HT2M_selection{}\".format(postfix), \"\", 100,400,900), \"ESV_JetMETLogic_HT2M_selection\", wgtVar)\n",
    "                #histos1D_dict[\"EventVars\"][\"H2M_selection\"] = input_df.Histo1D((\"ESV_JetMETLogic_H2M_selection{}\".format(postfix), \"\", 100,400,900), \"ESV_JetMETLogic_H2M_selection\", wgtVar)\n",
    "                #histos1D_dict[\"EventVars\"][\"HTb_selection\"] = input_df.Histo1D((\"ESV_JetMETLogic_HTb_selection{}\".format(postfix), \"\", 100,400,900), \"ESV_JetMETLogic_HTb_selection\", wgtVar)\n",
    "                #histos1D_dict[\"EventVars\"][\"HTH_selection\"] = input_df.Histo1D((\"ESV_JetMETLogic_HTH_selection{}\".format(postfix), \"\", 100,0,1), \"ESV_JetMETLogic_HTH_selection\", wgtVar)\n",
    "                #histos1D_dict[\"EventVars\"][\"HTRat_selection\"] = input_df.Histo1D((\"ESV_JetMETLogic_HTRat_selection{}\".format(postfix), \"\", 100,0,1), \"ESV_JetMETLogic_HTRat_selection\", wgtVar)\n",
    "                #histos1D_dict[\"EventVars\"][\"dRbb_selection\"] = input_df.Histo1D((\"ESV_JetMETLogic_dRbb_selection{}\".format(postfix), \"\", 64,0,2*pi), \"ESV_JetMETLogic_dRbb_selection\", wgtVar)\n",
    "                #histos1D_dict[\"EventVars\"][\"DiLepMass_selection\"] = input_df.Histo1D((\"ESV_JetMETLogic_DiLepMass_selection{}\".format(postfix), \"\", 100,0,500), \"ESV_JetMETLogic_DiLepMass_selection\", wgtVar)\n",
    "                histos1D_dict[\"EventVars\"][\"MET_pt\"] = input_df.Histo1D((\"MET_xycorr_pt{}\".format(postfix), \"\", 100,30,1030), fillMET_pt, wgtVar)\n",
    "                histos1D_dict[\"EventVars\"][\"MET_phi\"] = input_df.Histo1D((\"MET_xycorr_phi{}\".format(postfix), \"\", 100,-pi,pi), fillMET_phi, wgtVar)\n",
    "                histos1D_dict[\"EventVars\"][\"HT\"] = input_df.Histo1D((\"HT{}\".format(postfix), \"\", 130,400,1700), \"FTAJet{bpf}_HT\", wgtVar)\n",
    "                histos1D_dict[\"EventVars\"][\"H\"] = input_df.Histo1D((\"H{}\".format(postfix), \"\", 160,400,2000), \"FTAJet{bpf}_H\", wgtVar)\n",
    "                histos1D_dict[\"EventVars\"][\"HT2M\"] = input_df.Histo1D((\"HT2M{}\".format(postfix), \"\", 100,0,1000), \"FTAJet{bpf}_HT2M\", wgtVar)\n",
    "                histos1D_dict[\"EventVars\"][\"H2M\"] = input_df.Histo1D((\"H2M{}\".format(postfix), \"\", 150,0,1500), \"FTAJet{bpf}_H2M\", wgtVar)\n",
    "                histos1D_dict[\"EventVars\"][\"HTb\"] = input_df.Histo1D((\"HTb{}\".format(postfix), \"\", 100,0,1000), \"FTAJet{bpf}_HTb\", wgtVar)\n",
    "                histos1D_dict[\"EventVars\"][\"HTH\"] = input_df.Histo1D((\"HTH{}\".format(postfix), \"\", 100,0,1), \"FTAJet{bpf}_HTH\", wgtVar)\n",
    "                histos1D_dict[\"EventVars\"][\"HTRat\"] = input_df.Histo1D((\"HTRat{}\".format(postfix), \"\", 100,0,1), \"FTAJet{bpf}_HTRat\", wgtVar)\n",
    "                histos1D_dict[\"EventVars\"][\"dRbb\"] = input_df.Histo1D((\"dRbb{}\".format(postfix), \"\", 64,0,2*pi), \"FTAJet{bpf}_dRbb\", wgtVar)\n",
    "                histos1D_dict[\"EventVars\"][\"dPhibb\"] = input_df.Histo1D((\"dPhibb{}\".format(postfix), \"\", 64,-pi,pi), \"FTAJet{bpf}_dPhibb\", wgtVar)\n",
    "                histos1D_dict[\"EventVars\"][\"dEtabb\"] = input_df.Histo1D((\"dEtabb{}\".format(postfix), \"\", 50,0,5), \"FTAJet{bpf}_dEtabb\", wgtVar)\n",
    "                histos1D_dict[\"EventVars\"][\"dRll\"] = input_df.Histo1D((\"dRll{}\".format(postfix), \"\", 64,0,2*pi), \"FTALepton{lpf}_dRll\", wgtVar)\n",
    "                histos1D_dict[\"EventVars\"][\"dPhill\"] = input_df.Histo1D((\"dPhill{}\".format(postfix), \"\", 64,-pi,pi), \"FTALepton{lpf}_dPhill\", wgtVar)\n",
    "                histos1D_dict[\"EventVars\"][\"dEtall\"] = input_df.Histo1D((\"dEtall{}\".format(postfix), \"\", 50,0,5), \"FTALepton{lpf}_dEtall\", wgtVar)\n",
    "                histos1D_dict[\"EventVars\"][\"MTofMETandEl\"] = input_df.Histo1D((\"MTofMETandEl{}\".format(postfix), \"\", 100, 0, 200), \"MTofMETandEl\", wgtVar)\n",
    "                histos1D_dict[\"EventVars\"][\"MTofMETandMu\"] = input_df.Histo1D((\"MTofMETandMu{}\".format(postfix), \"\", 100, 0, 200), \"MTofMETandMu\", wgtVar)\n",
    "                histos1D_dict[\"EventVars\"][\"MTofElandMu\"] = input_df.Histo1D((\"MTofElandMu{}\".format(postfix), \"\", 100, 0, 200), \"MTofElandMu\", wgtVar)\n",
    "                #histos1D_dict[\"EventVars\"][\"MTMasslessCheck\"] = input_df.Histo1D((\"MTMasslessCheck{}\".format(postfix), \"\", 100, 0, 200), \"MTMasslessCheck\", wgtVar)\n",
    "                #histos1D_dict[\"EventVars\"][\"MTCrossCheck\"] = input_df.Histo1D((\"MTCrossCheck{}\".format(postfix), \"\", 100, 0, 200), \"MTCrossCheck\", wgtVar)\n",
    "                #histos1D_dict[\"EventVars\"][\"MTCrossCheckDifference\"] = input_df.Histo1D((\"MTCrossCheckDifference{}\".format(postfix), \"\", 100, 0, 10), \"MTCrossCheckDifference\", wgtVar)\n",
    "                #histos1D_dict[\"EventVars\"][\"MTCrossCheckMasslessDifference\"] = input_df.Histo1D((\"MTCrossCheckMasslessDifference{}\".format(postfix), \"\", 100, 0, 0.02), \"MTCrossCheckMasslessDifference\", wgtVar)\n",
    "                histos1D_dict[\"EventVars\"][\"PV_npvsGood\"] = input_df.Histo1D((\"PV_npvsGood{}\".format(postfix), \"\", 100, 0, 100), \"PV_npvsGood\", wgtVar)\n",
    "                histos1D_dict[\"EventVars\"][\"PV_npvs\"] = input_df.Histo1D((\"PV_npvs{}\".format(postfix), \"\", 150, 0, 150), \"PV_npvs\", wgtVar)\n",
    "                if isData == False:\n",
    "                    histos1D_dict[\"EventVars\"][\"Pileup_nTrueInt\"] = input_df.Histo1D((\"Pileup_TrueInt{}\".format(postfix), \";Pileup_TrueInt;Events\", 150, 0, 150), \"Pileup_nTrueInt\", wgtVar)\n",
    "                    histos1D_dict[\"EventVars\"][\"Pileup_nTrueInt_XS\"] = input_df.Histo1D((\"Pileup_TrueInt_({})\".format(\"wgt_SUMW\"), \";Pileup_TrueInt;Events\", 150, 0, 150), \"Pileup_nTrueInt\", \"wgt_SUMW\")\n",
    "                    histos1D_dict[\"EventVars\"][\"Pileup_nPU_XS\"] = input_df.Histo1D((\"Pileup_nPU_({})\".format(\"wgt_SUMW\"), \";Pileup_nPU;Events\", 150, 0, 150), \"Pileup_nPU\", \"wgt_SUMW\")\n",
    "                    histos1D_dict[\"EventVars\"][\"Pileup_nPU\"] = input_df.Histo1D((\"Pileup_nPU{}\".format(postfix), \";Pileup_nPU;Events\", 150, 0, 150), \"Pileup_nPU\", wgtVar)\n",
    "            \n",
    "            if histos2D_dict != None:\n",
    "                if \"EventVars\" not in histos2D_dict:\n",
    "                    histos2D_dict[\"EventVars\"] = {}\n",
    "                if isData == False:\n",
    "                    histos2D_dict[\"EventVars\"][\"npvsGood_vs_nTrueInt\"] = input_df.Histo2D((\"npvsGood_vs_nTrueInt{}\".format(postfix), \";nTrueInt;npvsGood\", 150, 0, 150, 150, 0, 150), \"Pileup_nTrueInt\", \"PV_npvsGood\", wgtVar)\n",
    "                    histos2D_dict[\"EventVars\"][\"npvsGood_vs_nPU\"] = input_df.Histo2D((\"npvsGood_vs_nPU{}\".format(postfix), \";nPU;npvsGood\", 150, 0, 150, 150, 0, 150), \"Pileup_nPU\", \"PV_npvsGood\", wgtVar)\n",
    "                    histos2D_dict[\"EventVars\"][\"npvs_vs_nTrueInt\"] = input_df.Histo2D((\"npvs_vs_nTrueInt{}\".format(postfix), \";nTrueInt;npvs\", 150, 0, 150, 150, 0, 150), \"Pileup_nTrueInt\", \"PV_npvs\", wgtVar)\n",
    "                    histos2D_dict[\"EventVars\"][\"npvs_vs_nPU\"] = input_df.Histo2D((\"npvs_vs_nPU{}\".format(postfix), \";nPU;npvs\", 150, 0, 150, 150, 0, 150), \"Pileup_nPU\", \"PV_npvs\", wgtVar)\n",
    "                \n",
    "                if debugInfo == True:\n",
    "                    #histos1D_dict[\"EventVars\"][\"Jet{bpf}_HT_Match\"] = input_df.Histo1D((\"Jet{bpf}_HT_Match{}\".format(postfix), \"\", 2,0,2), \"Jet{bpf}_HT_matches\", wgtVar)\n",
    "                    pass\n",
    "            \n",
    "        if makeMountains == True:\n",
    "            if useDeepCSV == True:\n",
    "                tagger = \"CSV\"\n",
    "            else:\n",
    "                tagger = \"Jet\"\n",
    "            theCatsL0 = collections.OrderedDict()#Baseline nJet selection (inclusive!) for each systematic scale variation\n",
    "            theCatsL1 = collections.OrderedDict() #Next level of selection, like nJet categories. If we need nBTag inclusive, they'll have to go here\n",
    "            theCatsL2 = collections.OrderedDict() #Next level of selection, i.e. nBTag categories. \n",
    "            theCatsL0[\"Inclusive{bpf}\".format(bpf=branchpostfix)] = \"nFTAJet{bpf} >= 4\".format(bpf=branchpostfix)\n",
    "            \n",
    "            theCatsL1[\"nMediumDeep{tag}B0{bpf}\".format(tag=tagger, bpf=branchpostfix)] = \"nMediumDeep{tag}B{bpf} == 0\".format(tag=tagger, bpf=branchpostfix)\n",
    "            theCatsL1[\"nMediumDeep{tag}B1{bpf}\".format(tag=tagger, bpf=branchpostfix)] = \"nMediumDeep{tag}B{bpf} == 1\".format(tag=tagger, bpf=branchpostfix)\n",
    "            theCatsL1[\"nMediumDeep{tag}B2{bpf}\".format(tag=tagger, bpf=branchpostfix)] = \"nMediumDeep{tag}B{bpf} == 2\".format(tag=tagger, bpf=branchpostfix)\n",
    "            theCatsL1[\"blind_nMediumDeep{tag}B3{bpf}\".format(tag=tagger, bpf=branchpostfix)] = \"nMediumDeep{tag}B{bpf} == 3\".format(tag=tagger, bpf=branchpostfix)\n",
    "            theCatsL1[\"blind_nMediumDeep{tag}B4+{bpf}\".format(tag=tagger, bpf=branchpostfix)] = \"nMediumDeep{tag}B{bpf} >= 4\".format(tag=tagger, bpf=branchpostfix)\n",
    "            \n",
    "            theCatsL2[\"nJet4{bpf}\".format(bpf=branchpostfix)] = \"nFTAJet{bpf} == 4\".format(bpf=branchpostfix)\n",
    "            theCatsL2[\"nJet5{bpf}\".format(bpf=branchpostfix)] = \"nFTAJet{bpf} == 5\".format(bpf=branchpostfix)\n",
    "            theCatsL2[\"blind_nJet6{bpf}\".format(bpf=branchpostfix)] = \"nFTAJet{bpf} == 6\".format(bpf=branchpostfix)\n",
    "            theCatsL2[\"blind_nJet7{bpf}\".format(bpf=branchpostfix)] = \"nFTAJet{bpf} == 7\".format(bpf=branchpostfix)\n",
    "            theCatsL2[\"blind_nJet8+{bpf}\".format(bpf=branchpostfix)] = \"nFTAJet{bpf} >= 8\".format(bpf=branchpostfix)\n",
    "            \n",
    "            #Don't redefine the nodes by accident! Do not redo for weight variations\n",
    "            rdf = input_df\n",
    "            if HTCut:\n",
    "                rdf = rdf.Filter(\"HT{bpf} >= {cut}\".format(bpf=branchpostfix, cut=HTCut))\n",
    "            if ZMassMETWindow:\n",
    "                rdf = rdf.Filter(\"{met} >= {metcut}\".format(met=fillMET_pt, metcut=ZMassMETWindow[1]),\n",
    "                               \"MET >= {metcut}\".format(metcut=ZMassMETWindow[1]))\n",
    "                rdf = rdf.Filter(\"(FTAElectron{lpf}_InvariantMass > 0 && abs(FTAElectron{lpf}_InvariantMass - 91.2) < {zwidth}) || \"\\\n",
    "                                 \"(FTAMuon{lpf}_InvariantMass > 0 && abs(FTAMuon{lpf}_InvariantMass - 91.2) < {zwidth})\".format(lpf=leppostfix,\n",
    "                                                                                                                               zwidth=ZMassMETWindow[0]\n",
    "                                                                                                                               ),\n",
    "                                 \"abs(Dilepton Invariant Mass - Z_mass) < {zwidth}\".format(zwidth=ZMassMETWindow[0]))\n",
    "            if not isWeightVariation:\n",
    "                for ck0, cs0 in theCatsL0.items():\n",
    "                    #Strip the FourTopAnalysis- (FTA) from the collection strings when naming the filters\n",
    "                    if ck0 in cat_df:\n",
    "                        print(\"ERROR! Attempted redefinition of the filter node for categorization. Check logic\")\n",
    "                        continue\n",
    "                    cat_df[ck0] = rdf.Filter(cs0, cs0.replace(\"FTA\", \"\"))\n",
    "                    for ck1, cs1 in theCatsL1.items():\n",
    "                        #Strip the FourTopAnalysis- (FTA) from the collection strings when naming the filters\n",
    "                        cat_df[ck1] = cat_df[ck0].Filter(cs1, cs1.replace(\"FTA\", \"\"))\n",
    "                        for ck2, cs2 in theCatsL2.items():\n",
    "                            l1 = ck1.replace(\"blind_\", \"\").replace(\"{bpf}\".format(bpf=branchpostfix), \"\")\n",
    "                            l2 = ck2.replace(\"blind_\", \"\").replace(\"{bpf}\".format(bpf=branchpostfix), \"\")\n",
    "                            if \"nJet7\" in l2 or \"nJet8+\" in l2 or \"nMediumDeep{tag}B3\".format(tag=tagger) in l1 or \"nMediumDeep{tag}B4\" in l1:\n",
    "                                #Blind these regions in the cross-group!\n",
    "                                cknest = \"blind_{c1}_{c2}_{bpf}\".format(c1=l2, c2=l1, bpf=branchpostfix)\n",
    "                            else:\n",
    "                                cknest = \"{c1}_{c2}_{bpf}\".format(c1=l2, c2=l1, bpf=branchpostfix)\n",
    "                            #Strip the FourTopAnalysis- (FTA) from the collection strings when naming the filters\n",
    "                            cat_df[cknest] = cat_df[ck1].Filter(cs2, \"{} && {}\".format(cs1.replace(\"FTA\", \"\"), cs2.replace(\"FTA\", \"\")))\n",
    "                                                                \n",
    "        \n",
    "            if histos1D_dict != None:\n",
    "                if \"sysVar{spf}\".format(spf=syspostfix) not in histos1D_dict:\n",
    "                    histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)] = {}\n",
    "                for tc in cat_df.keys(): \n",
    "                    if tc not in histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)]: \n",
    "                        histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc] = {}\n",
    "                for tc, node in cat_df.items():\n",
    "                    if verbose:\n",
    "                        print(\"Histogramming the {} node\".format(tc))\n",
    "                    tcn = tc#.replace(\"blind_\", \"\") #FIXME do I want blind stripped with new convention for naming?\n",
    "                    for x in xrange(nJetsToHisto):\n",
    "                        #FIXME: None of these are based upon jet scale variations, yet!\n",
    "                        histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"Jet_pt_jet{}\".format(x+1)] = cat_df[tc].Histo1D((\"{name}__{cat}__Jet_pt_jet{n}_{spf}\".format(name=sampleName, n=x+1, cat=tcn, spf=syspostfix), \"\", 100, 0, 500), \"{fj}_pt_jet{n}\".format(fj=fillJet, n=x+1), wgtVar)\n",
    "                        histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"Jet_eta_jet{}\".format(x+1)] = cat_df[tc].Histo1D((\"{name}__{cat}__Jet_eta_jet{n}_{spf}\".format(name=sampleName, n=x+1, cat=tcn, spf=syspostfix), \"\", 104, -2.6, 2.6), \"{fj}_eta_jet{n}\".format(fj=fillJet, n=x+1), wgtVar)\n",
    "                        histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"Jet_phi_jet{}\".format(x+1)] = cat_df[tc].Histo1D((\"{name}__{cat}__Jet_phi_jet{n}_{spf}\".format(name=sampleName, n=x+1, cat=tcn, spf=syspostfix), \"\", 64, -pi, pi), \"{fj}_phi_jet{n}\".format(fj=fillJet, n=x+1), wgtVar)\n",
    "                        histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"Jet_DeepCSVB_jet{}\".format(x+1)] = cat_df[tc].Histo1D((\"{name}__{cat}__Jet_DeepCSVB_jet{n}_{spf}\".format(name=sampleName, n=x+1, cat=tcn, spf=syspostfix), \"\", 100, 0.0, 1.0), \"{fj}_DeepCSVB_jet{n}\".format(fj=fillJet, n=x+1), wgtVar)\n",
    "                        histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"Jet_DeepJetB_jet{}\".format(x+1)] = cat_df[tc].Histo1D((\"{name}__{cat}__Jet_DeepJetB_jet{n}_{spf}\".format(name=sampleName, n=x+1, cat=tcn, spf=syspostfix), \"\", 100, 0.0, 1.0), \"{fj}_DeepJetB_jet{n}\".format(fj=fillJet, n=x+1), wgtVar)\n",
    "                        histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"Jet_DeepCSVB_sortedjet{}\".format(x+1)] = cat_df[tc].Histo1D((\"{name}__{cat}__Jet_DeepCSVB_jet{n}_{spf}\".format(name=sampleName, n=x+1, cat=tcn, spf=syspostfix), \"\", 100, 0.0, 1.0), \"{fj}_DeepCSVB_sortedjet{n}\".format(fj=fillJet, n=x+1), wgtVar)\n",
    "                        histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"Jet_DeepJetB_sortedjet{}\".format(x+1)] = cat_df[tc].Histo1D((\"{name}__{cat}__Jet_DeepJetB_jet{n}_{spf}\".format(name=sampleName, n=x+1, cat=tcn, spf=syspostfix), \"\", 100, 0.0, 1.0), \"{fj}_DeepJetB_sortedjet{n}\".format(fj=fillJet, n=x+1), wgtVar)\n",
    "                    histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"MET_pt\"] = cat_df[tc].Histo1D((\"{name}__{cat}__MET_pt_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \"\", 20,0,1000), fillMET_pt, wgtVar)\n",
    "                    histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"MET_phi\"] = cat_df[tc].Histo1D((\"{name}__{cat}__MET_phi_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \"\", 20,-pi,pi), fillMET_phi, wgtVar)\n",
    "                    histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"Muon_pfRelIso03_all\"] = cat_df[tc].Histo1D((\"{name}__{cat}__Muon_pfRelIso03_all_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \"\", 20, 0, 0.2), \"FTAMuon{lpf}_pfRelIso03_all\".format(lpf=leppostfix), wgtVar)\n",
    "                    histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"Muon_pfRelIso03_chg\"] = cat_df[tc].Histo1D((\"{name}__{cat}__Muon_pfRelIso03_chg_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \"\", 20, 0, 0.2), \"FTAMuon{lpf}_pfRelIso03_chg\".format(lpf=leppostfix), wgtVar)\n",
    "                    histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"Muon_pfRelIso04_all\"] = cat_df[tc].Histo1D((\"{name}__{cat}__Muon_pfRelIso04_all_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \"\", 20, 0, 0.2), \"FTAMuon{lpf}_pfRelIso04_all\".format(lpf=leppostfix), wgtVar)\n",
    "                    histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"Electron_pfRelIso03_all\"] = cat_df[tc].Histo1D((\"{name}__{cat}__Electron_pfRelIso03_all_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \"\", 20, 0, 0.2), \"FTAElectron{lpf}_pfRelIso03_all\".format(lpf=leppostfix), wgtVar)\n",
    "                    histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"Electron_pfRelIso03_chg\"] = cat_df[tc].Histo1D((\"{name}__{cat}__Electron_pfRelIso03_chg_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \"\", 20, 0, 0.2), \"FTAElectron{lpf}_pfRelIso03_chg\".format(lpf=leppostfix), wgtVar)\n",
    "                    histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"HT\"] = cat_df[tc].Histo1D((\"{name}__{cat}__HT_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \"\", 30,400,2000), \"HT{bpf}\".format(bpf=branchpostfix), wgtVar)\n",
    "                    histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"H\"] = cat_df[tc].Histo1D((\"{name}__{cat}__H_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \"\", 30,400,2000), \"H{bpf}\".format(bpf=branchpostfix), wgtVar)\n",
    "                    histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"HT2M\"] = cat_df[tc].Histo1D((\"{name}__{cat}__HT2M_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \"\", 20,0,1000), \"HT2M{bpf}\".format(bpf=branchpostfix), wgtVar)\n",
    "                    histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"H2M\"] = cat_df[tc].Histo1D((\"{name}__{cat}__H2M_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \"\", 20,0,1500), \"H2M{bpf}\".format(bpf=branchpostfix), wgtVar)\n",
    "                    histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"HTb\"] = cat_df[tc].Histo1D((\"{name}__{cat}__HTb_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \"\", 20,0,1000), \"HTb{bpf}\".format(bpf=branchpostfix), wgtVar)\n",
    "                    histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"HTH\"] = cat_df[tc].Histo1D((\"{name}__{cat}__HTH_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \"\", 20,0,1), \"HTH{bpf}\".format(bpf=branchpostfix), wgtVar)\n",
    "                    histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"HTRat\"] = cat_df[tc].Histo1D((\"{name}__{cat}__HTRat_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \"\", 20,0,1), \"HTRat{bpf}\".format(bpf=branchpostfix), wgtVar)\n",
    "                    histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"dRbb\"] = cat_df[tc].Histo1D((\"{name}__{cat}__dRbb_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \"\", 16,0,2*pi), \"dRbb{bpf}\".format(bpf=branchpostfix), wgtVar)\n",
    "                    histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"dPhibb\"] = cat_df[tc].Histo1D((\"{name}__{cat}__dPhibb_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \"\", 16,-pi,pi), \"dPhibb{bpf}\".format(bpf=branchpostfix), wgtVar)\n",
    "                    histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"dEtabb\"] = cat_df[tc].Histo1D((\"{name}__{cat}__dEtabb_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \"\", 10,0,5), \"dEtabb{bpf}\".format(bpf=branchpostfix), wgtVar)\n",
    "                    histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"Lepton{lpf}_pt_LeadLep\".format(lpf=leppostfix)] = cat_df[tc].Histo1D((\"{name}__{cat}__Lepton{lpf}_pt_LeadLep_{spf}\".format(name=sampleName, cat=tcn, lpf=leppostfix, spf=syspostfix), \"\", 100,0,500), \"FTALepton{lpf}_pt_LeadLep\".format(lpf=leppostfix), wgtVar)\n",
    "                    histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"Lepton{lpf}_pt_SubleadLep\".format(lpf=leppostfix)] = cat_df[tc].Histo1D((\"{name}__{cat}__Lepton{lpf}_pt_SubleadLep_{spf}\".format(name=sampleName, cat=tcn, lpf=leppostfix, spf=syspostfix), \"\", 100,0,500), \"FTALepton{lpf}_pt_SubleadLep\".format(lpf=leppostfix), wgtVar)\n",
    "                    #histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"Muon{lpf}_pt\".format(lpf=leppostfix)] = cat_df[tc].Histo1D((\"{name}__{cat}__Muon{lpf}_pt_{spf}\".format(name=sampleName, cat=tcn, lpf=leppostfix, spf=syspostfix), \"\", 100,0,500), \"FTAMuon{lpf}_pt\".format(lpf=leppostfix), wgtVar)\n",
    "                    histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"Lepton{lpf}_eta_LeadLep\".format(lpf=leppostfix)] = cat_df[tc].Histo1D((\"{name}__{cat}__Lepton{lpf}_eta_LeadLep_{spf}\".format(name=sampleName, cat=tcn, lpf=leppostfix, spf=syspostfix), \"\", 52,-2.6,2.6), \"FTALepton{lpf}_eta_LeadLep\".format(lpf=leppostfix), wgtVar)\n",
    "                    histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"Lepton{lpf}_eta_SubleadLep\".format(lpf=leppostfix)] = cat_df[tc].Histo1D((\"{name}__{cat}__Lepton{lpf}_eta_SubleadLep_{spf}\".format(name=sampleName, cat=tcn, lpf=leppostfix, spf=syspostfix), \"\", 52,-2.6,2.6), \"FTALepton{lpf}_eta_SubleadLep\".format(lpf=leppostfix), wgtVar)\n",
    "                    #histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"Lepton{lpf}_eta_SubleadLep\".format(lpf=leppostfix)] = cat_df[tc].Histo1D((\"{name}__{cat}__Lepton{lpf}_eta_SubleadLep_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \"\", 52,-2.6,2.6), \"FTALepton{lpf}_eta_SubleadLep\".format(lpf=leppostfix), wgtVar)\n",
    "                    histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"dRll\"] = cat_df[tc].Histo1D((\"{name}__{cat}__dRll_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \"\", 16,0,2*pi), \"FTALepton{lpf}_dRll\".format(lpf=leppostfix), wgtVar)\n",
    "                    histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"dPhill\"] = cat_df[tc].Histo1D((\"{name}__{cat}__dPhill_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \"\", 16,-pi,pi), \"FTALepton{lpf}_dPhill\".format(lpf=leppostfix), wgtVar)\n",
    "                    histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"dEtall\"] = cat_df[tc].Histo1D((\"{name}__{cat}__dEtall_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \"\", 10,0,5), \"FTALepton{lpf}_dEtall\".format(lpf=leppostfix), wgtVar)\n",
    "                    histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"MTofMETandEl\"] = cat_df[tc].Histo1D((\"{name}__{cat}__MTofMETandEl_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \"\", 20, 0, 200), \"MTofMETandEl{bpf}\".format(bpf=branchpostfix), wgtVar)\n",
    "                    histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"MTofMETandMu\"] = cat_df[tc].Histo1D((\"{name}__{cat}__MTofMETandMu_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \"\", 20, 0, 200), \"MTofMETandMu{bpf}\".format(bpf=branchpostfix), wgtVar)\n",
    "                    histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"MTofElandMu\"] = cat_df[tc].Histo1D((\"{name}__{cat}__MTofElandMu_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \"\", 20, 0, 200), \"MTofElandMu{bpf}\".format(bpf=branchpostfix), wgtVar)\n",
    "                    histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"nJet\"] = cat_df[tc].Histo1D((\"{name}__{cat}__nJet_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \"\", 14, 0, 14), \"n{fj}\".format(fj=fillJet), wgtVar)\n",
    "                    if isData is False:\n",
    "                        histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"nJet_genMatched\"] = cat_df[tc].Histo1D((\"{name}__{cat}__nJet_genMatched_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \"\", 14, 0, 14), \"n{fj}_genMatched\".format(fj=fillJet), wgtVar)\n",
    "                        histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"nJet_genMatched_puIdLoose\"] = cat_df[tc].Histo1D((\"{name}__{cat}__nJet_genMatched_puIdLoose_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \"\", 14, 0, 14), \"n{fj}_genMatched_puIdLoose\".format(fj=fillJet), wgtVar)\n",
    "                    histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"nLooseDeepCSV\"] = cat_df[tc].Histo1D((\"{name}__{cat}__nJet_LooseDeepCSV_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \"\", 6, 0, 6), \"nLooseDeepCSVB{bpf}\".format(bpf=branchpostfix), wgtVar)\n",
    "                    histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"nMediumDeepCSV\"] = cat_df[tc].Histo1D((\"{name}__{cat}__nJet_MediumDeepCSV_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \"\", 6, 0, 6), \"nMediumDeepCSVB{bpf}\".format(bpf=branchpostfix), wgtVar)\n",
    "                    histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"nTightDeepCSV\"] = cat_df[tc].Histo1D((\"{name}__{cat}__nJet_TightDeepCSV_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \"\", 6, 0, 6), \"nTightDeepCSVB{bpf}\".format(bpf=branchpostfix), wgtVar)\n",
    "                    histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"nLooseDeepJet\"] = cat_df[tc].Histo1D((\"{name}__{cat}__nJet_LooseDeepJet_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \"\", 6, 0, 6), \"nLooseDeepJetB{bpf}\".format(bpf=branchpostfix), wgtVar)\n",
    "                    histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"nMediumDeepJet\"] = cat_df[tc].Histo1D((\"{name}__{cat}__nJet_MediumDeepJet_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \"\", 6, 0, 6), \"nMediumDeepJetB{bpf}\".format(bpf=branchpostfix), wgtVar)\n",
    "                    histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"nTightDeepJet\"] = cat_df[tc].Histo1D((\"{name}__{cat}__nJet_TightDeepJet_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \"\", 6, 0, 6), \"nTightDeepJetB{bpf}\".format(bpf=branchpostfix), wgtVar)\n",
    "                    histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"nLooseMuon\"] = cat_df[tc].Histo1D((\"{name}__{cat}__nLooseFTAMuon{lpf}_{spf}\".format(name=sampleName, cat=tcn, lpf=leppostfix, spf=syspostfix), \"\", 4, 0, 4), \"nLooseFTAMuon{lpf}\".format(lpf=leppostfix), wgtVar)\n",
    "                    histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"nMediumMuon\"] = cat_df[tc].Histo1D((\"{name}__{cat}__nMediumFTAMuon{lpf}_{spf}\".format(name=sampleName, cat=tcn, lpf=leppostfix, spf=syspostfix), \"\", 4, 0, 4), \"nMediumFTAMuon{lpf}\".format(lpf=leppostfix), wgtVar)\n",
    "                    histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"nTightMuon\"] = cat_df[tc].Histo1D((\"{name}__{cat}__nTightFTAMuon{lpf}_{spf}\".format(name=sampleName, cat=tcn, lpf=leppostfix, spf=syspostfix), \"\", 4, 0, 4), \"nTightFTAMuon{lpf}\".format(lpf=leppostfix), wgtVar)\n",
    "                    histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"nLooseElectron\"] = cat_df[tc].Histo1D((\"{name}__{cat}__nLooseFTAElectron{lpf}_{spf}\".format(name=sampleName, cat=tcn, lpf=leppostfix, spf=syspostfix), \"\", 4, 0, 4), \"nLooseFTAElectron{lpf}\".format(lpf=leppostfix), wgtVar)\n",
    "                    histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"nMediumElectron\"] = cat_df[tc].Histo1D((\"{name}__{cat}__nMediumFTAElectron{lpf}_{spf}\".format(name=sampleName, cat=tcn, lpf=leppostfix, spf=syspostfix), \"\", 4, 0, 4), \"nMediumFTAElectron{lpf}\".format(lpf=leppostfix), wgtVar)\n",
    "                    histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"nTightElectron\"] = cat_df[tc].Histo1D((\"{name}__{cat}__nTightFTAElectron{lpf}_{spf}\".format(name=sampleName, cat=tcn, lpf=leppostfix, spf=syspostfix), \"\", 4, 0, 4), \"nTightFTAElectron{lpf}\".format(lpf=leppostfix), wgtVar)\n",
    "                    histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"nLooseLepton\"] = cat_df[tc].Histo1D((\"{name}__{cat}__nLooseFTALepton{lpf}_{spf}\".format(name=sampleName, cat=tcn, lpf=leppostfix, spf=syspostfix), \"\", 4, 0, 4), \"nLooseFTALepton{lpf}\".format(lpf=leppostfix), wgtVar)\n",
    "                    histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"nMediumLepton\"] = cat_df[tc].Histo1D((\"{name}__{cat}__nMediumFTALepton{lpf}_{spf}\".format(name=sampleName, cat=tcn, lpf=leppostfix, spf=syspostfix), \"\", 4, 0, 4), \"nMediumFTALepton{lpf}\".format(lpf=leppostfix), wgtVar)\n",
    "                    histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"nTightLepton\"] = cat_df[tc].Histo1D((\"{name}__{cat}__nTightFTALepton{lpf}_{spf}\".format(name=sampleName, cat=tcn, lpf=leppostfix, spf=syspostfix), \"\", 4, 0, 4), \"nTightFTALepton{lpf}\".format(lpf=leppostfix), wgtVar)\n",
    "                    histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"MTofMETandEl\"] = cat_df[tc].Histo1D((\"{name}__{cat}__MTofMETandEl_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \"\", 20, 0, 200), \"MTofMETandEl{bpf}\".format(bpf=branchpostfix), wgtVar)\n",
    "                    histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"MTofMETandMu\"] = cat_df[tc].Histo1D((\"{name}__{cat}__MTofMETandMu_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \"\", 20, 0, 200), \"MTofMETandMu{bpf}\".format(bpf=branchpostfix), wgtVar)\n",
    "                    histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"MTofElandMu\"] = cat_df[tc].Histo1D((\"{name}__{cat}__MTofElandMu_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \"\", 20, 0, 200), \"MTofElandMu{bpf}\".format(bpf=branchpostfix), wgtVar)\n",
    "                    histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"Muon_InvMass\"] = cat_df[tc].Histo1D((\"{name}__{cat}__Muon_InvMass_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \"\", 60, 0, 150), \"FTAMuon{lpf}_InvariantMass\".format(lpf=leppostfix), wgtVar)\n",
    "                    histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"Electron_InvMass\"] = cat_df[tc].Histo1D((\"{name}__{cat}__Electron_InvMass_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \"\", 60, 0, 150), \"FTAElectron{lpf}_InvariantMass\".format(lpf=leppostfix), wgtVar)\n",
    "                    histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"Muon_InvMass_v_MET\"] = cat_df[tc].Histo2D((\"{name}__{cat}__Muon_InvMass_v_MET_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \"\", 30, 0, 150, 20, 0, 400), \"FTAMuon{lpf}_InvariantMass\".format(lpf=leppostfix), fillMET_pt, wgtVar)\n",
    "                    histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"Electron_InvMass_v_MET\"] = cat_df[tc].Histo2D((\"{name}__{cat}__Electron_InvMass_v_MET_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \"\", 30, 0, 150, 20, 0, 400), \"FTAElectron{lpf}_InvariantMass\".format(lpf=leppostfix), fillMET_pt, wgtVar)\n",
    "                    #histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"Muon_pfRelIso03_all_vs_MET\"] = cat_df[tc].Histo2D((\"{name}__{cat}__Muon_pfRelIso03_all_vs_MET_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \";pfRelIso03_all;MET\", 30, 0., 0.2, 20,30.,1030.), \"FTAMuon{lpf}_pfRelIso03_all\", \"RVec_MET_pt\", wgtVar)\n",
    "                    #histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"Muon_pfRelIso03_chg_vs_MET\"] = cat_df[tc].Histo2D((\"{name}__{cat}__Muon_pfRelIso03_chg_vs_MET_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \";pfRelIso03_chg;MET\", 30, 0, 0.2, 20,30,1030), \"FTAMuon{lpf}_pfRelIso03_chg\", \"RVec_MET_pt\", wgtVar)\n",
    "                    #histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"Muon_pfRelIso04_all_vs_MET\"] = cat_df[tc].Histo2D((\"{name}__{cat}__Muon_pfRelIso04_all_vs_MET_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \";pfRelIso04_all;MET\", 30, 0, 0.2, 20,30,1030), \"FTAMuon{lpf}_pfRelIso04_all\", \"RVec_MET_pt\", wgtVar)\n",
    "                    #histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"Electron_pfRelIso03_all_vs_MET\"] = cat_df[tc].Histo2D((\"{name}__{cat}__Electron_pfRelIso03_all_vs_MET_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \";pfRelIso03_all;MET\", 30, 0, 0.2, 20,30,1030), \"FTAElectron{lpf}_pfRelIso03_all\", \"RVec_MET_pt\", wgtVar)\n",
    "                    #histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"Electron_pfRelIso03_chg_vs_MET\"] = cat_df[tc].Histo2D((\"{name}__{cat}__Electron_pfRelIso03_chg_vs_MET_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \";pfRelIso03_chg;MET\", 30, 0, 0.2, 20,30,1030), \"FTAElectron{lpf}_pfRelIso03_chg\", \"RVec_MET_pt\", wgtVar)\n",
    "                \n",
    "                    if isData == False:\n",
    "                        pass                \n",
    "            \n",
    "            if histos2D_dict != None:\n",
    "                if \"sysVar_{spf}\".format(spf=syspostfix) not in histos2D_dict:\n",
    "                    histos2D_dict[\"sysVar{spf}\".format(spf=syspostfix)] = {}\n",
    "                for tc in cat_df.keys(): \n",
    "                    if tc not in histos2D_dict[\"sysVar{spf}\".format(spf=syspostfix)]: \n",
    "                        histos2D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc] = {}\n",
    "                for tc, node in cat_df.items():\n",
    "                    tcn = tc#.replace(\"blind_\", \"\")\n",
    "                    #histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"Muon_pfRelIso03_all_vs_MET\"] = cat_df[tc].Histo2D((\"{name}__{cat}__Muon_pfRelIso03_all_vs_MET_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \";pfRelIso03_all;MET\", 30, 0., 0.2, 20,30.,1030.), \"FTAMuon{lpf}_pfRelIso03_all\", \"RVec_MET_pt\", wgtVar)\n",
    "                    #histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"Muon_pfRelIso03_chg_vs_MET\"] = cat_df[tc].Histo2D((\"{name}__{cat}__Muon_pfRelIso03_chg_vs_MET_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \";pfRelIso03_chg;MET\", 30, 0, 0.2, 20,30,1030), \"FTAMuon{lpf}_pfRelIso03_chg\", \"RVec_MET_pt\", wgtVar)\n",
    "                    #histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"Muon_pfRelIso04_all_vs_MET\"] = cat_df[tc].Histo2D((\"{name}__{cat}__Muon_pfRelIso04_all_vs_MET_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \";pfRelIso04_all;MET\", 30, 0, 0.2, 20,30,1030), \"FTAMuon{lpf}_pfRelIso04_all\", \"RVec_MET_pt\", wgtVar)\n",
    "                    #histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"Electron_pfRelIso03_all_vs_MET\"] = cat_df[tc].Histo2D((\"{name}__{cat}__Electron_pfRelIso03_all_vs_MET_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \";pfRelIso03_all;MET\", 30, 0, 0.2, 20,30,1030), \"FTAElectron{lpf}_pfRelIso03_all\", \"RVec_MET_pt\", wgtVar)\n",
    "                    #histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"Electron_pfRelIso03_chg_vs_MET\"] = cat_df[tc].Histo2D((\"{name}__{cat}__Electron_pfRelIso03_chg_vs_MET_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \";pfRelIso03_chg;MET\", 30, 0, 0.2, 20,30,1030), \"FTAElectron{lpf}_pfRelIso03_chg\", \"RVec_MET_pt\", wgtVar)\n",
    "                    #### Older versions\n",
    "                    #histos2D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"Muon_pfRelIso03_all_vs_MET\"] = cat_df[tc].Histo2D((\"{name}__{cat}__Muon_pfRelIso03_all_vs_MET_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \";pfRelIso03_all;MET\", 100, 0., 0.2, 100,30.,1030.), \"FTAMuon{lpf}_pfRelIso03_all\", \"RVec_MET_pt\", wgtVar)\n",
    "                    #histos2D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"Muon_pfRelIso03_chg_vs_MET\"] = cat_df[tc].Histo2D((\"{name}__{cat}__Muon_pfRelIso03_chg_vs_MET_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \";pfRelIso03_chg;MET\", 100, 0, 0.2, 100,30,1030), \"FTAMuon{lpf}_pfRelIso03_chg\", \"RVec_MET_pt\", wgtVar)\n",
    "                    #histos2D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"Muon_pfRelIso04_all_vs_MET\"] = cat_df[tc].Histo2D((\"{name}__{cat}__Muon_pfRelIso04_all_vs_MET_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \";pfRelIso04_all;MET\", 100, 0, 0.2, 100,30,1030), \"FTAMuon{lpf}_pfRelIso04_all\", \"RVec_MET_pt\", wgtVar)\n",
    "                    #histos2D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"Electron_pfRelIso03_all_vs_MET\"] = cat_df[tc].Histo2D((\"{name}__{cat}__Electron_pfRelIso03_all_vs_MET_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \";pfRelIso03_all;MET\", 100, 0, 0.2, 100,30,1030), \"FTAElectron{lpf}_pfRelIso03_all\", \"RVec_MET_pt\", wgtVar)\n",
    "                    #histos2D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"Electron_pfRelIso03_chg_vs_MET\"] = cat_df[tc].Histo2D((\"{name}__{cat}__Electron_pfRelIso03_chg_vs_MET_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \";pfRelIso03_chg;MET\", 100, 0, 0.2, 100,30,1030), \"FTAElectron{lpf}_pfRelIso03_chg\", \"RVec_MET_pt\", wgtVar)\n",
    "                    if isData == False:\n",
    "                        #histos2D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"test1\"] = cat_df[tc].Histo2D((\"{name}__{cat}__test1_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \";nTrueInt;npvsGood\", 150, 0, 150, 150, 0, 150), \"FTAMuon{lpf}_pfRelIso03_all\", \"PV_npvsGood\", wgtVar)\n",
    "                        #histos2D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"test2\"] = cat_df[tc].Histo2D((\"{name}__{cat}__test2_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \";nTrueInt;npvsGood\", 150, 0, 150, 150, 0, 150), \"FTAMuon{lpf}_pfRelIso03_all\", \"METFixEE2017_pt\", wgtVar)\n",
    "                        #histos2D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"npvsGood_vs_nTrueInttest\"] = cat_df[tc].Histo2D((\"{name}__{cat}__npvsGood_vs_nTrueInttest_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \";nTrueInt;npvsGood\", 150, 0, 150, 150, 0, 150), \"FTAElectron{lpf}_pfRelIso03_all\", \"MET_pt_flat\", wgtVar)\n",
    "                        histos2D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"npvsGood_vs_nTrueInt\"] = cat_df[tc].Histo2D((\"{name}__{cat}__npvsGood_vs_nTrueInt_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \";nTrueInt;npvsGood\", 150, 0, 150, 150, 0, 150), \"Pileup_nTrueInt\", \"PV_npvsGood\", wgtVar)\n",
    "                        histos2D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"npvsGood_vs_nPU\"] = cat_df[tc].Histo2D((\"{name}__{cat}__npvsGood_vs_nPU_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \";nPU;npvsGood\", 150, 0, 150, 150, 0, 150), \"Pileup_nPU\", \"PV_npvsGood\", wgtVar)\n",
    "                        histos2D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"npvs_vs_nTrueInt\"] = cat_df[tc].Histo2D((\"{name}__{cat}__npvs_vs_nTrueInt_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \";nTrueInt;npvs\", 150, 0, 150, 150, 0, 150), \"Pileup_nTrueInt\", \"PV_npvs\", wgtVar)\n",
    "                        histos2D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"npvs_vs_nPU\"] = cat_df[tc].Histo2D((\"{name}__{cat}__npvs_vs_nPU_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \";nPU;npvs\", 150, 0, 150, 150, 0, 150), \"Pileup_nPU\", \"PV_npvs\", wgtVar)\n",
    "      \n",
    "    return cat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fillHistosOld(input_df, sampleName=None, wgtVar=\"wgt_SUMW\", isData = True, histos1D_dict=None, histos2D_dict=None, histosNS_dict=None, \n",
    "               doMuons=False, doElectrons=False, doLeptons=False, doJets=False, doWeights=False, doEventVars=False, \n",
    "               makeMountains=False, debugInfo=True, nJetsToHisto=10, useDeepCSV=False,\n",
    "               sysVariations={\"$NOMINAL\": {\"jet_mask\": \"jet_mask\", \n",
    "                                           \"wgt_prebTag\": \"wgt_SUMW_PU_LSF_L1PF\",\n",
    "                                           \"wgt_final\": \"_deepcsv_shape\",\n",
    "                                           \"jet_pt_var\": \"Jet_pt\",\n",
    "                                           \"met_pt_var\": \"METFixEE2017_pt\",\n",
    "                                           \"met_phi_var\": \"METFixEE2017_phi\",\n",
    "                                           \"btagSF\": \"Jet_btagSF_deepcsv_shape\",\n",
    "                                           \"weightVariation\": False}\n",
    "                                     },):\n",
    "    \"\"\"Method to fill histograms given an input RDataFrame, input sample/dataset name, input histogram dictionaries.\n",
    "    Has several options of which histograms to fill, such as Leptons, Jets, Weights, EventVars, etc.\n",
    "    Types of histograms (1D, 2D, those which will not be stacked(NS - histosNS)) are filled by passing non-None\n",
    "    value to that histosXX_dict variable. Internally stored with structure separating the categories of histos,\n",
    "    with 'Muons,' 'Electrons,' 'Leptons,' 'Jets,' 'EventVars,' 'Weights' subcategories.\"\"\"\n",
    "    if doMuons == False and doElectrons == False and doLeptons == False\\\n",
    "                and doJets == False and doWeights == False and doEventVars == False\\\n",
    "                and makeMountains == False:\n",
    "        raise RuntimeError(\"Must select something to plot:\"\\\n",
    "                               \"Set do{Muons,Electrons,Leptons,Jets,Weights,EventVars,etc} = True in init method\")\n",
    "    \n",
    "    pi = ROOT.TMath.Pi()\n",
    "    sysVar, sysDict = sysVariations.items()\n",
    "    #Everything here needs to be indented and looped over, excepting FILTERS!\n",
    "    wgtVar = sysDict.get(wgt_final, \"wgt_SUMW_PU_LSF_L1PF\")\n",
    "    isWeightVariation = sysDict.get(\"weightVariation\", False)\n",
    "    #Name histograms with their actual systematic variation postfix\n",
    "    syspostfix = sysVar.replace(\"$NOMINAL\", \"_nom\")\n",
    "    #name branches for filling with the nominal postfix if weight variations, and systematic postfix if scale variation (jes_up, etc.)\n",
    "    branchpostfix = None\n",
    "    if isWeightVariation:\n",
    "        branchpostfix = \"_nom\"\n",
    "    else:\n",
    "        branchpostfix = sysVar.replace(\"$NOMINAL\", \"_nom\")\n",
    "    fillJet_pt = \"FTAJet{bpf}_pt\".format(bpf=branchpostfix)\n",
    "    fillJet_phi = \"FTAJet{bpf}_phi\".format(bpf=branchpostfix)\n",
    "    fillJet_eta = \"FTAJet{bpf}_eta\".format(bpf=branchpostfix)\n",
    "    fillJet_mass = \"FTAJet{bpf}_mass\".format(bpf=branchpostfix)\n",
    "    fillMET_pt = \"FTAMET{bpf}_pt\".format(bpf=branchpostfix)\n",
    "    fillMET_phi = \"FTAMET{bpf}_phi\".format(bpf=branchpostfix)\n",
    "    \n",
    "    if doWeights == True:\n",
    "        if histosNS_dict != None:\n",
    "            if \"EventVars\" not in histosNS_dict:\n",
    "                histosNS_dict[\"EventVars\"] = {}\n",
    "            histosNS[name][lvl][\"EventVars\"][\"wgt_NUMW\"] = input_df.Histo1D(\"wgt_NUMW\")\n",
    "            histosNS[name][lvl][\"EventVars\"][wgtVar] = input_df.Histo1D(wgtVar)\n",
    "        if histos1D_dict != None:\n",
    "            if \"EventVars\" not in histos1D_dict:\n",
    "                histos1D_dict[\"EventVars\"] = {}\n",
    "            histos1D_dict[\"EventVars\"][\"wgt_diff\"] = input_df.Histo1D((\"wgt_diff\", \"(wgt_NUMW - wgt_SUMW)/wgt_SUMW\", 2000, -1, 1), \"wgt_diff\", \"1\")\n",
    "            histos1D_dict[\"EventVars\"][\"wgt_PU\"] = input_df.Histo1D((\"wgt_PU\", \"\", 2000, 0, 5), \"puWeight\", \"wgt_SUMW\")\n",
    "            histos1D_dict[\"EventVars\"][\"wgt_LSF\"] = input_df.Histo1D((\"wgt_LSF\", \"\", 2000, 0, 5), \"wgt_LSF\", \"wgt_SUMW\")\n",
    "            histos1D_dict[\"EventVars\"][\"wgt_L1PF\"] = input_df.Histo1D((\"wgt_L1PF\", \"\", 2000, 0, 5), \"L1PreFiringWeight_Nom\", \"wgt_SUMW\")\n",
    "            histos1D_dict[\"EventVars\"][\"wgt_PU_LSF_L1PF\"] = input_df.Histo1D((\"wgt_PU_LSF_L1PF\", \"\", 2000, 0, 5), \"wgt_PU_LSF_L1PF\", \"wgt_SUMW\")\n",
    "    if doMuons == True:\n",
    "        if histos1D_dict != None:\n",
    "            if \"Muons\" not in histos1D_dict: \n",
    "                histos1D_dict[\"Muons\"] = {}\n",
    "            histos1D_dict[\"Muons\"][\"idx\"] = input_df.Histo1D((\"idx{}\".format(postfix), \"\", 5, 0, 5), \"Muon_idx\", wgtVar)\n",
    "            histos1D_dict[\"Muons\"][\"Gidx\"] = input_df.Histo1D((\"Gidx{}\".format(postfix), \"\", 5, 0, 5), \"FTAMuon{lpf}_idx\", wgtVar)\n",
    "            histos1D_dict[\"Muons\"][\"nMu\"] = input_df.Histo1D((\"nMuon{}\".format(postfix), \"\", 5, 0, 5), \"nFTAMuon{lpf}\", wgtVar)\n",
    "            histos1D_dict[\"Muons\"][\"nLooseMu\"] = input_df.Histo1D((\"nLooseMuon{}\".format(postfix), \"\", 5, 0, 5), \"nLooseFTAMuon{lpf}\", wgtVar)\n",
    "            histos1D_dict[\"Muons\"][\"nMediumMu\"] = input_df.Histo1D((\"nMediumMuon{}\".format(postfix), \"\", 5, 0, 5), \"nMediumFTAMuon{lpf}\", wgtVar)\n",
    "            histos1D_dict[\"Muons\"][\"pt\"] = input_df.Histo1D((\"Muon_pt{}\".format(postfix), \"\", 100, 0, 500), \"FTAMuon{lpf}_pt\", wgtVar)\n",
    "            histos1D_dict[\"Muons\"][\"eta\"] = input_df.Histo1D((\"Muon_eta{}\".format(postfix), \"\", 104, -2.6, 2.6), \"FTAMuon{lpf}_eta\", wgtVar)\n",
    "            histos1D_dict[\"Muons\"][\"phi\"] = input_df.Histo1D((\"Muon_phi{}\".format(postfix), \"\", 64, -pi, pi), \"FTAMuon{lpf}_phi\", wgtVar)\n",
    "            #histos1D_dict[\"Muons\"][\"mass\"] = input_df.Histo1D((\"Muon_mass{}\".format(postfix), \"\", 50, 0, 1), \"FTAMuon{lpf}_mass\", wgtVar)\n",
    "            histos1D_dict[\"Muons\"][\"iso\"] = input_df.Histo1D((\"Muon_iso{}\".format(postfix), \"\", 8, 0, 8), \"FTAMuon{lpf}_pfIsoId\", wgtVar)\n",
    "            histos1D_dict[\"Muons\"][\"dz\"] = input_df.Histo1D((\"Muon_dz{}\".format(postfix), \"\", 100, -0.01, 0.01), \"FTAMuon{lpf}_dz\", wgtVar)\n",
    "            histos1D_dict[\"Muons\"][\"dxy\"] = input_df.Histo1D((\"Muon_dxy{}\".format(postfix), \"\", 100, -0.1, 0.1), \"FTAMuon{lpf}_dxy\", wgtVar)\n",
    "            #histos1D_dict[\"Muons\"][\"d0\"] = input_df.Histo1D((\"Muon_d0{}\".format(postfix), \"\", 100, -0.01, 0.01), \"FTAMuon{lpf}_d0\", wgtVar)\n",
    "            histos1D_dict[\"Muons\"][\"ip3d\"] = input_df.Histo1D((\"Muon_ip3d{}\".format(postfix), \"\", 100, 0, 0.01), \"FTAMuon{lpf}_ip3d\", wgtVar)\n",
    "            histos1D_dict[\"Muons\"][\"pfRelIso03_all\"] = input_df.Histo1D((\"Muon_pfRelIso03_all{}\".format(postfix), \"\", 100, 0, 0.2), \"FTAMuon{lpf}_pfRelIso03_all\", wgtVar)\n",
    "            histos1D_dict[\"Muons\"][\"pfRelIso03_chg\"] = input_df.Histo1D((\"Muon_pfRelIso03_chg{}\".format(postfix), \"\", 100, 0, 0.2), \"FTAMuon{lpf}_pfRelIso03_chg\", wgtVar)\n",
    "            histos1D_dict[\"Muons\"][\"pfRelIso04_all\"] = input_df.Histo1D((\"Muon_pfRelIso04_all{}\".format(postfix), \"\", 100, 0, 0.2), \"FTAMuon{lpf}_pfRelIso04_all\", wgtVar)\n",
    "        if histos2D_dict != None:\n",
    "            if \"Muons\" not in histos2D_dict:\n",
    "                histos2D_dict[\"Muons\"] = {}\n",
    "            histos2D_dict[\"Muons\"][\"eta_phi\"] = input_df.Histo2D((\"Muon_eta_phi{}\".format(postfix), \"\",\n",
    "                                                                  104, -2.6, 2.6,\n",
    "                                                                  64, -pi, pi),\n",
    "                                                                 \"FTAMuon{lpf}_eta\", \"FTAMuon{lpf}_phi\", wgtVar)\n",
    "            histos2D_dict[\"Muons\"][\"dz_ip3d\"] = input_df.Histo2D((\"Muon_dz_ip3d{}\".format(postfix), \"\",\n",
    "                                                                  100, -0.01, 0.01,\n",
    "                                                                  100, 0, 0.01),\n",
    "                                                                 \"FTAMuon{lpf}_dz\", \"FTAMuon{lpf}_ip3d\", wgtVar)\n",
    "    if doElectrons == True:\n",
    "        if histos1D_dict != None:\n",
    "            if \"Electrons\" not in histos1D_dict: \n",
    "                histos1D_dict[\"Electrons\"] = {}\n",
    "            histos1D_dict[\"Electrons\"][\"nEl\"] = input_df.Histo1D((\"nElectron{}\".format(postfix), \"\", 5, 0, 5), \"nFTAElectron{lpf}\", wgtVar)\n",
    "            histos1D_dict[\"Electrons\"][\"nLooseEl\"] = input_df.Histo1D((\"nLooseElectron{}\".format(postfix), \"\", 5, 0, 5), \"nLooseFTAElectron{lpf}\", wgtVar)\n",
    "            histos1D_dict[\"Electrons\"][\"nMediumEl\"] = input_df.Histo1D((\"nMediumElectron{}\".format(postfix), \"\", 5, 0, 5), \"nMediumFTAElectron{lpf}\", wgtVar)\n",
    "            histos1D_dict[\"Electrons\"][\"pt\"] = input_df.Histo1D((\"Electron_pt{}\".format(postfix), \"\", 100, 0, 500), \"FTAElectron{lpf}_pt\", wgtVar)\n",
    "            histos1D_dict[\"Electrons\"][\"eta\"] = input_df.Histo1D((\"Electron_eta{}\".format(postfix), \"\", 104, -2.6, 2.6), \"FTAElectron{lpf}_eta\", wgtVar)\n",
    "            histos1D_dict[\"Electrons\"][\"phi\"] = input_df.Histo1D((\"Electron_phi{}\".format(postfix), \"\", 64, -pi, pi), \"FTAElectron{lpf}_phi\", wgtVar)\n",
    "            #histos1D_dict[\"Electrons\"][\"mass\"] = input_df.Histo1D((\"Electron_mass{}\".format(postfix), \"\", 50, 0, 1), \"FTAElectron{lpf}_mass\", wgtVar)\n",
    "            histos1D_dict[\"Electrons\"][\"dz\"] = input_df.Histo1D((\"Electron_dz{}\".format(postfix), \"\", 100, -0.01, 0.01), \"FTAElectron{lpf}_dz\", wgtVar)\n",
    "            #histos1D_dict[\"Electrons\"][\"d0\"] = input_df.Histo1D((\"Electron_d0{}\".format(postfix), \"\", 100, 0, 0.01), \"FTAElectron{lpf}_d0\", wgtVar)\n",
    "            histos1D_dict[\"Electrons\"][\"ip3d\"] = input_df.Histo1D((\"Electron_ip3d{}\".format(postfix), \"\", 100, 0, 0.01), \"FTAElectron{lpf}_ip3d\", wgtVar)\n",
    "            histos1D_dict[\"Electrons\"][\"pfRelIso03_all\"] = input_df.Histo1D((\"Electron_pfRelIso03_all{}\".format(postfix), \"\", 100, 0, 0.2), \"FTAElectron{lpf}_pfRelIso03_all\", wgtVar)\n",
    "            histos1D_dict[\"Electrons\"][\"pfRelIso03_chg\"] = input_df.Histo1D((\"Electron_pfRelIso03_chg{}\".format(postfix), \"\", 100, 0, 0.2), \"FTAElectron{lpf}_pfRelIso03_chg\", wgtVar)\n",
    "            histos1D_dict[\"Electrons\"][\"cutBased\"] = input_df.Histo1D((\"Electron_cutBased{}\".format(postfix), \"\", 5, 0, 5), \"FTAElectron{lpf}_cutBased\", wgtVar)\n",
    "        if histos2D_dict != None:\n",
    "            if \"Electrons\" not in histos2D_dict: \n",
    "                histos2D_dict[\"Electrons\"] = {}\n",
    "            histos2D_dict[\"Electrons\"][\"eta_phi\"] = input_df.Histo2D((\"Electron_eta_phi{}\".format(postfix), \"\",\n",
    "                                                                      104, -2.6, 2.6,\n",
    "                                                                      64, -pi, pi),\n",
    "                                                                     \"FTAElectron{lpf}_eta\", \"FTAElectron{lpf}_phi\", wgtVar)\n",
    "            histos2D_dict[\"Electrons\"][\"dz_ip3d\"] = input_df.Histo2D((\"Electron_dz_ip3d{}\".format(postfix), \"\",\n",
    "                                                                      100, -0.01, 0.01,\n",
    "                                                                      100, 0, 0.01),\n",
    "                                                                     \"FTAElectron{lpf}_dz\", \"FTAElectron{lpf}_ip3d\", wgtVar)\n",
    "    if doLeptons == True:\n",
    "        if histos1D_dict != None:\n",
    "            if \"Leptons\" not in histos1D_dict: \n",
    "                histos1D_dict[\"Leptons\"] = {}\n",
    "            histos1D_dict[\"Leptons\"][\"pt_LeadLep\"] = input_df\\\n",
    "                    .Histo1D((\"FTALepton{lpf}_pt_LeadLep{}\".format(postfix), \"\", 100, 0, 500),\"FTALepton{lpf}_pt_LeadLep\", wgtVar)\n",
    "            histos1D_dict[\"Leptons\"][\"pt_SubleadLep\"] = input_df\\\n",
    "                    .Histo1D((\"FTALepton{lpf}_pt_SubleadLep{}\".format(postfix), \"\", 100, 0, 500),\"FTALepton{lpf}_pt_SubleadLep\", wgtVar)\n",
    "            histos1D_dict[\"Leptons\"][\"eta\"] = input_df\\\n",
    "                    .Histo1D((\"FTALepton{lpf}_eta{}\".format(postfix), \"\", 104, -2.6, 2.6),\"FTALepton{lpf}_eta\", wgtVar)\n",
    "            histos1D_dict[\"Leptons\"][\"phi\"] = input_df\\\n",
    "                    .Histo1D((\"FTALepton{lpf}_phi{}\".format(postfix), \"\", 64, -pi, pi),\"FTALepton{lpf}_phi\", wgtVar)\n",
    "            histos1D_dict[\"Leptons\"][\"nLepton\"] = input_df\\\n",
    "                    .Histo1D((\"nLepton{}\".format(postfix), \"\", 5, 0, 5), \"nFTALepton{lpf}\", wgtVar)\n",
    "            histos1D_dict[\"Leptons\"][\"pdgId\"] = input_df\\\n",
    "                    .Histo1D((\"Lepton_pdgId{}\".format(postfix), \"\", 32, -16, 16), \"FTALepton{lpf}_pdgId\", wgtVar)\n",
    "            histos1D_dict[\"Leptons\"][\"jetIdx\"] = input_df\\\n",
    "                    .Histo1D((\"Lepton_jetIdx{}\".format(postfix), \"\", 20, 0, 20), \"FTALepton{lpf}_jetIdx\", wgtVar)\n",
    "            #histos1D_dict[\"Leptons\"][\"LepSF\"] = input_df\\\n",
    "            #        .Histo1D((\"Lepton_SF_({})\".format(\"wgt_SUMW_PU:HARDCODED\"), \"\", 100, 0.93, 1.03), \"FTALepton{lpf}_SF_nom\", \"wgt_SUMW_PU\")\n",
    "            #histos1D_dict[\"Leptons\"][\"LSF\"] = input_df\\\n",
    "            #        .Histo1D((\"LSF_({})\".format(\"wgt_SUMW_PU:HARDCODED\"), \"\", 200, 0.80, 1.1), \"wgt_LSF\", \"wgt_SUMW_PU\")\n",
    "            #histos1D_dict[\"Leptons\"][\"SPL_SP\"] = input_df\\\n",
    "            #        .Histo1D((\"SPL_SP_({})\".format(\"wgt_SUMW_PU:HARDCODED\"), \"\", 200, 0.80, 1.1), \"SPL_SP\", \"wgt_SUMW_PU\")\n",
    "            #histos1D_dict[\"Leptons\"][\"LepSF\"] = input_df.Histo1D(\"FTALepton{lpf}_SF_nom\")#, \"wgt_SUMW_PU\")\n",
    "            #histos1D_dict[\"Leptons\"][\"LSF\"] = input_df.Histo1D(\"wgt_LSF\")#, \"wgt_SUMW_PU\")\n",
    "            #histos1D_dict[\"Leptons\"][\"SPL_SP\"] = input_df.Histo1D(\"SPL_SP\")#, \"wgt_SUMW_PU\")\n",
    "            #histos1D_dict[\"Leptons\"][\"SUMW_PU\"] = input_df.Histo1D(\"wgt_SUMW_PU\")#, \"wgt_SUMW_PU\")\n",
    "            #histos1D_dict[\"Leptons\"][\"SUMW_PU_LSF\"] = input_df.Histo1D(\"wgt_SUMW_PU_LSF\")#, \"wgt_SUMW_PU\")\n",
    "            #histos1D_dict[\"Leptons\"][\"PU\"] = input_df.Histo1D(\"puWeight\")#, \"wgt_SUMW_PU\")\n",
    "    if doJets == True:\n",
    "        if histos1D_dict != None:\n",
    "            if \"Jets\" not in histos1D_dict:\n",
    "                histos1D_dict[\"Jets\"] = {}\n",
    "            histos1D_dict[\"Jets\"][\"pt\"] = input_df.Histo1D((\"Jet_pt{}\".format(postfix), \"\", 100, 0, 500), fillJet_pt, wgtVar)\n",
    "            for x in xrange(nJetsToHisto):\n",
    "                histos1D_dict[\"Jets\"][\"pt_jet{}\".format(x+1)] = input_df.Histo1D((\"Jet_pt_jet{}({})\".format(x+1, wgtVar), \"\", 100, 0, 500), \"Jet{bpf}_pt_jet{}\".format(x+1), wgtVar)\n",
    "                histos1D_dict[\"Jets\"][\"eta_jet{}\".format(x+1)] = input_df.Histo1D((\"Jet_eta_jet{}({})\".format(x+1, wgtVar), \"\", 104, -2.6, 2.6), \"Jet{bpf}_eta_jet{}\".format(x+1), wgtVar)\n",
    "                histos1D_dict[\"Jets\"][\"phi_jet{}\".format(x+1)] = input_df.Histo1D((\"Jet_phi_jet{}({})\".format(x+1, wgtVar), \"\", 64, -pi, pi), \"Jet{bpf}_phi_jet{}\".format(x+1), wgtVar)\n",
    "            histos1D_dict[\"Jets\"][\"eta\"] = input_df.Histo1D((\"Jet_eta{}\".format(postfix), \"\", 104, -2.6, 2.6), fillJet_eta, wgtVar)\n",
    "            histos1D_dict[\"Jets\"][\"phi\"] = input_df.Histo1D((\"Jet_phi{}\".format(postfix), \"\", 64, -pi, pi), fillJet_phi, wgtVar)\n",
    "            histos1D_dict[\"Jets\"][\"mass\"] = input_df.Histo1D((\"Jet_mass{}\".format(postfix), \"\", 100, 0, 500), fillJet_mass, wgtVar)\n",
    "            histos1D_dict[\"Jets\"][\"jetId\"] = input_df.Histo1D((\"Jet_jetId{}\".format(postfix), \"\", 8, 0, 8), \"Jet{bpf}_jetId\", wgtVar) #FIXME: not based on variation... okay? maybe not with masks...\n",
    "            #histos1D_dict[\"Jets\"][\"btagDeepB_LeadtagJet\"] = input_df.Histo1D((\"Jet_btagDeepB_LeadtagJet{}\".format(postfix), \"\", 101, -0.01, 1), \"Jet{bpf}_btagDeepB_LeadtagJet\", wgtVar)\n",
    "            #histos1D_dict[\"Jets\"][\"btagDeepB_SubleadtagJet\"] = input_df.Histo1D((\"Jet_btagDeepB_SubleadtagJet{}\".format(postfix), \"\", 101, -0.01, 1), \"Jet{bpf}_btagDeepB_SubleadtagJet\", wgtVar)\n",
    "            #histos1D_dict[\"Jets\"][\"btagDeepJet_LeadtagJet\"] = input_df.Histo1D((\"Jet_btagDeepJetB_LeadtagJet{}\".format(postfix), \"\", 101, -0.01, 1), \"Jet{bpf}_btagDeepFlavB_sorted_LeadtagJet\", wgtVar)\n",
    "            #histos1D_dict[\"Jets\"][\"btagDeepJet_SubleadtagJet\"] = input_df.Histo1D((\"Jet_btagDeepJetB_SubleadtagJet{}\".format(postfix), \"\", 101, -0.01, 1), \"Jet{bpf}_btagDeepFlavB_sorted_SubleadtagJet\", wgtVar)\n",
    "            #histos1D_dict[\"Jets\"][\"nMediumCSVv2\"] = input_df.Histo1D((\"nJet_MediumCSVv2{}\".format(postfix), \"\", 10, 0, 10), \"nJet{bpf}_MediumCSVv2\", wgtVar)\n",
    "            histos1D_dict[\"Jets\"][\"nMediumDeepCSV\"] = input_df.Histo1D((\"nJet_MediumDeepCSV{}\".format(postfix), \"\", 10, 0, 10), \"nJet{bpf}_MediumDeepCSV\", wgtVar)\n",
    "            histos1D_dict[\"Jets\"][\"nMediumDeepJet\"] = input_df.Histo1D((\"nJet_MediumDeepJet{}\".format(postfix), \"\", 10, 0, 10), \"nJet{bpf}_MediumDeepJet\", wgtVar)\n",
    "            histos1D_dict[\"Jets\"][\"nJet\"] = input_df.Histo1D((\"nJet{}\".format(postfix), \"\", 15, 0, 15), \"nJet{bpf}\", wgtVar)\n",
    "            histos1D_dict[\"Jets\"][\"dR_Jet_Mu_leading\"] = input_df.Histo1D((\"dR_Jet_Mu_leading{}\".format(postfix), \"dR(Jet, #mu_{leading}); dR; Events)\", 40, 0, 0.8), \"dR_Jet_Mu_leading\", wgtVar)\n",
    "            histos1D_dict[\"Jets\"][\"dR_Jet_Mu_sublead\"] = input_df.Histo1D((\"dR_Jet_Mu_sublead{}\".format(postfix), \"dR(Jet, #mu_{subleading}); dR; Events)\", 40, 0, 0.8), \"dR_Jet_Mu_sublead\", wgtVar)\n",
    "            histos1D_dict[\"Jets\"][\"dR_Jet_El_leading\"] = input_df.Histo1D((\"dR_Jet_El_leading{}\".format(postfix), \"dR(Jet, #e_{leading}); dR; Events)\", 40, 0, 0.8), \"dR_Jet_El_leading\", wgtVar)\n",
    "            histos1D_dict[\"Jets\"][\"dR_Jet_El_sublead\"] = input_df.Histo1D((\"dR_Jet_El_sublead{}\".format(postfix), \"dR(Jet, #e_{subleading}); dR; Events)\", 40, 0, 0.8), \"dR_Jet_El_sublead\", wgtVar)\n",
    "            \n",
    "            if debugInfo == True:\n",
    "                #histos1D_dict[\"Jets\"][\"DiffMaskVsALT\"] = input_df.Histo1D((\"DiffMaskVsALT\", \"\", 10, -10, 10), \"DiffMaskVsALT\", wgtVar)\n",
    "                #histos1D_dict[\"Jets\"][\"DiffnJet\"] = input_df.Histo1D((\"DiffnJet\", \"\", 10, -10, 10), \"DiffnJet\", wgtVar)\n",
    "                histos1D_dict[\"Jets\"][\"DeepJetSorted\"] = input_df.Histo1D(\"DeepJetSorted\", wgtVar)\n",
    "                histos1D_dict[\"Jets\"][\"DeepJetLeadtagMinusSubleadtag\"] = input_df.Histo1D((\"DeepJetLeadtagMinusSubleadtag\", \"DeepJet(Leadtag - Subleadtag);;Events\", 100, -1, 1), \"DeepJet0Minus1\", wgtVar)\n",
    "                histos1D_dict[\"Jets\"][\"MediumDeepJetSorted\"] = input_df.Histo1D(\"MediumDeepJetSorted\", wgtVar)\n",
    "                #histos1D_dict[\"Jets\"][\"MediumDeepJet0Minus1\"] = input_df.Histo1D((\"MediumDeepJet0Minus1\", \"\", 100, -1, 1), \"MediumDeepJet0Minus1\", wgtVar)\n",
    "                #histos1D_dict[\"Jets\"][\"btagDeepJet_jet0Med\"] = input_df.Histo1D((\"Jet_btagDeepJetB_jet0Med{}\".format(postfix), \"\", 102, -0.02, 1), \"Jet{bpf}_btagDeepFlavB_jet0Med\", wgtVar)\n",
    "                #histos1D_dict[\"Jets\"][\"btagDeepJet_jet1Med\"] = input_df.Histo1D((\"Jet_btagDeepJetB_jet1Med{}\".format(postfix), \"\", 102, -0.02, 1), \"Jet{bpf}_btagDeepFlavB_jet1Med\", wgtVar)\n",
    "                #histos1D_dict[\"Jets\"][\"nJetNUMW\"] = input_df.Histo1D((\"nJet_NUMW\", \"\", 15, 0, 15), \"nJet{bpf}\", \"wgt_NUMW_V2\")\n",
    "                #histos1D_dict[\"Jets\"][\"nJetSUMW_PU\"] = input_df.Histo1D((\"nJet_SUMW_PU\", \"\", 15, 0, 15), \"nJet{bpf}\", \"wgt_SUMW_PU\")\n",
    "                #histos1D_dict[\"Jets\"][\"nJetSUMW_LSF\"] = input_df.Histo1D((\"nJet_SUMW_LSF\", \"\", 15, 0, 15), \"nJet{bpf}\", \"wgt_SUMW_LSF\")\n",
    "                #histos1D_dict[\"Jets\"][\"ptALT\"] = input_df.Histo1D((\"Jet_ptALT{}\".format(postfix), \"\", 100, 0, 500), \"Jet{bpf}_ptALT\", wgtVar)\n",
    "                #histos1D_dict[\"Jets\"][\"etaALT\"] = input_df.Histo1D((\"Jet_etaALT{}\".format(postfix), \"\", 104, -2.6, 2.6), \"Jet{bpf}_etaALT\", wgtVar)\n",
    "                #histos1D_dict[\"Jets\"][\"phiALT\"] = input_df.Histo1D((\"Jet_phiALT{}\".format(postfix), \"\", 64, -pi, pi), \"Jet{bpf}_phiALT\", wgtVar)\n",
    "                #histos1D_dict[\"Jets\"][\"massALT\"] = input_df.Histo1D((\"Jet_massALT{}\".format(postfix), \"\", 100, 0, 500), \"Jet{bpf}_massALT\", wgtVar)\n",
    "                #histos1D_dict[\"Jets\"][\"jetIdALT\"] = input_df.Histo1D((\"Jet_jetIdALT{}\".format(postfix), \"\", 8, 0, 8), \"Jet{bpf}_jetIdALT\", wgtVar)\n",
    "        \n",
    "        if histos2D_dict != None:\n",
    "            if \"Jets\" not in histos2D_dict:\n",
    "                histos2D_dict[\"Jets\"] = {}\n",
    "            histos2D_dict[\"Jets\"][\"eta_phi\"] = input_df.Histo2D((\"Jet_eta_phi{}\".format(postfix), \"\",\n",
    "                                                                 104, -2.6, 2.6,\n",
    "                                                                 64, -pi, pi),\n",
    "                                                                fillJet_eta, fillJet_phi, wgtVar)\n",
    "    if doEventVars == True:\n",
    "        if histos1D_dict != None:\n",
    "            if \"EventVars\" not in histos1D_dict:\n",
    "                histos1D_dict[\"EventVars\"] = {}\n",
    "            #histos1D_dict[\"EventVars\"][\"JML_baseline\"] = input_df.Histo1D((\"JML_baseline{}\".format(postfix), \"\", 2,0,2), \"JML_baseline_pass\", wgtVar)\n",
    "            #histos1D_dict[\"EventVars\"][\"JML_selection\"] = input_df.Histo1D((\"JML_selection{}\".format(postfix), \"\", 2,0,2), \"JML_selection_pass\", wgtVar)\n",
    "            #histos1D_dict[\"EventVars\"][\"HT_baseline\"] = input_df.Histo1D((\"ESV_JetMETLogic_HT_baseline{}\".format(postfix), \"\", 100,400,1400), \"ESV_JetMETLogic_HT_baseline\", wgtVar)\n",
    "            #histos1D_dict[\"EventVars\"][\"H_baseline\"] = input_df.Histo1D((\"ESV_JetMETLogic_H_baseline{}\".format(postfix), \"\", 100,400,1400), \"ESV_JetMETLogic_H_baseline\", wgtVar)\n",
    "            #histos1D_dict[\"EventVars\"][\"HT2M_baseline\"] = input_df.Histo1D((\"ESV_JetMETLogic_HT2M_baseline{}\".format(postfix), \"\", 100,400,900), \"ESV_JetMETLogic_HT2M_baseline\", wgtVar)\n",
    "            #histos1D_dict[\"EventVars\"][\"H2M_baseline\"] = input_df.Histo1D((\"ESV_JetMETLogic_H2M_baseline{}\".format(postfix), \"\", 100,400,900), \"ESV_JetMETLogic_H2M_baseline\", wgtVar)\n",
    "            #histos1D_dict[\"EventVars\"][\"HTb_baseline\"] = input_df.Histo1D((\"ESV_JetMETLogic_HTb_baseline{}\".format(postfix), \"\", 100,400,900), \"ESV_JetMETLogic_HTb_baseline\", wgtVar)\n",
    "            #histos1D_dict[\"EventVars\"][\"HTH_baseline\"] = input_df.Histo1D((\"ESV_JetMETLogic_HTH_baseline{}\".format(postfix), \"\", 100,0,1), \"ESV_JetMETLogic_HTH_baseline\", wgtVar)\n",
    "            #histos1D_dict[\"EventVars\"][\"HTRat_baseline\"] = input_df.Histo1D((\"ESV_JetMETLogic_HTRat_baseline{}\".format(postfix), \"\", 100,0,1), \"ESV_JetMETLogic_HTRat_baseline\", wgtVar)\n",
    "            #histos1D_dict[\"EventVars\"][\"dRbb_baseline\"] = input_df.Histo1D((\"ESV_JetMETLogic_dRbb_baseline{}\".format(postfix), \"\", 64,0,2*pi), \"ESV_JetMETLogic_dRbb_baseline\", wgtVar)\n",
    "            #histos1D_dict[\"EventVars\"][\"DiLepMass_baseline\"] = input_df.Histo1D((\"ESV_JetMETLogic_DiLepMass_baseline{}\".format(postfix), \"\", 100,0,500), \"ESV_JetMETLogic_DiLepMass_baseline\", wgtVar)\n",
    "            #histos1D_dict[\"EventVars\"][\"HT_selection\"] = input_df.Histo1D((\"ESV_JetMETLogic_HT_selection{}\".format(postfix), \"\", 100,400,1400), \"ESV_JetMETLogic_HT_selection\", wgtVar)\n",
    "            #histos1D_dict[\"EventVars\"][\"H_selection\"] = input_df.Histo1D((\"ESV_JetMETLogic_H_selection{}\".format(postfix), \"\", 100,400,1400), \"ESV_JetMETLogic_H_selection\", wgtVar)\n",
    "            #histos1D_dict[\"EventVars\"][\"HT2M_selection\"] = input_df.Histo1D((\"ESV_JetMETLogic_HT2M_selection{}\".format(postfix), \"\", 100,400,900), \"ESV_JetMETLogic_HT2M_selection\", wgtVar)\n",
    "            #histos1D_dict[\"EventVars\"][\"H2M_selection\"] = input_df.Histo1D((\"ESV_JetMETLogic_H2M_selection{}\".format(postfix), \"\", 100,400,900), \"ESV_JetMETLogic_H2M_selection\", wgtVar)\n",
    "            #histos1D_dict[\"EventVars\"][\"HTb_selection\"] = input_df.Histo1D((\"ESV_JetMETLogic_HTb_selection{}\".format(postfix), \"\", 100,400,900), \"ESV_JetMETLogic_HTb_selection\", wgtVar)\n",
    "            #histos1D_dict[\"EventVars\"][\"HTH_selection\"] = input_df.Histo1D((\"ESV_JetMETLogic_HTH_selection{}\".format(postfix), \"\", 100,0,1), \"ESV_JetMETLogic_HTH_selection\", wgtVar)\n",
    "            #histos1D_dict[\"EventVars\"][\"HTRat_selection\"] = input_df.Histo1D((\"ESV_JetMETLogic_HTRat_selection{}\".format(postfix), \"\", 100,0,1), \"ESV_JetMETLogic_HTRat_selection\", wgtVar)\n",
    "            #histos1D_dict[\"EventVars\"][\"dRbb_selection\"] = input_df.Histo1D((\"ESV_JetMETLogic_dRbb_selection{}\".format(postfix), \"\", 64,0,2*pi), \"ESV_JetMETLogic_dRbb_selection\", wgtVar)\n",
    "            #histos1D_dict[\"EventVars\"][\"DiLepMass_selection\"] = input_df.Histo1D((\"ESV_JetMETLogic_DiLepMass_selection{}\".format(postfix), \"\", 100,0,500), \"ESV_JetMETLogic_DiLepMass_selection\", wgtVar)\n",
    "            histos1D_dict[\"EventVars\"][\"MET_pt\"] = input_df.Histo1D((\"MET_xycorr_pt{}\".format(postfix), \"\", 100,30,1030), fillMET_pt, wgtVar)\n",
    "            histos1D_dict[\"EventVars\"][\"MET_phi\"] = input_df.Histo1D((\"MET_xycorr_phi{}\".format(postfix), \"\", 100,-pi,pi), fillMET_phi, wgtVar)\n",
    "            histos1D_dict[\"EventVars\"][\"HT\"] = input_df.Histo1D((\"HT{}\".format(postfix), \"\", 130,400,1700), \"FTAJet{bpf}_HT\", wgtVar)\n",
    "            histos1D_dict[\"EventVars\"][\"H\"] = input_df.Histo1D((\"H{}\".format(postfix), \"\", 160,400,2000), \"FTAJet{bpf}_H\", wgtVar)\n",
    "            histos1D_dict[\"EventVars\"][\"HT2M\"] = input_df.Histo1D((\"HT2M{}\".format(postfix), \"\", 100,0,1000), \"FTAJet{bpf}_HT2M\", wgtVar)\n",
    "            histos1D_dict[\"EventVars\"][\"H2M\"] = input_df.Histo1D((\"H2M{}\".format(postfix), \"\", 150,0,1500), \"FTAJet{bpf}_H2M\", wgtVar)\n",
    "            histos1D_dict[\"EventVars\"][\"HTb\"] = input_df.Histo1D((\"HTb{}\".format(postfix), \"\", 100,0,1000), \"FTAJet{bpf}_HTb\", wgtVar)\n",
    "            histos1D_dict[\"EventVars\"][\"HTH\"] = input_df.Histo1D((\"HTH{}\".format(postfix), \"\", 100,0,1), \"FTAJet{bpf}_HTH\", wgtVar)\n",
    "            histos1D_dict[\"EventVars\"][\"HTRat\"] = input_df.Histo1D((\"HTRat{}\".format(postfix), \"\", 100,0,1), \"FTAJet{bpf}_HTRat\", wgtVar)\n",
    "            histos1D_dict[\"EventVars\"][\"dRbb\"] = input_df.Histo1D((\"dRbb{}\".format(postfix), \"\", 64,0,2*pi), \"FTAJet{bpf}_dRbb\", wgtVar)\n",
    "            histos1D_dict[\"EventVars\"][\"dPhibb\"] = input_df.Histo1D((\"dPhibb{}\".format(postfix), \"\", 64,-pi,pi), \"FTAJet{bpf}_dPhibb\", wgtVar)\n",
    "            histos1D_dict[\"EventVars\"][\"dEtabb\"] = input_df.Histo1D((\"dEtabb{}\".format(postfix), \"\", 50,0,5), \"FTAJet{bpf}_dEtabb\", wgtVar)\n",
    "            histos1D_dict[\"EventVars\"][\"dRll\"] = input_df.Histo1D((\"dRll{}\".format(postfix), \"\", 64,0,2*pi), \"FTALepton{lpf}_dRll\", wgtVar)\n",
    "            histos1D_dict[\"EventVars\"][\"dPhill\"] = input_df.Histo1D((\"dPhill{}\".format(postfix), \"\", 64,-pi,pi), \"FTALepton{lpf}_dPhill\", wgtVar)\n",
    "            histos1D_dict[\"EventVars\"][\"dEtall\"] = input_df.Histo1D((\"dEtall{}\".format(postfix), \"\", 50,0,5), \"FTALepton{lpf}_dEtall\", wgtVar)\n",
    "            histos1D_dict[\"EventVars\"][\"MTofMETandEl\"] = input_df.Histo1D((\"MTofMETandEl{}\".format(postfix), \"\", 100, 0, 200), \"MTofMETandEl\", wgtVar)\n",
    "            histos1D_dict[\"EventVars\"][\"MTofMETandMu\"] = input_df.Histo1D((\"MTofMETandMu{}\".format(postfix), \"\", 100, 0, 200), \"MTofMETandMu\", wgtVar)\n",
    "            histos1D_dict[\"EventVars\"][\"MTofElandMu\"] = input_df.Histo1D((\"MTofElandMu{}\".format(postfix), \"\", 100, 0, 200), \"MTofElandMu\", wgtVar)\n",
    "            #histos1D_dict[\"EventVars\"][\"MTMasslessCheck\"] = input_df.Histo1D((\"MTMasslessCheck{}\".format(postfix), \"\", 100, 0, 200), \"MTMasslessCheck\", wgtVar)\n",
    "            #histos1D_dict[\"EventVars\"][\"MTCrossCheck\"] = input_df.Histo1D((\"MTCrossCheck{}\".format(postfix), \"\", 100, 0, 200), \"MTCrossCheck\", wgtVar)\n",
    "            #histos1D_dict[\"EventVars\"][\"MTCrossCheckDifference\"] = input_df.Histo1D((\"MTCrossCheckDifference{}\".format(postfix), \"\", 100, 0, 10), \"MTCrossCheckDifference\", wgtVar)\n",
    "            #histos1D_dict[\"EventVars\"][\"MTCrossCheckMasslessDifference\"] = input_df.Histo1D((\"MTCrossCheckMasslessDifference{}\".format(postfix), \"\", 100, 0, 0.02), \"MTCrossCheckMasslessDifference\", wgtVar)\n",
    "            histos1D_dict[\"EventVars\"][\"PV_npvsGood\"] = input_df.Histo1D((\"PV_npvsGood{}\".format(postfix), \"\", 100, 0, 100), \"PV_npvsGood\", wgtVar)\n",
    "            histos1D_dict[\"EventVars\"][\"PV_npvs\"] = input_df.Histo1D((\"PV_npvs{}\".format(postfix), \"\", 150, 0, 150), \"PV_npvs\", wgtVar)\n",
    "            if isData == False:\n",
    "                histos1D_dict[\"EventVars\"][\"Pileup_nTrueInt\"] = input_df.Histo1D((\"Pileup_TrueInt{}\".format(postfix), \";Pileup_TrueInt;Events\", 150, 0, 150), \"Pileup_nTrueInt\", wgtVar)\n",
    "                histos1D_dict[\"EventVars\"][\"Pileup_nTrueInt_XS\"] = input_df.Histo1D((\"Pileup_TrueInt_({})\".format(\"wgt_SUMW\"), \";Pileup_TrueInt;Events\", 150, 0, 150), \"Pileup_nTrueInt\", \"wgt_SUMW\")\n",
    "                histos1D_dict[\"EventVars\"][\"Pileup_nPU_XS\"] = input_df.Histo1D((\"Pileup_nPU_({})\".format(\"wgt_SUMW\"), \";Pileup_nPU;Events\", 150, 0, 150), \"Pileup_nPU\", \"wgt_SUMW\")\n",
    "                histos1D_dict[\"EventVars\"][\"Pileup_nPU\"] = input_df.Histo1D((\"Pileup_nPU{}\".format(postfix), \";Pileup_nPU;Events\", 150, 0, 150), \"Pileup_nPU\", wgtVar)\n",
    "            \n",
    "        if histos2D_dict != None:\n",
    "            if \"EventVars\" not in histos2D_dict:\n",
    "                histos2D_dict[\"EventVars\"] = {}\n",
    "            if isData == False:\n",
    "                histos2D_dict[\"EventVars\"][\"npvsGood_vs_nTrueInt\"] = input_df.Histo2D((\"npvsGood_vs_nTrueInt{}\".format(postfix), \";nTrueInt;npvsGood\", 150, 0, 150, 150, 0, 150), \"Pileup_nTrueInt\", \"PV_npvsGood\", wgtVar)\n",
    "                histos2D_dict[\"EventVars\"][\"npvsGood_vs_nPU\"] = input_df.Histo2D((\"npvsGood_vs_nPU{}\".format(postfix), \";nPU;npvsGood\", 150, 0, 150, 150, 0, 150), \"Pileup_nPU\", \"PV_npvsGood\", wgtVar)\n",
    "                histos2D_dict[\"EventVars\"][\"npvs_vs_nTrueInt\"] = input_df.Histo2D((\"npvs_vs_nTrueInt{}\".format(postfix), \";nTrueInt;npvs\", 150, 0, 150, 150, 0, 150), \"Pileup_nTrueInt\", \"PV_npvs\", wgtVar)\n",
    "                histos2D_dict[\"EventVars\"][\"npvs_vs_nPU\"] = input_df.Histo2D((\"npvs_vs_nPU{}\".format(postfix), \";nPU;npvs\", 150, 0, 150, 150, 0, 150), \"Pileup_nPU\", \"PV_npvs\", wgtVar)\n",
    "            \n",
    "            if debugInfo == True:\n",
    "                #histos1D_dict[\"EventVars\"][\"Jet{bpf}_HT_Match\"] = input_df.Histo1D((\"Jet{bpf}_HT_Match{}\".format(postfix), \"\", 2,0,2), \"Jet{bpf}_HT_matches\", wgtVar)\n",
    "                pass\n",
    "            \n",
    "    if makeMountains == True:\n",
    "        if useDeepCSV == True:\n",
    "            tagger = \"CSV\"\n",
    "        else:\n",
    "            tagger = \"Jet\"\n",
    "        theCatsL0 = collections.OrderedDict()#Baseline nJet selection (inclusive!) for each systematic scale variation\n",
    "        theCatsL1 = collections.OrderedDict() #Next level of selection, like nJet categories. If we need nBTag inclusive, they'll have to go here\n",
    "        theCatsL2 = collections.OrderedDict() #Next level of selection, i.e. nBTag categories. \n",
    "        theCatsL0[\"Inclusive{bpf}\".format(bpf=branchpostfix)] = \"nFTAJet{bpf} >= 4\".format(bpf=branchpostfix)\n",
    "        \n",
    "        theCatsL1[\"nMediumDeep{tag}0{bpf}\".format(tag=tagger, bpf=branchpostfix)] = \"nMediumDeep{tag}B{pf} == 0\".format(tag=tagger)\n",
    "        theCatsL1[\"nMediumDeep{tag}1{bpf}\".format(tag=tagger, bpf=branchpostfix)] = \"nMediumDeep{tag}B{pf} == 1\".format(tag=tagger)\n",
    "        theCatsL1[\"nMediumDeep{tag}2{bpf}\".format(tag=tagger, bpf=branchpostfix)] = \"nMediumDeep{tag}B{pf} == 2\".format(tag=tagger)\n",
    "        theCatsL1[\"blind_nMediumDeep{tag}3{bpf}\".format(tag=tagger, bpf=branchpostfix)] = \"nMediumDeep{tag}B{pf} == 3\".format(tag=tagger)\n",
    "        theCatsL1[\"blind_nMediumDeep{tag}4{bpf}\".format(tag=tagger, bpf=branchpostfix)] = \"nMediumDeep{tag}B{pf} == 4\".format(tag=tagger)\n",
    "        \n",
    "        theCatsL2[\"nJet4{bpf}\".format(bpf=branchpostfix)] = \"nFTAJet{bpf} == 4\".format(bpf=branchpostfix)\n",
    "        theCatsL2[\"nJet5{bpf}\".format(bpf=branchpostfix)] = \"nFTAJet{bpf} == 5\".format(bpf=branchpostfix)\n",
    "        theCatsL2[\"blind_nJet6{bpf}\".format(bpf=branchpostfix)] = \"nFTAJet{bpf} == 6\".format(bpf=branchpostfix)\n",
    "        theCatsL2[\"blind_nJet7{bpf}\".format(bpf=branchpostfix)] = \"nFTAJet{bpf} == 7\".format(bpf=branchpostfix)\n",
    "        theCatsL2[\"blind_nJet8+{bpf}\".format(bpf=branchpostfix)] = \"nFTAJet{bpf} >= 8\".format(bpf=branchpostfix)\n",
    "        \n",
    "        cat_df = collections.OrderedDict()\n",
    "        for ck, cs in theCats.items():\n",
    "            cat_df[ck] = input_df.Filter(cs, cs)\n",
    "        \n",
    "        \n",
    "        #theCats[\"nMediumDeep{tag}0\".format(tag=tagger)] = \"nJet{bpf}_MediumDeep{tag} == 0\".format(tag=tagger)\n",
    "        #theCats[\"nMediumDeep{tag}1\".format(tag=tagger)] = \"nJet{bpf}_MediumDeep{tag} == 1\".format(tag=tagger)\n",
    "        #theCats[\"nMediumDeep{tag}2\".format(tag=tagger)] = \"nJet{bpf}_MediumDeep{tag} == 2\".format(tag=tagger)\n",
    "        \n",
    "        #theCats[\"nMediumDeep{}0_nJet4\".format(tagger)] = \"nJet{bpf}_MediumDeep{} == 0 && nJet{bpf} == 4\".format(tagger)\n",
    "        #theCats[\"nMediumDeep{}1_nJet4\".format(tagger)] = \"nJet{bpf}_MediumDeep{} == 1 && nJet{bpf} == 4\".format(tagger)\n",
    "        #theCats[\"nMediumDeep{}2_nJet4\".format(tagger)] = \"nJet{bpf}_MediumDeep{} == 2 && nJet{bpf} == 4\".format(tagger)\n",
    "        #theCats[\"blind_nMediumDeep{}3_nJet4\".format(tagger)] = \"nJet{bpf}_MediumDeep{} == 3 && nJet{bpf} == 4\".format(tagger)\n",
    "        #theCats[\"blind_nMediumDeep{}4+_nJet4\".format(tagger)] = \"nJet{bpf}_MediumDeep{} >= 4 && nJet{bpf} == 4\".format(tagger)\n",
    "        \n",
    "        #theCats[\"nMediumDeep{}0_nJet5\".format(tagger)] = \"nJet{bpf}_MediumDeep{} == 0 && nJet{bpf} == 5\".format(tagger)\n",
    "        #theCats[\"nMediumDeep{}1_nJet5\".format(tagger)] = \"nJet{bpf}_MediumDeep{} == 1 && nJet{bpf} == 5\".format(tagger)\n",
    "        #theCats[\"nMediumDeep{}2_nJet5\".format(tagger)] = \"nJet{bpf}_MediumDeep{} == 2 && nJet{bpf} == 5\".format(tagger)\n",
    "        #theCats[\"blind_nMediumDeep{}3_nJet5\".format(tagger)] = \"nJet{bpf}_MediumDeep{} == 3 && nJet{bpf} == 5\".format(tagger)\n",
    "        #theCats[\"blind_nMediumDeep{}4+_nJet5\".format(tagger)] = \"nJet{bpf}_MediumDeep{} >= 4 && nJet{bpf} == 5\".format(tagger)\n",
    "        \n",
    "        #theCats[\"nMediumDeep{}0_nJet6\".format(tagger)] = \"nJet{bpf}_MediumDeep{} == 0 && nJet{bpf} == 6\".format(tagger)\n",
    "        #theCats[\"nMediumDeep{}1_nJet6\".format(tagger)] = \"nJet{bpf}_MediumDeep{} == 1 && nJet{bpf} == 6\".format(tagger)\n",
    "        #theCats[\"nMediumDeep{}2_nJet6\".format(tagger)] = \"nJet{bpf}_MediumDeep{} == 2 && nJet{bpf} == 6\".format(tagger)\n",
    "        #theCats[\"blind_nMediumDeep{}3_nJet6\".format(tagger)] = \"nJet{bpf}_MediumDeep{} == 3 && nJet{bpf} == 6\".format(tagger)\n",
    "        #theCats[\"blind_nMediumDeep{}4+_nJet6\".format(tagger)] = \"nJet{bpf}_MediumDeep{} >= 4 && nJet{bpf} == 6\".format(tagger)\n",
    "        \n",
    "        #theCats[\"nMediumDeep{}0_nJet7\".format(tagger)] = \"nJet{bpf}_MediumDeep{} == 0 && nJet{bpf} == 7\".format(tagger)\n",
    "        #theCats[\"nMediumDeep{}1_nJet7\".format(tagger)] = \"nJet{bpf}_MediumDeep{} == 1 && nJet{bpf} == 7\".format(tagger)\n",
    "        #theCats[\"blind_nMediumDeep{}2_nJet7\".format(tagger)] = \"nJet{bpf}_MediumDeep{} == 2 && nJet{bpf} == 7\".format(tagger)\n",
    "        #theCats[\"blind_nMediumDeep{}3_nJet7\".format(tagger)] = \"nJet{bpf}_MediumDeep{} == 3 && nJet{bpf} == 7\".format(tagger)\n",
    "        #theCats[\"blind_nMediumDeep{}4+_nJet7\".format(tagger)] = \"nJet{bpf}_MediumDeep{} >= 4 && nJet{bpf} == 7\".format(tagger)\n",
    "        \n",
    "        #theCats[\"nMediumDeep{}0_nJet8+\".format(tagger)] = \"nJet{bpf}_MediumDeep{} == 0 && nJet{bpf} >= 8\".format(tagger)\n",
    "        #theCats[\"nMediumDeep{}1_nJet8+\".format(tagger)] = \"nJet{bpf}_MediumDeep{} == 1 && nJet{bpf} >= 8\".format(tagger)\n",
    "        #theCats[\"blind_nMediumDeep{}2_nJet8+\".format(tagger)] = \"nJet{bpf}_MediumDeep{} == 2 && nJet{bpf} >= 8\".format(tagger)\n",
    "        #theCats[\"blind_nMediumDeep{}3_nJet8+\".format(tagger)] = \"nJet{bpf}_MediumDeep{} == 3 && nJet{bpf} >= 8\".format(tagger)\n",
    "        #theCats[\"blind_nMediumDeep{}4+_nJet8+\".format(tagger)] = \"nJet{bpf}_MediumDeep{} >= 4 && nJet{bpf} >= 8\".format(tagger)\n",
    "        \n",
    "        if histos1D_dict != None:\n",
    "            if \"sysVar_{spf}\".format(spf=syspostfix) not in histos1D_dict:\n",
    "                histos1D_dict[\"sysVar_{spf}\".format(spf=syspostfix)] = {}\n",
    "            for tc in theCats.keys(): \n",
    "                if tc not in histos1D_dict[\"sysVar_{spf}\".format(spf=syspostfix)]: \n",
    "                    histos1D_dict[\"sysVar_{spf}\".format(spf=syspostfix)][tc] = {}\n",
    "            for tc, cut in theCats.items():\n",
    "                tcn = tc#.replace(\"blind_\", \"\") #FIXME do I want blind stripped with new convention for naming?\n",
    "                for x in xrange(nJetsToHisto):\n",
    "                    #FIXME: None of these are based upon jet scale variations, yet!\n",
    "                    histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"Jet_pt_jet{}\".format(x+1)] = cat_df[tc].Histo1D((\"{name}__{cat}__Jet_pt_jet{n}_{spf}\".format(name=sampleName, n=x+1, cat=tcn, spf=syspostfix), \"\", 100, 0, 500), \"FTAJet{bpf}_pt_jet{n}\".format(bpf=branchpostfix, n=x+1), wgtVar)\n",
    "                    histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"Jet_eta_jet{}\".format(x+1)] = cat_df[tc].Histo1D((\"{name}__{cat}__Jet_eta_jet{n}_{spf}\".format(name=sampleName, n=x+1, cat=tcn, spf=syspostfix), \"\", 104, -2.6, 2.6), \"FTAJet{bpf}_eta_jet{n}\".format(bpf=branchpostfix, n=x+1), wgtVar)\n",
    "                    histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"Jet_phi_jet{}\".format(x+1)] = cat_df[tc].Histo1D((\"{name}__{cat}__Jet_phi_jet{n}_{spf}\".format(name=sampleName, n=x+1, cat=tcn, spf=syspostfix), \"\", 64, -pi, pi), \"FTAJet{bpf}_phi_jet{n}\".format(bpf=branchpostfix, n=x+1), wgtVar)\n",
    "                    histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"Jet_DeepCSV_jet{}\".format(x+1)] = cat_df[tc].Histo1D((\"{name}__{cat}__Jet_DeepCSV_jet{n}_{spf}\".format(name=sampleName, n=x+1, cat=tcn, spf=syspostfix), \"\", 100, 0.0, 1.0), \"FTAJet{bpf}_DeepCSV_jet{n}\".format(bpf=branchpostfix, n=x+1), wgtVar)\n",
    "                    histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"Jet_DeepJet_jet{}\".format(x+1)] = cat_df[tc].Histo1D((\"{name}__{cat}__Jet_DeepJet_jet{n}_{spf}\".format(name=sampleName, n=x+1, cat=tcn, spf=syspostfix), \"\", 100, 0.0, 1.0), \"FTAJet{bpf}_DeepJet_jet{n}\".format(bpf=branchpostfix, n=x+1), wgtVar)\n",
    "                    histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"Jet_DeepCSV_sortedjet{}\".format(x+1)] = cat_df[tc].Histo1D((\"{name}__{cat}__Jet_DeepCSV_jet{n}_{spf}\".format(name=sampleName, n=x+1, cat=tcn, spf=syspostfix), \"\", 100, 0.0, 1.0), \"FTAJet{bpf}_DeepCSV_sortedjet{n}\".format(bpf=branchpostfix, n=x+1), wgtVar)\n",
    "                    histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"Jet_DeepJet_sortedjet{}\".format(x+1)] = cat_df[tc].Histo1D((\"{name}__{cat}__Jet_DeepJet_jet{n}_{spf}\".format(name=sampleName, n=x+1, cat=tcn, spf=syspostfix), \"\", 100, 0.0, 1.0), \"FTAJet{bpf}_DeepJet_sortedjet{n}\".format(bpf=branchpostfix, n=x+1), wgtVar)\n",
    "                histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"MET_pt\"] = cat_df[tc].Histo1D((\"{name}__{cat}__MET_pt_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \"\", 20,0,1000), fillMET_pt, wgtVar)\n",
    "                histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"MET_phi\"] = cat_df[tc].Histo1D((\"{name}__{cat}__MET_phi_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \"\", 20,-pi,pi), fillMET_phi, wgtVar)\n",
    "                histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"Muon_pfRelIso03_all\"] = cat_df[tc].Histo1D((\"{name}__{cat}__Muon_pfRelIso03_all_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \"\", 20, 0, 0.2), \"FTAMuon{lpf}_pfRelIso03_all\", wgtVar)\n",
    "                histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"Muon_pfRelIso03_chg\"] = cat_df[tc].Histo1D((\"{name}__{cat}__Muon_pfRelIso03_chg_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \"\", 20, 0, 0.2), \"FTAMuon{lpf}_pfRelIso03_chg\", wgtVar)\n",
    "                histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"Muon_pfRelIso04_all\"] = cat_df[tc].Histo1D((\"{name}__{cat}__Muon_pfRelIso04_all_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \"\", 20, 0, 0.2), \"FTAMuon{lpf}_pfRelIso04_all\", wgtVar)\n",
    "                histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"Electron_pfRelIso03_all\"] = cat_df[tc].Histo1D((\"{name}__{cat}__Electron_pfRelIso03_all_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \"\", 20, 0, 0.2), \"FTAElectron{lpf}_pfRelIso03_all\", wgtVar)\n",
    "                histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"Electron_pfRelIso03_chg\"] = cat_df[tc].Histo1D((\"{name}__{cat}__Electron_pfRelIso03_chg_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \"\", 20, 0, 0.2), \"FTAElectron{lpf}_pfRelIso03_chg\", wgtVar)\n",
    "                histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"HT\"] = cat_df[tc].Histo1D((\"{name}__{cat}__HT_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \"\", 30,400,2000), \"Jet{bpf}_HT\", wgtVar)\n",
    "                histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"H\"] = cat_df[tc].Histo1D((\"{name}__{cat}__H_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \"\", 30,400,2000), \"Jet{bpf}_H\", wgtVar)\n",
    "                histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"HT2M\"] = cat_df[tc].Histo1D((\"{name}__{cat}__HT2M_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \"\", 20,0,1000), \"Jet{bpf}_HT2M\", wgtVar)\n",
    "                histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"H2M\"] = cat_df[tc].Histo1D((\"{name}__{cat}__H2M_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \"\", 20,0,1500), \"Jet{bpf}_H2M\", wgtVar)\n",
    "                histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"HTb\"] = cat_df[tc].Histo1D((\"{name}__{cat}__HTb_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \"\", 20,0,1000), \"Jet{bpf}_HTb\", wgtVar)\n",
    "                histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"HTH\"] = cat_df[tc].Histo1D((\"{name}__{cat}__HTH_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \"\", 20,0,1), \"Jet{bpf}_HTH\", wgtVar)\n",
    "                histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"HTRat\"] = cat_df[tc].Histo1D((\"{name}__{cat}__HTRat_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \"\", 20,0,1), \"Jet{bpf}_HTRat\", wgtVar)\n",
    "                histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"dRbb\"] = cat_df[tc].Histo1D((\"{name}__{cat}__dRbb_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \"\", 16,0,2*pi), \"Jet{bpf}_dRbb\", wgtVar)\n",
    "                histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"dPhibb\"] = cat_df[tc].Histo1D((\"{name}__{cat}__dPhibb_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \"\", 16,-pi,pi), \"Jet{bpf}_dPhibb\", wgtVar)\n",
    "                histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"dEtabb\"] = cat_df[tc].Histo1D((\"{name}__{cat}__dEtabb_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \"\", 10,0,5), \"Jet{bpf}_dEtabb\", wgtVar)\n",
    "                histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"Lepton{lpf}_pt_LeadLep\"] = cat_df[tc].Histo1D((\"{name}__{cat}__Lepton{lpf}_pt_LeadLep_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \"\", 100,0,500), \"FTALepton{lpf}_pt_LeadLep\", wgtVar)\n",
    "                histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"Lepton{lpf}_pt_SubleadLep\"] = cat_df[tc].Histo1D((\"{name}__{cat}__Lepton{lpf}_pt_SubleadLep_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \"\", 100,0,500), \"FTALepton{lpf}_pt_SubleadLep\", wgtVar)\n",
    "                #histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"Muon{lpf}_pt\"] = cat_df[tc].Histo1D((\"{name}__{cat}__Muon{lpf}_pt_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \"\", 100,0,500), \"FTAMuon{lpf}_pt\", wgtVar)\n",
    "                histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"Lepton{lpf}_eta_LeadLep\"] = cat_df[tc].Histo1D((\"{name}__{cat}__Lepton{lpf}_eta_LeadLep_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \"\", 52,-2.6,2.6), \"FTALepton{lpf}_eta_LeadLep\", wgtVar)\n",
    "                histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"Lepton{lpf}_eta_SubleadLep\"] = cat_df[tc].Histo1D((\"{name}__{cat}__Lepton{lpf}_eta_SubleadLep_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \"\", 52,-2.6,2.6), \"FTALepton{lpf}_eta_SubleadLep\", wgtVar)\n",
    "                #histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"Lepton{lpf}_eta_SubleadLep\"] = cat_df[tc].Histo1D((\"{name}__{cat}__Lepton{lpf}_eta_SubleadLep_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \"\", 52,-2.6,2.6), \"FTALepton{lpf}_eta_SubleadLep\", wgtVar)\n",
    "                histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"dRll\"] = cat_df[tc].Histo1D((\"{name}__{cat}__dRll_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \"\", 16,0,2*pi), \"FTALepton{lpf}_dRll\", wgtVar)\n",
    "                histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"dPhill\"] = cat_df[tc].Histo1D((\"{name}__{cat}__dPhill_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \"\", 16,-pi,pi), \"FTALepton{lpf}_dPhill\", wgtVar)\n",
    "                histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"dEtall\"] = cat_df[tc].Histo1D((\"{name}__{cat}__dEtall_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \"\", 10,0,5), \"FTALepton{lpf}_dEtall\", wgtVar)\n",
    "                histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"MTofMETandEl\"] = cat_df[tc].Histo1D((\"{name}__{cat}__MTofMETandEl_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \"\", 20, 0, 200), \"MTofMETandEl{}\".format(postfix), wgtVar)\n",
    "                histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"MTofMETandMu\"] = cat_df[tc].Histo1D((\"{name}__{cat}__MTofMETandMu_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \"\", 20, 0, 200), \"MTofMETandMu{}\".format(postfix), wgtVar)\n",
    "                histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"MTofElandMu\"] = cat_df[tc].Histo1D((\"{name}__{cat}__MTofElandMu_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \"\", 20, 0, 200), \"MTofElandMu{}\".format(postfix), wgtVar)\n",
    "                histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"nJet\"] = cat_df[tc].Histo1D((\"{name}__{cat}__nJet_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \"\", 14, 0, 14), \"nJet{bpf}\", wgtVar)\n",
    "                if isData is False:\n",
    "                    histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"nJet_genMatched\"] = cat_df[tc].Histo1D((\"{name}__{cat}__nJet_genMatched_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \"\", 14, 0, 14), \"nJet{bpf}_genMatched\", wgtVar)\n",
    "                    histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"nJet_genMatched_puIdLoose\"] = cat_df[tc].Histo1D((\"{name}__{cat}__nJet_genMatched_puIdLoose_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \"\", 14, 0, 14), \"nJet{bpf}_genMatched_puIdLoose\", wgtVar)\n",
    "                histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"nLooseDeepCSV\"] = cat_df[tc].Histo1D((\"{name}__{cat}__nJet_LooseDeepCSV_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \"\", 6, 0, 6), \"nJet{bpf}_LooseDeepCSV\", wgtVar)\n",
    "                histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"nMediumDeepCSV\"] = cat_df[tc].Histo1D((\"{name}__{cat}__nJet_MediumDeepCSV_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \"\", 6, 0, 6), \"nJet{bpf}_MediumDeepCSV\", wgtVar)\n",
    "                histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"nTightDeepCSV\"] = cat_df[tc].Histo1D((\"{name}__{cat}__nJet_TightDeepCSV_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \"\", 6, 0, 6), \"nJet{bpf}_TightDeepCSV\", wgtVar)\n",
    "                histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"nLooseDeepJet\"] = cat_df[tc].Histo1D((\"{name}__{cat}__nJet_LooseDeepJet_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \"\", 6, 0, 6), \"nJet{bpf}_LooseDeepJet\", wgtVar)\n",
    "                histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"nMediumDeepJet\"] = cat_df[tc].Histo1D((\"{name}__{cat}__nJet_MediumDeepJet_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \"\", 6, 0, 6), \"nJet{bpf}_MediumDeepJet\", wgtVar)\n",
    "                histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"nTightDeepJet\"] = cat_df[tc].Histo1D((\"{name}__{cat}__nJet_TightDeepJet_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \"\", 6, 0, 6), \"nJet{bpf}_TightDeepJet\", wgtVar)\n",
    "                histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"nLooseMuon\"] = cat_df[tc].Histo1D((\"{name}__{cat}__nLooseFTAMuon{lpf}_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \"\", 4, 0, 4), \"nLooseFTAMuon{lpf}\", wgtVar)\n",
    "                histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"nMediumMuon\"] = cat_df[tc].Histo1D((\"{name}__{cat}__nMediumFTAMuon{lpf}_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \"\", 4, 0, 4), \"nMediumFTAMuon{lpf}\", wgtVar)\n",
    "                histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"nTightMuon\"] = cat_df[tc].Histo1D((\"{name}__{cat}__nTightFTAMuon{lpf}_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \"\", 4, 0, 4), \"nTightFTAMuon{lpf}\", wgtVar)\n",
    "                histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"nLooseElectron\"] = cat_df[tc].Histo1D((\"{name}__{cat}__nLooseFTAElectron{lpf}_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \"\", 4, 0, 4), \"nLooseFTAElectron{lpf}\", wgtVar)\n",
    "                histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"nMediumElectron\"] = cat_df[tc].Histo1D((\"{name}__{cat}__nMediumFTAElectron{lpf}_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \"\", 4, 0, 4), \"nMediumFTAElectron{lpf}\", wgtVar)\n",
    "                histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"nTightElectron\"] = cat_df[tc].Histo1D((\"{name}__{cat}__nTightFTAElectron{lpf}_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \"\", 4, 0, 4), \"nTightFTAElectron{lpf}\", wgtVar)\n",
    "                histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"nLooseLepton\"] = cat_df[tc].Histo1D((\"{name}__{cat}__nLooseFTALepton{lpf}_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \"\", 4, 0, 4), \"nLooseFTALepton{lpf}\", wgtVar)\n",
    "                histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"nMediumLepton\"] = cat_df[tc].Histo1D((\"{name}__{cat}__nMediumFTALepton{lpf}_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \"\", 4, 0, 4), \"nMediumFTALepton{lpf}\", wgtVar)\n",
    "                histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"nTightLepton\"] = cat_df[tc].Histo1D((\"{name}__{cat}__nTightFTALepton{lpf}_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \"\", 4, 0, 4), \"nTightFTALepton{lpf}\", wgtVar)\n",
    "                histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"MTofMETandEl\"] = cat_df[tc].Histo1D((\"{name}__{cat}__MTofMETandEl_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \"\", 20, 0, 200), \"MTofMETandEl\", wgtVar)\n",
    "                histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"MTofMETandMu\"] = cat_df[tc].Histo1D((\"{name}__{cat}__MTofMETandMu_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \"\", 20, 0, 200), \"MTofMETandMu\", wgtVar)\n",
    "                histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"MTofElandMu\"] = cat_df[tc].Histo1D((\"{name}__{cat}__MTofElandMu_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \"\", 20, 0, 200), \"MTofElandMu\", wgtVar)\n",
    "                histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"Muon_InvMass\"] = cat_df[tc].Histo1D((\"{name}__{cat}__Muon_InvMass_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \"\", 60, 0, 150), \"FTAMuon{lpf}_InvariantMass\", wgtVar)\n",
    "                histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"Electron_InvMass\"] = cat_df[tc].Histo1D((\"{name}__{cat}__Electron_InvMass_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \"\", 60, 0, 150), \"FTAElectron{lpf}_InvariantMass\", wgtVar)\n",
    "                histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"Muon_InvMass_v_MET\"] = cat_df[tc].Histo2D((\"{name}__{cat}__Muon_InvMass_v_MET_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \"\", 30, 0, 150, 20, 0, 400), \"FTAMuon{lpf}_InvariantMass\", \"METFixEE2017_pt\", wgtVar)\n",
    "                histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"Electron_InvMass_v_MET\"] = cat_df[tc].Histo2D((\"{name}__{cat}__Electron_InvMass_v_MET_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \"\", 30, 0, 150, 20, 0, 400), \"FTAElectron{lpf}_InvariantMass\", \"METFixEE2017_pt\", wgtVar)\n",
    "                #histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"Muon_pfRelIso03_all_vs_MET\"] = cat_df[tc].Histo2D((\"{name}__{cat}__Muon_pfRelIso03_all_vs_MET_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \";pfRelIso03_all;MET\", 30, 0., 0.2, 20,30.,1030.), \"FTAMuon{lpf}_pfRelIso03_all\", \"RVec_MET_pt\", wgtVar)\n",
    "                #histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"Muon_pfRelIso03_chg_vs_MET\"] = cat_df[tc].Histo2D((\"{name}__{cat}__Muon_pfRelIso03_chg_vs_MET_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \";pfRelIso03_chg;MET\", 30, 0, 0.2, 20,30,1030), \"FTAMuon{lpf}_pfRelIso03_chg\", \"RVec_MET_pt\", wgtVar)\n",
    "                #histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"Muon_pfRelIso04_all_vs_MET\"] = cat_df[tc].Histo2D((\"{name}__{cat}__Muon_pfRelIso04_all_vs_MET_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \";pfRelIso04_all;MET\", 30, 0, 0.2, 20,30,1030), \"FTAMuon{lpf}_pfRelIso04_all\", \"RVec_MET_pt\", wgtVar)\n",
    "                #histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"Electron_pfRelIso03_all_vs_MET\"] = cat_df[tc].Histo2D((\"{name}__{cat}__Electron_pfRelIso03_all_vs_MET_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \";pfRelIso03_all;MET\", 30, 0, 0.2, 20,30,1030), \"FTAElectron{lpf}_pfRelIso03_all\", \"RVec_MET_pt\", wgtVar)\n",
    "                #histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"Electron_pfRelIso03_chg_vs_MET\"] = cat_df[tc].Histo2D((\"{name}__{cat}__Electron_pfRelIso03_chg_vs_MET_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \";pfRelIso03_chg;MET\", 30, 0, 0.2, 20,30,1030), \"FTAElectron{lpf}_pfRelIso03_chg\", \"RVec_MET_pt\", wgtVar)\n",
    "                \n",
    "                if isData == False:\n",
    "                    pass                \n",
    "            \n",
    "        if histos2D_dict != None:\n",
    "            if \"sysVar_{spf}\".format(spf=syspostfix) not in histos2D_dict:\n",
    "                histos2D_dict[\"sysVar{spf}\".format(spf=syspostfix)] = {}\n",
    "            for tc in theCats.keys(): \n",
    "                if tc not in histos2D_dict[\"sysVar{spf}\".format(spf=syspostfix)]: \n",
    "                    histos2D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc] = {}\n",
    "            for tc, cut in theCats.items():\n",
    "                tcn = tc.replace(\"blind_\", \"\")\n",
    "                #histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"Muon_pfRelIso03_all_vs_MET\"] = cat_df[tc].Histo2D((\"{name}__{cat}__Muon_pfRelIso03_all_vs_MET_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \";pfRelIso03_all;MET\", 30, 0., 0.2, 20,30.,1030.), \"FTAMuon{lpf}_pfRelIso03_all\", \"RVec_MET_pt\", wgtVar)\n",
    "                #histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"Muon_pfRelIso03_chg_vs_MET\"] = cat_df[tc].Histo2D((\"{name}__{cat}__Muon_pfRelIso03_chg_vs_MET_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \";pfRelIso03_chg;MET\", 30, 0, 0.2, 20,30,1030), \"FTAMuon{lpf}_pfRelIso03_chg\", \"RVec_MET_pt\", wgtVar)\n",
    "                #histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"Muon_pfRelIso04_all_vs_MET\"] = cat_df[tc].Histo2D((\"{name}__{cat}__Muon_pfRelIso04_all_vs_MET_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \";pfRelIso04_all;MET\", 30, 0, 0.2, 20,30,1030), \"FTAMuon{lpf}_pfRelIso04_all\", \"RVec_MET_pt\", wgtVar)\n",
    "                #histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"Electron_pfRelIso03_all_vs_MET\"] = cat_df[tc].Histo2D((\"{name}__{cat}__Electron_pfRelIso03_all_vs_MET_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \";pfRelIso03_all;MET\", 30, 0, 0.2, 20,30,1030), \"FTAElectron{lpf}_pfRelIso03_all\", \"RVec_MET_pt\", wgtVar)\n",
    "                #histos1D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"Electron_pfRelIso03_chg_vs_MET\"] = cat_df[tc].Histo2D((\"{name}__{cat}__Electron_pfRelIso03_chg_vs_MET_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \";pfRelIso03_chg;MET\", 30, 0, 0.2, 20,30,1030), \"FTAElectron{lpf}_pfRelIso03_chg\", \"RVec_MET_pt\", wgtVar)\n",
    "                #### Older versions\n",
    "                #histos2D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"Muon_pfRelIso03_all_vs_MET\"] = cat_df[tc].Histo2D((\"{name}__{cat}__Muon_pfRelIso03_all_vs_MET_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \";pfRelIso03_all;MET\", 100, 0., 0.2, 100,30.,1030.), \"FTAMuon{lpf}_pfRelIso03_all\", \"RVec_MET_pt\", wgtVar)\n",
    "                #histos2D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"Muon_pfRelIso03_chg_vs_MET\"] = cat_df[tc].Histo2D((\"{name}__{cat}__Muon_pfRelIso03_chg_vs_MET_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \";pfRelIso03_chg;MET\", 100, 0, 0.2, 100,30,1030), \"FTAMuon{lpf}_pfRelIso03_chg\", \"RVec_MET_pt\", wgtVar)\n",
    "                #histos2D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"Muon_pfRelIso04_all_vs_MET\"] = cat_df[tc].Histo2D((\"{name}__{cat}__Muon_pfRelIso04_all_vs_MET_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \";pfRelIso04_all;MET\", 100, 0, 0.2, 100,30,1030), \"FTAMuon{lpf}_pfRelIso04_all\", \"RVec_MET_pt\", wgtVar)\n",
    "                #histos2D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"Electron_pfRelIso03_all_vs_MET\"] = cat_df[tc].Histo2D((\"{name}__{cat}__Electron_pfRelIso03_all_vs_MET_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \";pfRelIso03_all;MET\", 100, 0, 0.2, 100,30,1030), \"FTAElectron{lpf}_pfRelIso03_all\", \"RVec_MET_pt\", wgtVar)\n",
    "                #histos2D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"Electron_pfRelIso03_chg_vs_MET\"] = cat_df[tc].Histo2D((\"{name}__{cat}__Electron_pfRelIso03_chg_vs_MET_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \";pfRelIso03_chg;MET\", 100, 0, 0.2, 100,30,1030), \"FTAElectron{lpf}_pfRelIso03_chg\", \"RVec_MET_pt\", wgtVar)\n",
    "                if isData == False:\n",
    "                    #histos2D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"test1\"] = cat_df[tc].Histo2D((\"{name}__{cat}__test1_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \";nTrueInt;npvsGood\", 150, 0, 150, 150, 0, 150), \"FTAMuon{lpf}_pfRelIso03_all\", \"PV_npvsGood\", wgtVar)\n",
    "                    #histos2D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"test2\"] = cat_df[tc].Histo2D((\"{name}__{cat}__test2_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \";nTrueInt;npvsGood\", 150, 0, 150, 150, 0, 150), \"FTAMuon{lpf}_pfRelIso03_all\", \"METFixEE2017_pt\", wgtVar)\n",
    "                    #histos2D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"npvsGood_vs_nTrueInttest\"] = cat_df[tc].Histo2D((\"{name}__{cat}__npvsGood_vs_nTrueInttest_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \";nTrueInt;npvsGood\", 150, 0, 150, 150, 0, 150), \"FTAElectron{lpf}_pfRelIso03_all\", \"MET_pt_flat\", wgtVar)\n",
    "                    histos2D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"npvsGood_vs_nTrueInt\"] = cat_df[tc].Histo2D((\"{name}__{cat}__npvsGood_vs_nTrueInt_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \";nTrueInt;npvsGood\", 150, 0, 150, 150, 0, 150), \"Pileup_nTrueInt\", \"PV_npvsGood\", wgtVar)\n",
    "                    histos2D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"npvsGood_vs_nPU\"] = cat_df[tc].Histo2D((\"{name}__{cat}__npvsGood_vs_nPU_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \";nPU;npvsGood\", 150, 0, 150, 150, 0, 150), \"Pileup_nPU\", \"PV_npvsGood\", wgtVar)\n",
    "                    histos2D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"npvs_vs_nTrueInt\"] = cat_df[tc].Histo2D((\"{name}__{cat}__npvs_vs_nTrueInt_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \";nTrueInt;npvs\", 150, 0, 150, 150, 0, 150), \"Pileup_nTrueInt\", \"PV_npvs\", wgtVar)\n",
    "                    histos2D_dict[\"sysVar{spf}\".format(spf=syspostfix)][tc][\"npvs_vs_nPU\"] = cat_df[tc].Histo2D((\"{name}__{cat}__npvs_vs_nPU_{spf}\".format(name=sampleName, cat=tcn, spf=syspostfix), \";nPU;npvs\", 150, 0, 150, 150, 0, 150), \"Pileup_nPU\", \"PV_npvs\", wgtVar)\n",
    "      \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ROOT.gInterpreter.Declare(\"std::map<std::string, std::vector<TH2Lookup*>> LUM;\")\n",
    "#ROOT.LUM[\"no\"].push_back(ROOT.TH2Lookup(\"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/BTaggingYields.root\"))\n",
    "#ROOT.LUM[\"no\"].getEventYieldRatio(\"Aggregate\", \"_deepcsv\", 5, 695.0)\n",
    "#ROOT.LUM[\"bleh\"].size()\n",
    "#ROOT.LUM[\"no\"].push_back(ROOT.TH2Lookup(\"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/BTaggingYields.root\"))\n",
    "#ROOT.LUM[\"no\"].size()\n",
    "#ROOT.LUM[\"no\"][0].getEventYieldRatio(\"Aggregate\", \"_deepcsv\", 5, 695.0)\n",
    "#ROOT.LUM[\"yes\"].size()\n",
    "#str(type(ROOT.LUM))\n",
    "#getattr(ROOT, \"LUM\")[\"no\"].size()\n",
    "#if str(type(getattr(ROOT, \"LUM\"))) == \"<class 'ROOT.map<string,vector<TH2Lookup*> >'>\":\n",
    "#    print(\"Okay!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def lepMatchingEfficiency(input_df, wgtVar=\"\")\n",
    "\n",
    "def jetMatchingEfficiency(input_df, max_eta = 2.5, min_pt = 30.0, wgtVar=\"wgt_SUMW_PU_L1PF\", stats_dict=None,\n",
    "                         isData=True):\n",
    "    if isData == True:\n",
    "        pass\n",
    "    else:\n",
    "        theCats = collections.OrderedDict()\n",
    "        #Subtract 2 for the GenJets which are actually leptons\n",
    "        theCats[\"nGenJet2\"] = \"jetmatch_nGenJet == 4\"\n",
    "        theCats[\"nGenJet3\"] = \"jetmatch_nGenJet == 5\"\n",
    "        theCats[\"nGenJet4\"] = \"jetmatch_nGenJet == 6\"\n",
    "        theCats[\"nGenJet5\"] = \"jetmatch_nGenJet == 7\"\n",
    "        theCats[\"nGenJet6\"] = \"jetmatch_nGenJet == 8\"\n",
    "        theCats[\"nGenJet7\"] = \"jetmatch_nGenJet == 9\"\n",
    "        theCats[\"nGenJet8\"] = \"jetmatch_nGenJet == 10\"\n",
    "        theCats[\"nGenJet9\"] = \"jetmatch_nGenJet == 11\"\n",
    "        theCats[\"nGenJet10+\"] = \"jetmatch_nGenJet >= 12\"\n",
    "        #define genjets as needed for this study\n",
    "        input_df_defined = input_df.Define(\"jetmatch_nGenJet\", \"GenJet_pt[GenJet_pt >= {}  && abs(GenJet_eta) <= {}].size()\".format(min_pt, max_eta))\n",
    "        cat_df = collections.OrderedDict()\n",
    "        for ck, cs in theCats.items():\n",
    "            cat_df[ck] = input_df_defined.Filter(cs, \"Jet Matching Efficiency \" + cs)\n",
    "            stats_dict[ck] = {}\n",
    "            stats_dict[ck][\"nJet\"] = cat_df[ck].Stats(\"nGJet\", wgtVar)\n",
    "            stats_dict[ck][\"nJet_genMatched\"] = cat_df[ck].Stats(\"nGJet_genMatched\", wgtVar)\n",
    "            stats_dict[ck][\"nJet_puIdLoose\"] = cat_df[ck].Stats(\"nGJet_puIdLoose\", wgtVar)\n",
    "            stats_dict[ck][\"nJet_genMatched_puIdLoose\"] = cat_df[ck].Stats(\"nGJet_genMatched_puIdLoose\", wgtVar)\n",
    "\n",
    "def fillHLTMeans(input_df, wgtVar=\"wgt_SUMW_PU_L1PF\", stats_dict=None, debugInfo=False):\n",
    "    theCats = collections.OrderedDict()\n",
    "    theCats[\"Inclusive\"] = \"nGJet >= 4\"\n",
    "    theCats[\"nJet4to5\"] = \"nGJet == 4 || nGJet == 5\"\n",
    "    theCats[\"nJet6+\"] = \"nGJet >= 6\"\n",
    "    \n",
    "    branches = [branch for branch in input_df.GetColumnNames() if \"HLT_\" in branch and \"Ele\" not in branch\n",
    "                and \"Mu\" not in branch and \"Tau\" not in branch]\n",
    "                #and (\"PF\" in branch or \"HT\" in branch or \"MET\" in branch)]\n",
    "    #print(branches)\n",
    "    \n",
    "    input_df_defined = input_df\n",
    "    branches_weighted = []\n",
    "    for branch in branches:\n",
    "        branches_weighted.append(\"{}_weighted\".format(branch))\n",
    "        input_df_defined = input_df_defined.Define(\"{}_weighted\".format(branch), \n",
    "                                                   \"{} == true ? {} : 0\".format(branch, wgtVar))\n",
    "                \n",
    "    cat_df = collections.OrderedDict()\n",
    "    for ck, cs in theCats.items():\n",
    "        cat_df[ck] = input_df_defined.Filter(cs, \"HLT Report \" + cs)\n",
    "    if stats_dict != None:\n",
    "        if \"unweighted\" not in stats_dict:\n",
    "            stats_dict[\"unweighted\"] = {}\n",
    "        if \"weighted\" not in stats_dict:\n",
    "            stats_dict[\"weighted\"] = {}\n",
    "        if \"weightedStats\" not in stats_dict:\n",
    "            stats_dict[\"weightedStats\"] = {}\n",
    "        if \"weightedStatsSMT\" not in stats_dict:\n",
    "            stats_dict[\"weightedStatsSMT\"] = {}\n",
    "        if \"counts\" not in stats_dict:\n",
    "            stats_dict[\"counts\"] = {}\n",
    "        for tc, cut in theCats.items():\n",
    "            if tc not in stats_dict[\"unweighted\"]: \n",
    "                stats_dict[\"unweighted\"][tc] = {}\n",
    "            if tc not in stats_dict[\"weighted\"]: \n",
    "                stats_dict[\"weighted\"][tc] = {}\n",
    "            if tc not in stats_dict[\"weightedStats\"]: \n",
    "                stats_dict[\"weightedStats\"][tc] = {}\n",
    "            if tc not in stats_dict[\"weightedStatsSMT\"]: \n",
    "                stats_dict[\"weightedStatsSMT\"][tc] = {}\n",
    "            if tc not in stats_dict[\"counts\"]: \n",
    "                stats_dict[\"counts\"][tc] = cat_df[tc].Count()\n",
    "            for branch in branches:\n",
    "                stats_dict[\"unweighted\"][tc][\"{}\".format(branch)] = cat_df[tc].Sum(\"{}\".format(branch)) #instead of mean\n",
    "                stats_dict[\"weightedStatsSMT\"][tc][\"{}\".format(branch)] = cat_df[tc].Stats(\"{}\".format(branch), wgtVar)\n",
    "            for branch in branches_weighted:\n",
    "                stats_dict[\"weighted\"][tc][\"{}\".format(branch)] = cat_df[tc].Sum(\"{}\".format(branch)) \n",
    "                stats_dict[\"weightedStats\"][tc][\"{}\".format(branch)] = cat_df[tc].Stats(\"{}\".format(branch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeHistosForCombine(hist_dict, directory, levels_of_iterest, dict_key=\"Mountains\", mode=\"RECREATE\"):\n",
    "    rootDict = {}\n",
    "    if not os.path.isdir(directory):\n",
    "        os.makedirs(directory)\n",
    "    for name, levels_dict in hist_dict.items():\n",
    "        for level, obj_dict in levels_dict.items():\n",
    "            if level not in levels_of_interest: continue\n",
    "            if not os.path.isdir(directory + \"/\" + level):\n",
    "                os.makedirs(directory + \"/\" + level)\n",
    "            for pre_obj_name, obj_val in obj_dict[dict_key].items():\n",
    "                for hname, hist in obj_val.items():\n",
    "                    dictKey = pre_obj_name + \"_\" + hname\n",
    "                    if dictKey not in rootDict:\n",
    "                        rootDict[dictKey] = ROOT.TFile.Open(\"{}.root\"\\\n",
    "                                     .format(directory + \"/\" + level + \"/\"+ dictKey), mode)\n",
    "                    rootDict[dictKey].cd()\n",
    "                    hptr = hist.GetPtr()\n",
    "                    oldname = hptr.GetName()\n",
    "                    hptr.SetName(\"{}\".format(name))\n",
    "                    hptr.Write()\n",
    "                    hptr.SetName(\"{}\".format(oldname)) #Avoid overwriting things by switching back, save from segfault\n",
    "                    #hptr.SetDirectory(0)\n",
    "    for f in rootDict.values():\n",
    "        f.Close()\n",
    "        \n",
    "def histoCombine(directory, outDirectory=\"{}/Combine\", globKey=\"*.root\", stripKey=\".root\", internalSeperator=\"*\",\n",
    "                systematicSeperator=\"__\", mode=\"RECREATE\"):\n",
    "    \"\"\"take list of files in <directory>, with optional <globKey>, and create individual root files containing\n",
    "    each sample (nominal and systematic variation) for each histogram category. Keys can be parsed with \n",
    "    <internalSeperator> (default '*') and <systematicSeperator> (default '__') such that file 'ttWH.root' with\n",
    "    'Mountains*nJet4*DeepCSV_jet1__JESup' will generate a file 'nJet4_DeepCSV_jet1.root' containing the systematic \n",
    "    variation histogram 'ttWH_JESup'\"\"\"\n",
    "    if \"{}/\" in outDirectory:\n",
    "        outDirectory = outDirectory.format(directory)\n",
    "    if not os.path.isdir(outDirectory):\n",
    "        print(\"Checking for (and if necessary, creating) directory {}\".format(outDirectory))\n",
    "        os.makedirs(outDirectory)\n",
    "    #Get the files\n",
    "    if 'glob' not in dir():\n",
    "        try:\n",
    "            import glob\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(\"Could not import the glob module in method histoCombine\")\n",
    "    files = glob.glob(\"{}/{}\".format(directory, globKey))\n",
    "    #deduce names from the filenames, with optional stripKey parameter that defaults to .root\n",
    "    names = [fname.split(\"/\")[-1].replace(stripKey, \"\") for fname in files]\n",
    "    fileDict = {}\n",
    "    keysDict = {}\n",
    "    nominalDict = {}\n",
    "    keySet = set([])\n",
    "    for name, fname in zip(names, files):\n",
    "        #inFiles\n",
    "        fileDict[name] = ROOT.TFile.Open(fname)\n",
    "        #hist names\n",
    "        keysDict[name] = [hist.GetName() for hist in fileDict[name].GetListOfKeys()]\n",
    "        #for creation of outFiles (group nominal + systematic variations!)\n",
    "        nominalDict[name] = [hist.split(\"{}\".format(systematicSeperator))[0] for hist in keysDict[name]]\n",
    "        keySet = keySet.union(set(nominalDict[name]))\n",
    "    #start parsing to generate outFile names\n",
    "    for outname_raw in keySet:\n",
    "        splt = outname_raw.split(\"{}\".format(internalSeperator))\n",
    "        n = len(splt)\n",
    "        if n == 1:\n",
    "            var = splt[0]\n",
    "        elif n == 2:\n",
    "            cat, var = splt\n",
    "        elif n == 3:\n",
    "            super_cat, cat, var = splt\n",
    "        #ignore super_cat names for now, create the name with the outDirectory and guard against doubled '/' character\n",
    "        outname = \"{}/{}_{}.root\".format(outDirectory, cat, var).replace(\"//\", \"/\")\n",
    "        oFile = ROOT.TFile.Open(outname, mode)\n",
    "        for name, hNameList_raw in keysDict.items():\n",
    "            hNameList = [hName for hName in hNameList_raw if outname_raw == hName.split(\"{}\".format(systematicSeperator))[0]]\n",
    "            for hName in hNameList:\n",
    "                hist = fileDict[name].Get(hName)\n",
    "                original_name = hist.GetName()\n",
    "                #format the new name by replacing the non-systematic portion with the sample's name\n",
    "                new_name = hName.replace(outname_raw, name)\n",
    "                hist.SetName(new_name)\n",
    "                hist.Write()\n",
    "                hist.SetName(original_name)\n",
    "        oFile.Close()\n",
    "        \n",
    "        \n",
    "        \n",
    "def writeHistos(hist_dict, directory, levels_of_iterest, samples_of_interest=\"All\", dict_keys=\"All\", mode=\"RECREATE\"):\n",
    "    rootDict = {}\n",
    "    if not os.path.isdir(directory):\n",
    "        os.makedirs(directory)\n",
    "    for name, levels_dict in hist_dict.items():\n",
    "        if samples_of_interest == \"All\": pass\n",
    "        elif name not in samples_of_interest: continue\n",
    "        for level, obj_dict in levels_dict.items():\n",
    "            if level not in levels_of_interest: continue\n",
    "            if not os.path.isdir(directory + \"/\" + level):\n",
    "                os.makedirs(directory + \"/\" + level)\n",
    "            rootDict[name] = ROOT.TFile.Open(\"{}.root\"\\\n",
    "                                         .format(directory + \"/\" + level + \"/\"+ name), mode)\n",
    "            for dict_key in obj_dict.keys():\n",
    "                if dict_keys == \"All\": pass\n",
    "                elif dict_key not in dict_keys: continue\n",
    "\n",
    "                for pre_obj_name, obj_val in obj_dict[dict_key].items():\n",
    "                    if type(obj_val) == dict:\n",
    "                        for hname, hist in obj_val.items():\n",
    "                            #help(hist)\n",
    "                            #dictKey = pre_obj_name + \"$\" + hname\n",
    "                            #if dictKey not in rootDict:\n",
    "                            #rootDict[dictKey].cd()\n",
    "                            hptr = hist.GetPtr()\n",
    "                            oldname = hptr.GetName()\n",
    "                            #hptr.SetName(\"{}\".format(dict_key + \"*\" + pre_obj_name + \"*\" + hname))\n",
    "                            hptr.Write()\n",
    "                            #hptr.SetName(\"{}\".format(oldname)) #Avoid overwriting things by switching back, save from segfault\n",
    "                    elif \"ROOT.TH\" in str(type(obj_val)):\n",
    "                        hptr = obj_val.GetPtr()\n",
    "                        oldname = hptr.GetName()\n",
    "                        #hptr.SetName(\"{}\".format(dict_key + \"*\" + pre_obj_name))\n",
    "                        hptr.Write()\n",
    "                        #hptr.SetName(\"{}\".format(oldname)) #Avoid overwriting things by switching back, save from segfault\n",
    "            print(\"Wrote histogram file for {} - {}\".format(name, directory + \"/\" + level + \"/\"+ name))\n",
    "    for f in rootDict.values():\n",
    "        f.Close()\n",
    "        \n",
    "def makeHLTReport(stats_dict, directory, levels_of_iterest=\"All\"):\n",
    "    if not os.path.isdir(directory):\n",
    "        os.makedirs(directory)\n",
    "    #name, level, weighted/unweighted, category, (count?)\n",
    "    path_dict = collections.OrderedDict()\n",
    "    count_dict = collections.OrderedDict()\n",
    "    all_names = []\n",
    "    for name, name_dict in stats_dict.items():\n",
    "        all_names.append(name)\n",
    "        for level, level_dict in name_dict.items():\n",
    "            if level not in path_dict.keys():\n",
    "                path_dict[level] = collections.OrderedDict()\n",
    "            if level not in count_dict.keys():\n",
    "                count_dict[level] = collections.OrderedDict()\n",
    "            if levels_of_interest is not \"All\" and level not in levels_of_interest: continue\n",
    "            for stat_category, stat_category_dict in level_dict.items():\n",
    "                if stat_category is \"counts\":\n",
    "                    for category, counter in stat_category_dict.items():\n",
    "                        count_dict[level][category] = str(counter.GetValue())\n",
    "                elif stat_category in [\"weighted\", \"unweighted\"]:\n",
    "                    if stat_category not in path_dict[level].keys():\n",
    "                        path_dict[level][stat_category] = collections.OrderedDict()\n",
    "                    #pprint.pprint(stat_category_dict)\n",
    "                    for category, category_dict in stat_category_dict.items():\n",
    "                        if category not in path_dict[level][stat_category].keys():\n",
    "                            path_dict[level][stat_category][category] = collections.OrderedDict()\n",
    "                        for path, count in category_dict.items():\n",
    "                            if path not in path_dict[level][stat_category][category].keys():\n",
    "                                #path_dict[level][stat_category][category][path] = collections.OrderedDict()\n",
    "                                path_dict[level][stat_category][category][path] = {}\n",
    "                            path_dict[level][stat_category][category][path][name] = str(count.GetValue())\n",
    "                elif stat_category in [\"weightedStats\", \"weightedStatsSMT\"]:\n",
    "                    if stat_category not in path_dict[level].keys():\n",
    "                        path_dict[level][stat_category] = collections.OrderedDict()\n",
    "                    #pprint.pprint(stat_category_dict)\n",
    "                    for category, category_dict in stat_category_dict.items():\n",
    "                        if category not in path_dict[level][stat_category].keys():\n",
    "                            path_dict[level][stat_category][category] = collections.OrderedDict()\n",
    "                        for path, count in category_dict.items():\n",
    "                            if path not in path_dict[level][stat_category][category].keys():\n",
    "                                #path_dict[level][stat_category][category][path] = collections.OrderedDict()\n",
    "                                path_dict[level][stat_category][category][path] = {}\n",
    "                            path_dict[level][stat_category][category][path][name] = str(count.GetMean() * count.GetW())\n",
    "                    \n",
    "                        \n",
    "    for level, level_dict in path_dict.items():\n",
    "        with open(\"{}/{}_HLTReport.csv\".format(directory, level), \"w\") as f:\n",
    "            for stat_category, stat_category_dict in level_dict.items():\n",
    "                f.write(\"====={}=====\\n\".format(stat_category))\n",
    "                for category, category_dict in stat_category_dict.items():\n",
    "                    f.write(\"=========={}==========\\n\".format(category))\n",
    "                    wroteKey = False\n",
    "                    for path, path_values in category_dict.items():\n",
    "                        #pad values in the dictionary, this depends on it NOT being an OrderedDict at this level...\n",
    "                        for n in all_names:\n",
    "                            if n not in path_values: path_values[n] = \"-0.000000000000000001\"\n",
    "                    for path, path_values in sorted(category_dict.items(), key=lambda k: k[0]):\n",
    "                        if wroteKey is False: \n",
    "                            line = \"HLT Path,\" + \",\".join(path_values.keys()) + \"\\n\"\n",
    "                            f.write(line)\n",
    "                            wroteKey = True\n",
    "                        line = path + \",\" + \",\".join(path_values.values()) + \"\\n\"\n",
    "                        f.write(line)\n",
    "            \n",
    "                    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "##################################################\n",
    "### CHOOSE SAMPLE DICT AND CHANNEL TO ANALYZE ####\n",
    "##################################################\n",
    "##################################################\n",
    "\n",
    "#Focus on limited set of events at a time\n",
    "#levels_of_interest = set([\"ElMu_selection\"])\n",
    "#levels_of_interest = set([\"MuMu_selection\",])\n",
    "#levels_of_interest = set([\"ElEl_selection\",])\n",
    "#levels_of_interest = set([\"selection\", \"ElMu_selection\", \"ElEl_selection\", \"MuMu_selection\", \"Mu_selection\", \"El_selection\"])\n",
    "#levels_of_interest = set([\"baseline\", \"MuMu_baseline\", \"ElEl_baseline\", \"selection\", \"MuMu_selection\", \"ElMu_selection\"])\n",
    "#levels_of_interest = set([\"baseline\", \"MuMu_selection\", \"ElMu_selection\"])\n",
    "\n",
    "#Choose the sample dictionary to run\n",
    "#theSampleDict = ttbooker #needs modification to make work...\n",
    "#theSampleDict = ttttbooker\n",
    "#theSampleDict = microbooker #tttt, ttbar-DL unfiltered, DY, one single top sample\n",
    "#theSampleDict = minibooker #tttt, all ttbar, both single top, DY\n",
    "#theSampleDict = booker #All\n",
    "#theSampleDict = bookerV2 #All with reprocessing (WIP: Other data streams, ttVJets, Filtered samples!)\n",
    "#theSampleDict = tt_data_V2\n",
    "#theSampleDict = pyrdfbooker\n",
    "\n",
    "#Where to write histograms\n",
    "analysisDir = \"hists20200414\"\n",
    "\n",
    "#trigger verbose output for all the define and histo functions\n",
    "beVerbose = True\n",
    "\n",
    "all_samples = [\"ElMu\", \"MuMu\", \"ElEl\", \"tttt\", \"ST_tW\", \"ST_tbarW\", \"tt_DL\", \"tt_DL-GF\", \n",
    "                 \"tt_SL\", \"tt_SL-GF\", \"ttH\", \"ttWJets\", \"ttZJets\", \"ttWH\", \"ttWW\", \"ttWZ\", \n",
    "                 \"ttZZ\", \"ttZH\", \"ttHH\", \"tttW\", \"tttJ\", \"DYJets_DL\",]\n",
    "#Mask samples to divy up work\n",
    "#valid_samples = [\"ElMu\", \"MuMu\", \"ElEl\", ]\n",
    "#valid_samples = [\"tttt\", \"ST_tW\", \"ST_tbarW\"]\n",
    "#valid_samples = [\"tt_DL\",]\n",
    "#valid_samples = [\"tt_DL-GF\",]\n",
    "#valid_samples = [\"tt_SL\", \"tt_SL-GF\",]\n",
    "#valid_samples = [\"ttH\", \"ttWJets\", \"ttZJets\"]\n",
    "#valid_samples = [\"ttWH\", \"ttWW\", \"ttWZ\", \"ttZZ\", \"ttZH\", \"ttHH\", \"tttW\", \"tttJ\"]\n",
    "#valid_samples = [\"DYJets_DL\",]\n",
    "valid_samples = [\"ElMu\", \"tttt\", \"ST_tW\", \"ST_tbarW\"]\n",
    "\n",
    "\n",
    "#Decide on things to do\n",
    "doHLTMeans = False\n",
    "doHistos = False\n",
    "doBtaggingEfficiencies = False\n",
    "doBtaggingYields = True\n",
    "doJetEfficiency = False\n",
    "\n",
    "if doBtaggingYields:\n",
    "    print(\"Loading all samples for calculating BtaggingYields\")\n",
    "    valid_samples = all_samples\n",
    "\n",
    "#Use skimmed channels flag\n",
    "useSkimmed = True\n",
    "chooseChannel = \"ElMu\"\n",
    "#chooseChannel = \"MuMu\"\n",
    "#chooseChannel = \"ElEl\"\n",
    "#chooseChannel = \"Mu\"\n",
    "#chooseChannel = \"El\"\n",
    "#chooseChannel = \"test\"\n",
    "source_level = \"LJMLogic\"\n",
    "#source_level = \"LJMLogic/ElMu_selection\"\n",
    "#source_level = \"LJMLogic/MuMu_selection\"\n",
    "#source_level = \"LJMLogic/ElEl_selection\"\n",
    "if chooseChannel == \"ElMu\":\n",
    "    levels_of_interest = set([\"ElMu_selection\",])\n",
    "    theSampleDict = bookerV2_ElMu.copy()\n",
    "    theSampleDict.update(bookerV2_MC)\n",
    "    if useSkimmed == True:\n",
    "        source_level = \"LJMLogic/ElMu_selection\"\n",
    "elif chooseChannel == \"MuMu\":\n",
    "    levels_of_interest = set([\"MuMu_selection\",])\n",
    "    theSampleDict = bookerV2_MuMu.copy()\n",
    "    theSampleDict.update(bookerV2_MC)\n",
    "    if useSkimmed == True:\n",
    "        source_level = \"LJMLogic/MuMu_selection\"\n",
    "elif chooseChannel == \"ElEl\":    \n",
    "    levels_of_interest = set([\"ElEl_selection\",])\n",
    "    theSampleDict = bookerV2_ElEl.copy()\n",
    "    theSampleDict.update(bookerV2_MC)\n",
    "    if useSkimmed == True:\n",
    "        source_level = \"LJMLogic/ElEl_selection\"\n",
    "elif chooseChannel == \"Mu\":    \n",
    "    levels_of_interest = set([\"Mu_selection\",])\n",
    "    theSampleDict = bookerV2_Mu.copy()\n",
    "    theSampleDict.update(bookerV2_MC)\n",
    "    if useSkimmed == True:\n",
    "        print(\"No skimmed samples prepared for this selection level, please advise\")\n",
    "elif chooseChannel == \"El\":    \n",
    "    levels_of_interest = set([\"El_selection\",])\n",
    "    theSampleDict = bookerV2_El.copy()\n",
    "    theSampleDict.update(bookerV2_MC)\n",
    "    if useSkimmed == True:\n",
    "        print(\"No skimmed samples prepared for this selection level, please advise\")\n",
    "elif chooseChannel == \"test\":\n",
    "    levels_of_interest = set([\"ElMu_selection\", \"MuMu_selection\", \"ElEl_selection\"])\n",
    "    theSampleDict = microbookerV2.copy()\n",
    "    if useSkimmed == True:\n",
    "        print(\"No skimmed samples prepared for this selection level, please advise\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Choose the weight variation\n",
    "#theWeight = \"wgt_SUMW\"\n",
    "#theWeight = \"wgt_SUMW_PU\"\n",
    "#theWeight = \"wgt_SUMW_LSF\"\n",
    "#theWeight = \"wgt_SUMW_L1PF\"\n",
    "#theWeight = \"wgt_SUMW_PU_LSF\"\n",
    "theWeight = \"wgt_SUMW_PU_LSF_L1PF\"\n",
    "#theWeight = \"wgt_SUMW_LSF_L1PF\"\n",
    "#theWeight = \"wgt_NUMW_LSF_L1PF\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating selection and baseline bits\")\n",
    "b = {}\n",
    "b[\"ElMu_baseline\"] = \"(ESV_TriggerAndLeptonLogic_baseline & {0}) > 0\".format(Chan[\"ElMu_baseline\"])\n",
    "b[\"MuMu_baseline\"] = \"(ESV_TriggerAndLeptonLogic_baseline & {0}) == 0 && (ESV_TriggerAndLeptonLogic_baseline & {1}) > 0\".format(Chan[\"ElMu_baseline\"], \n",
    "                                                                                                                                Chan[\"MuMu_baseline\"])\n",
    "b[\"ElEl_baseline\"] = \"(ESV_TriggerAndLeptonLogic_baseline & {0}) == 0 && (ESV_TriggerAndLeptonLogic_baseline & {1}) > 0\".format(Chan[\"ElMu_baseline\"] + Chan[\"MuMu_baseline\"], \n",
    "                                                                                                                                Chan[\"ElEl_baseline\"])\n",
    "b[\"Mu_baseline\"] = \"(ESV_TriggerAndLeptonLogic_baseline & {0}) == 0 && (ESV_TriggerAndLeptonLogic_baseline & {1}) > 0\".format(Chan[\"ElMu_baseline\"] + Chan[\"MuMu_baseline\"] + Chan[\"ElEl_baseline\"], Chan[\"Mu_baseline\"])\n",
    "b[\"El_baseline\"] = \"(ESV_TriggerAndLeptonLogic_baseline & {0}) == 0 && (ESV_TriggerAndLeptonLogic_baseline & {1}) > 0\".format(Chan[\"ElMu_baseline\"] + Chan[\"MuMu_baseline\"] + \n",
    "                                                                                                                    Chan[\"ElEl_baseline\"] + Chan[\"Mu_baseline\"], Chan[\"El_baseline\"])\n",
    "b[\"selection\"] = \"ESV_TriggerAndLeptonLogic_selection > 0\"\n",
    "b[\"ElMu_selection\"] = \"(ESV_TriggerAndLeptonLogic_selection & {0}) > 0\".format(Chan[\"ElMu_selection\"])\n",
    "b[\"MuMu_selection\"] = \"(ESV_TriggerAndLeptonLogic_selection & {0}) == 0 && (ESV_TriggerAndLeptonLogic_selection & {1}) > 0\".format(Chan[\"ElMu_selection\"], Chan[\"MuMu_selection\"])\n",
    "b[\"ElEl_selection\"] = \"(ESV_TriggerAndLeptonLogic_selection & {0}) == 0 && (ESV_TriggerAndLeptonLogic_selection & {1}) > 0\".format(Chan[\"ElMu_selection\"] + Chan[\"MuMu_selection\"], Chan[\"ElEl_selection\"])\n",
    "b[\"Mu_selection\"] = \"(ESV_TriggerAndLeptonLogic_selection & {0}) == 0 && (ESV_TriggerAndLeptonLogic_selection & {1}) > 0\".format(Chan[\"ElMu_selection\"] + Chan[\"MuMu_selection\"] + Chan[\"ElEl_selection\"], Chan[\"Mu_selection\"])\n",
    "b[\"El_selection\"] = \"(ESV_TriggerAndLeptonLogic_selection & {0}) == 0 && (ESV_TriggerAndLeptonLogic_selection & {1}) > 0\".format(Chan[\"ElMu_selection\"] + Chan[\"MuMu_selection\"] + Chan[\"ElEl_selection\"]\n",
    "                                                                            + Chan[\"Mu_selection\"], Chan[\"El_selection\"]) \n",
    "#b[\"ESV_JetMETLogic_baseline\"] = \"(ESV_JetMETLogic_baseline & {0}) >= {0}\".format(0b00001100011111111111) #This enables the MET pt cut (11) and nJet (15) and HT (16) cuts from PostProcessor\n",
    "b[\"ESV_JetMETLogic_baseline\"] = \"(ESV_JetMETLogic_baseline & {0}) >= {0}\".format(0b00000000001111111111) #Only do the PV and MET filters, nothing else\n",
    "#b[\"ESV_JetMETLogic_selection\"] = \"(ESV_JetMETLogic_baseline & {0}) >= {0}\".format(0b00001100011111111111) #FIXME, this isn't right!\n",
    "b[\"ESV_JetMETLogic_selection\"] = \"(ESV_JetMETLogic_selection & {0}) >= {0}\".format(0b00001100011111111111)#This enables the MET pt cut (11) and nJet (15) and HT (16) cuts from PostProcessor\n",
    "b[\"ESV_JetMETLogic_selection\"] = \"(ESV_JetMETLogic_selection & {0}) >= {0}\".format(0b00000000001111111111)\n",
    "#b[\"ESV_JetMETLogic_default\"] = \"(ESV_JetMETLogic_baseline & {}) > 0\".format(0b11111111111111111111)\n",
    "#print(b[\"ESV_JetMETLogic_selection\"])\n",
    "\n",
    "stitchDict = {'2016': {'SL': {'nGenJets': None,\n",
    "                                           'nGenLeps': None,\n",
    "                                           'GenHT': None},\n",
    "                                    'DL': {'nGenJets': None,\n",
    "                                           'nGenLeps': None,\n",
    "                                           'GenHT': None}\n",
    "                                },\n",
    "                           '2017': {'SL': {'nGenJets': 9,\n",
    "                                           'nGenLeps': 1,\n",
    "                                           'GenHT': 500},\n",
    "                                    'DL': {'nGenJets': 7,\n",
    "                                           'nGenLeps': 2,\n",
    "                                           'GenHT': 500}\n",
    "                                },\n",
    "                           '2018': {'SL': {'nGenJets': 9,\n",
    "                                           'nGenLeps': 1,\n",
    "                                           'GenHT': 500},\n",
    "                                    'DL': {'nGenJets': 7,\n",
    "                                           'nGenLeps': 2,\n",
    "                                           'GenHT': 500}\n",
    "                                }\n",
    "                       }\n",
    "\n",
    "\n",
    "filtered = {}\n",
    "base = {}\n",
    "reports = {}\n",
    "#theValidSet = set([\"tt_SL\", \"tt_SL-GF\", \"tt_DL-GF\"])\n",
    "for name, vals in theSampleDict.items():\n",
    "    #print(\"Skipping all samples except {}\".format(theValidSet))\n",
    "    if name not in valid_samples: continue\n",
    "    print(\"Initializing RDataFrame - {} - {}\".format(name, vals[\"source\"][source_level]))\n",
    "    filtered[name] = {}\n",
    "    base[name] = RDF(\"Events\", vals[\"source\"][source_level])\n",
    "    reports[name] = base[name].Report()\n",
    "    for lvl in levels_of_interest:\n",
    "        if \"baseline\" in lvl:\n",
    "            JMLOG = \"ESV_JetMETLogic_baseline\"\n",
    "        elif \"selection\" in lvl:\n",
    "            JMLOG = \"ESV_JetMETLogic_selection\"\n",
    "        else:\n",
    "            JMLOG = \"ESV_JetMETLogic_default\"\n",
    "            \n",
    "        if lvl == \"baseline\":\n",
    "            filtered[name][lvl] = base[name]#.Filter(b[JMLOG], JMLOG)#.Cache()\n",
    "        else:\n",
    "            filtered[name][lvl] = base[name].Filter(b[lvl], lvl)#.Filter(b[JMLOG], JMLOG)#.Cache()\n",
    "        #Cache() seemingly has an issue with the depth/breadth of full NanoAOD file. Perhaps one with fewer branches would work\n",
    "        #filtered[name][lvl] = filtered[name][lvl].Cache()\n",
    "        if vals.get(\"stitch\") != None:\n",
    "            stitch_def = collections.OrderedDict()\n",
    "            stitch_def[\"stitch_jet_mask\"] = \"GenJet_pt > 30\"\n",
    "            stitch_def[\"stitch_HT_mask\"] = \"GenJet_pt > 30 && abs(GenJet_eta) < 2.4\"\n",
    "            stitch_def[\"stitch_lep_mask\"] = \"abs(LHEPart_pdgId) == 15 || abs(LHEPart_pdgId) == 13 || abs(LHEPart_pdgId) == 11\"\n",
    "            stitch_def[\"stitch_nGenLep\"] = \"LHEPart_pdgId[stitch_lep_mask].size()\"\n",
    "            stitch_def[\"stitch_nGenJet\"] = \"GenJet_pt[stitch_jet_mask].size()\"\n",
    "            stitch_def[\"stitch_GenHT\"] = \"Sum(GenJet_pt[stitch_HT_mask])\"\n",
    "            \n",
    "            stdict = stitchDict[vals.get(\"era\")][vals.get(\"stitch\").get(\"channel\")]\n",
    "            stitch_cut = \"stitch_nGenLep == {} && stitch_nGenJet >= {} && stitch_GenHT >= {}\"\\\n",
    "                .format(stdict.get(\"nGenLeps\"), stdict.get(\"nGenJets\"), stdict.get(\"GenHT\"))\n",
    "            if vals.get(\"stitch\").get(\"source\") == \"Nominal\":\n",
    "                stitch_cut = \"!({})\".format(stitch_cut)\n",
    "            elif vals.get(\"stitch\").get(\"source\") == \"Filtered\":\n",
    "                pass\n",
    "            else:\n",
    "                print(\"Invalid stitching source type\")\n",
    "                sys.exit(1)\n",
    "            print(stitch_cut)\n",
    "            for k, v in stitch_def.items():\n",
    "                filtered[name][lvl] = filtered[name][lvl].Define(\"{}\".format(k), \"{}\".format(v))\n",
    "            filtered[name][lvl] = filtered[name][lvl].Filter(stitch_cut, \"nJet_GenHT_Filter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "samples = {}\n",
    "counts = {}\n",
    "histos1D = {}\n",
    "histos1D_PU = {}\n",
    "histos2D = {}\n",
    "histosNS = {} #unstacked histograms\n",
    "the_df = {}\n",
    "stats = {} #Stats for HLT branches\n",
    "effic = {} #Stats for jet matching efficiencies\n",
    "btagging = {} #For btagging efficiencies\n",
    "print(\"Starting loop for booking\")\n",
    "for name, vals in theSampleDict.items():\n",
    "    if name not in valid_samples: continue\n",
    "    #if name not in [\"tttt\", \"ElMu_F\"]: continue\n",
    "    print(\"Booking - {}\".format(name))\n",
    "    counts[name] = {}\n",
    "    histos1D[name] = {}\n",
    "    histos1D_PU[name] = {}\n",
    "    histos2D[name] = {}\n",
    "    histosNS[name] = {}\n",
    "    the_df[name] = {}\n",
    "    stats[name] = {}\n",
    "    effic[name] = {}\n",
    "    btagging[name] = {}\n",
    "    #counts[name][\"baseline\"] = filtered[name].Count() #Unnecessary with baseline in levels of interest?\n",
    "    for lvl in levels_of_interest:\n",
    "        the_df[name][lvl] = METXYCorr(filtered[name][lvl],\n",
    "                                      run_branch = \"run\",\n",
    "                                      era=\"2017\", \n",
    "                                      isData=vals[\"isData\"],\n",
    "                                      verbose=beVerbose,\n",
    "                                      )\n",
    "        the_df[name][lvl] = defineLeptons(the_df[name][lvl], \n",
    "                                          input_lvl_filter=lvl,\n",
    "                                          isData=vals[\"isData\"], \n",
    "                                          useBackupChannel=False,\n",
    "                                          verbose=beVerbose,\n",
    "                                         )\n",
    "        #Use the cutPV and METFilters function to do cutflow on these requirements...\n",
    "        the_df[name][lvl] = cutPVandMETFilters(the_df[name][lvl], lvl, isData=vals[\"isData\"])\n",
    "        if vals[\"isData\"] == False:\n",
    "            the_df[name][lvl] = defineInitWeights(the_df[name][lvl],\n",
    "                                                  crossSection=vals[\"crossSection\"], \n",
    "                                                  sumWeights=vals[\"sumWeights\"], \n",
    "                                                  lumi=lumi[era],\n",
    "                                                  nEvents=vals[\"nEvents\"], \n",
    "                                                  nEventsPositive=vals[\"nEventsPositive\"], \n",
    "                                                  nEventsNegative=vals[\"nEventsNegative\"], \n",
    "                                                  isData=vals[\"isData\"], \n",
    "                                                  verbose=beVerbose,\n",
    "                                                 )\n",
    "        else:\n",
    "            the_df[name][lvl] = defineInitWeights(the_df[name][lvl],\n",
    "                                                  isData=True,\n",
    "                                                  verbose=beVerbose,\n",
    "                                                 )\n",
    "        the_df[name][lvl] = defineJets(the_df[name][lvl],\n",
    "                                       era=\"2017\",\n",
    "                                       useDeepCSV=True,\n",
    "                                       isData=vals[\"isData\"],\n",
    "                                       verbose=beVerbose,\n",
    "                                      )\n",
    "        print(\"Need to make cuts on HT, MET, InvariantMass, ETC.\")\n",
    "        #the_df[name][lvl] = the_df[name][lvl].Filter(\"nGJet > 2\", \"nJet > 2\")\n",
    "        #the_df[name][lvl] = the_df[name][lvl].Filter(\"nGJet > 3\", \"nJet > 3\")\n",
    "        #the_df[name][lvl] = the_df[name][lvl].Filter(\"nGJet_MediumDeepCSV > 1\")\n",
    "        #the_df[name][lvl] = the_df[name][lvl].Filter(\"nGJet_MediumDeepJet > 1\", \"nMedDeepJet > 1\")\n",
    "        #the_df[name][lvl] = the_df[name][lvl].Filter(\"METFixEE2017_pt > 40\", \"MET > 40\")\n",
    "        #the_df[name][lvl] = the_df[name][lvl].Filter(\"METFixEE2017_pt > 0\", \"MET > 50\")\n",
    "        #the_df[name][lvl] = the_df[name][lvl].Filter(\"GJet_HT > 450\", \"HT > 450\")\n",
    "        #the_df[name][lvl] = the_df[name][lvl].Filter(\"GJet_HT > 500\", \"HT > 500\")\n",
    "        counts[name][lvl] = the_df[name][lvl].Count()\n",
    "        histos1D[name][lvl] = {}\n",
    "        histos1D_PU[name][lvl] = {}\n",
    "        histosNS[name][lvl] = {}\n",
    "        histos2D[name][lvl] = {}\n",
    "        stats[name][lvl] = {}\n",
    "        effic[name][lvl] = {}\n",
    "        btagging[name][lvl] = {}\n",
    "        #Define all the btag event weights or calculate yields based on defining the btag pre-event weight\n",
    "        #as well as nJet_variation, HT_variation if necessary (move to defineJets function later)\n",
    "        ##yb = the_df[name][lvl].GetDefinedColumnNames()\n",
    "        ##nb = the_df[name][lvl].GetColumnNames()\n",
    "        ##print(len(yb), len(nb))\n",
    "        ##bc = 0\n",
    "        ##print(the_df[name][lvl])\n",
    "        ##for branch in yb:\n",
    "        ##    if \"FTAMuon\" in branch or \"FTAElectron\" in branch or \"FTALepton\" in branch:\n",
    "        ##        continue\n",
    "        ##    btype = the_df[name][lvl].GetColumnType(branch)\n",
    "        ##    if \"bool\" in str(btype):\n",
    "        ##        continue\n",
    "        ##    if \"_doublet\" in branch:\n",
    "        ##        continue\n",
    "        ##    print(\"Testing branch {} of type {}\".format(branch, btype))\n",
    "        ##    h = the_df[name][lvl].Histo1D(branch)\n",
    "        ##    hv = h.GetValue()\n",
    "        ##    n = hv.GetBinContent(1)\n",
    "        ##    print(n)\n",
    "        #Calculate or load yields to produce the btag event weights\n",
    "        if doBtaggingYields == True:\n",
    "            loadTheYields = None\n",
    "            calculateTheYields = True\n",
    "        else:\n",
    "            loadTheYields = \"/eos/user/n/nmangane/SWAN_projects/LogicChainRDF/BTaggingYields.root\"\n",
    "            calculateTheYields = False\n",
    "        #Insert the yields or calculate them\n",
    "        the_df[name][lvl] = BtaggingYields(the_df[name][lvl], sampleName=name, isData = vals[\"isData\"], \n",
    "                                           histos_dict=btagging[name][lvl], loadYields=loadTheYields,\n",
    "                                           useAggregate=True, calculateYields=calculateTheYields,\n",
    "                                           HTBinWidth=10, HTMin=200, HTMax=3200,\n",
    "                                           nJetBinWidth=1, nJetMin=4, nJetMax=20,\n",
    "                                           verbose=beVerbose,\n",
    "                                          )\n",
    "        #Define the final weights/variations so long as we have btagging yields inserted...\n",
    "        if loadTheYields:\n",
    "            the_df[name][lvl] = defineFinalWeights(the_df[name][lvl],\n",
    "                                                isData = vals[\"isData\"],\n",
    "                                                verbose=beVerbose,\n",
    "                                               )\n",
    "        if doBtaggingEfficiencies == True:\n",
    "            BtaggingEfficiencies(the_df[name][lvl], sampleName=None, era=\"2017\", wgtVar=\"wgt_SUMW_PU_LSF_L1PF\", \n",
    "                           isData = vals[\"isData\"], histos_dict=btagging[name][lvl], \n",
    "                           doDeepCSV=True, doDeepJet=True, debugInfo=False)\n",
    "        if doJetEfficiency:\n",
    "            jetMatchingEfficiency(the_df[name][lvl], wgtVar=\"wgt_SUMW_PU_LSF_L1PF\", stats_dict=effic[name][lvl],\n",
    "                                  isData = vals[\"isData\"])\n",
    "        if doHLTMeans:\n",
    "            fillHLTMeans(the_df[name][lvl], wgtVar=\"wgt_SUMW_PU_L1PF\", stats_dict=stats[name][lvl])\n",
    "        cat_df = None\n",
    "        if doHistos:\n",
    "            cat_df = fillHistos(the_df[name][lvl], wgtVar=theWeight, isData = vals[\"isData\"],\n",
    "                   histos1D_dict=histos1D[name][lvl], histos2D_dict=histos2D[name][lvl], \n",
    "                   histosNS_dict=histosNS[name][lvl],\n",
    "                   doMuons=False, doElectrons=False, doLeptons=False, \n",
    "                   doJets=False, doWeights=False, doEventVars=False,\n",
    "                   makeMountains=True, useDeepCSV=True)\n",
    "        #print(cat_df)\n",
    "            \n",
    "        #Trigger the loop\n",
    "        start = time.clock()\n",
    "        processed = counts[name][lvl].GetValue()\n",
    "        finish =time.clock()\n",
    "        print(\"Took {}m {}s ({}s) to process {} events from sample {} in channel {}\"\\\n",
    "             .format((finish-start)//60, (finish-start)%60, (finish-start), processed, name, lvl))\n",
    "        #Write the output!\n",
    "        writeHistos(histos1D[name][lvl],\n",
    "                    analysisDir,\n",
    "                    [lvl],\n",
    "                    samples_of_interest=name,\n",
    "                    dict_keys=\"All\",\n",
    "                    mode=\"RECREATE\"\n",
    "                   )\n",
    "#        fillHistos(the_df[name][lvl], wgtVar=\"wgt_SUMW_PU_LSF\", histos1D_dict=histos1D[name][lvl], \n",
    "#                   histos2D_dict=histos2D[name][lvl], histosNS_dict=histosNS[name][lvl],\n",
    "#                   doMuons=False, doElectrons=False, doLeptons=True, \n",
    "#                   doJets=False, doWeights=True, doEventVars=False)\n",
    "#        fillHistos(the_df[name][lvl], wgtVar=\"wgt_SUMW_PU\", histos1D_dict=histos1D[name][lvl], \n",
    "#                   histos2D_dict=histos2D[name][lvl], histosNS_dict=histosNS[name][lvl],\n",
    "#                   doMuons=False, doElectrons=False, doLeptons=False, \n",
    "#                   doJets=False, doWeights=False, doEventVars=True)\n",
    "#        fillHistos(the_df[name][lvl], wgtVar=\"wgt_SUMW_PU\", histos1D_dict=histos1D_PU[name][lvl], \n",
    "#                   histos2D_dict=histos2D[name][lvl], histosNS_dict=histosNS[name][lvl],\n",
    "#                   doMuons=True, doElectrons=True, doLeptons=True, \n",
    "#                   doJets=True, doWeights=True, doEventVars=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Warning: if filtered[name][lvl] RDFs are not reset, then calling Define(*) on them will cause the error\"\\\n",
    "      \" with 'program state reset' due to multiple definitions for the same variable\")\n",
    "loopcounter = 0\n",
    "masterstart = time.clock()\n",
    "substart = {}\n",
    "subfinish = {}\n",
    "for name, cnt in counts.items():\n",
    "    #if name in [\"MuMu\", \"ElMu\", \"ElEl\"]: continue\n",
    "    substart[name] = time.clock()\n",
    "    loopcounter += 1\n",
    "    print(\"==========={}/{}\\n{}\".format(loopcounter, len(counts), name))\n",
    "    if \"baseline\" in cnt:\n",
    "        print(\"Baseline = \" + str(cnt[\"baseline\"].GetValue()))\n",
    "    else:\n",
    "        print(\"Baseline\")\n",
    "    if \"ElMu_baseline\" in cnt:\n",
    "        print(\"\\tElMu = {}\".format(cnt[\"ElMu_baseline\"].GetValue()),end='')\n",
    "    if \"MuMu_baseline\" in cnt:\n",
    "        print(\"\\tMuMu = {}\".format(cnt[\"MuMu_baseline\"].GetValue()),end='')\n",
    "    if \"ElEl_baseline\" in cnt:\n",
    "        print(\"\\tElEl = {}\".format(cnt[\"ElEl_baseline\"].GetValue()),end='')\n",
    "    if \"Mu_baseline\" in cnt:\n",
    "        print(\"\\tMu = {}\".format(cnt[\"Mu_baseline\"].GetValue()),end='')\n",
    "    if \"El_baseline\" in cnt:\n",
    "        print(\"\\tEl = {}\".format(cnt[\"El_baseline\"].GetValue()),end='')\n",
    "    print(\"\")\n",
    "    if \"ElMu_baseline\" in cnt and \"ElEl_baseline\" in cnt and \"MuMu_baseline\" in cnt\\\n",
    "            and \"Mu_baseline\" in cnt and \"El_baseline\" in cnt:\n",
    "        print(\"\\nTotal = {}\".format(cnt[\"ElMu_baseline\"].GetValue() + cnt[\"MuMu_baseline\"].GetValue() + cnt[\"ElEl_baseline\"].GetValue() + cnt[\"Mu_baseline\"].GetValue() + cnt[\"El_baseline\"].GetValue()))\n",
    "    if \"selection\" in cnt:\n",
    "        print(\"Selection = \" + str(cnt[\"selection\"].GetValue()))\n",
    "    else: \n",
    "        print(\"Selection\")\n",
    "    if \"ElMu_selection\" in cnt:\n",
    "        print(\"\\tElMu = {}\".format(cnt[\"ElMu_selection\"].GetValue()),end='')\n",
    "    if \"MuMu_selection\" in cnt:\n",
    "        print(\"\\tMuMu = {}\".format(cnt[\"MuMu_selection\"].GetValue()),end='')\n",
    "    if \"ElEl_selection\" in cnt:\n",
    "        print(\"\\tElEl = {}\".format(cnt[\"ElEl_selection\"].GetValue()),end='')\n",
    "    if \"Mu_selection\" in cnt:\n",
    "        print(\"\\tMu = {}\".format(cnt[\"Mu_selection\"].GetValue()),end='')\n",
    "    if \"El_selection\" in cnt:\n",
    "        print(\"\\tEl = {}\".format(cnt[\"El_selection\"].GetValue()),end='')\n",
    "    print(\"\")  \n",
    "    if \"ElMu_selection\" in cnt and \"ElEl_selection\" in cnt and \"MuMu_selection\" in cnt\\\n",
    "            and \"Mu_selection\" in cnt and \"El_selection\" in cnt:\n",
    "        print(\"\\nTotal = {}\".format(cnt[\"ElMu_selection\"].GetValue() + cnt[\"MuMu_selection\"].GetValue() + cnt[\"ElEl_selection\"].GetValue() + cnt[\"Mu_selection\"].GetValue() + cnt[\"El_selection\"].GetValue()))\n",
    "    subfinish[name] = time.time()\n",
    "    print(\"====> Took {}s to process sample {}\".format(subfinish[name] - substart[name], name))\n",
    "finish = time.clock()\n",
    "masterfinish = time.clock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, val in substart.items():\n",
    "    print(\"Took {}s to process sample {}\".format(subfinish[name] - substart[name], name))\n",
    "print()\n",
    "masterfinish = time.time() #clock gives cpu time, not accurate multi-core?\n",
    "print(\"Took {}m {}s to process in real-time\".format((masterfinish - masterstart)//60, (masterfinish - masterstart)%60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BtaggingYieldsAnalyzer(directory, outDirectory=\"{}/BtaggingYields\", globKey=\"*.root\", stripKey=\".root\",\n",
    "                           internalKeys = {\"Numerators\":[\"*sumW_before\"],\n",
    "                                           \"Denominator\": \"*sumW_after\",\n",
    "                                          },\n",
    "                           internalKeysReplacements = {\"BtaggingYields*btagPreWgt\": \"\",\n",
    "                                                       \"*sumW_before\": \"\",\n",
    "                                                       \"*sumW_after\": \"\",\n",
    "                                                      },\n",
    "                           sample_rebin={\"default\": {\"Y\": [4, 5, 6, 7, 8, 9, 20],\n",
    "                                                     \"X\": [500.0, 600, 700.0, 900.0, 1100.0, 3200.0],\n",
    "                                                    },\n",
    "                                         },\n",
    "                           overrides={\"Title\": \"$NAME BtaggingYield r=#frac{#Sigma#omega_{before}}{#Sigma#omega_{after}}($INTERNALS)\",\n",
    "                                      \"Xaxis\": \"H_{T} (GeV)\",\n",
    "                                      \"Yaxis\": \"nJet\",\n",
    "                                     },\n",
    "                           mode=\"RECREATE\", doNumpyValidation=False, forceDefaultRebin=False, debug=False, debug_dict={}):\n",
    "    \"\"\"For btagging yield ratio calculations using method 1d (shape corrections)\n",
    "    \n",
    "    take list of files in <directory>, with optional <globKey>, and create individual root files containing\n",
    "    each sample's btagging yield ratio histograms, based on derived categories. \n",
    "    \n",
    "    \n",
    "    #FIXME (below this line)\n",
    "    Keys can be parsed with \n",
    "    <name_format> (default 'BtaggingYield*btagPreSF_$VARIATION*$SUM') where $CAT, $JETTYPE, and $TAG are cycled through from \n",
    "    their respective input lists, format_dict{<categories>, <jettypes>, <tags>}. The format_dict{<untag>} option \n",
    "    specifies the denominator histogram (where $TAG is replaced by <untag>). A file 'ttWH.root' with \n",
    "    'Btagging*nJet4*bjets_DeepJet_T' will generate a file 'ttWH_BTagEff.root' containing the histogram \n",
    "    'nJet4_bjets_DeepJet_T'\"\"\"\n",
    "    \n",
    "    if doNumpyValidation == True:\n",
    "        #FORCE consistent binning and warn the user\n",
    "        print(\"Setting rebinning to the default\")\n",
    "        forceDefaultRebin = True\n",
    "        if 'numpy' not in dir() and 'np' not in dir():\n",
    "            try:\n",
    "                import numpy as np\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(\"Could not import the numpy (as np) module in method BtaggingYieldsAnalyzer\")\n",
    "    \n",
    "    if \"{}/\" in outDirectory:\n",
    "        outDirectory = outDirectory.format(directory)\n",
    "    print(\"Checking for (and if necessary, creating) directory {}\".format(outDirectory))\n",
    "    if not os.path.isdir(outDirectory):\n",
    "        os.makedirs(outDirectory)\n",
    "    #Get the files\n",
    "    if 'glob' not in dir():\n",
    "        try:\n",
    "            import glob\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(\"Could not import the glob module in method BtaggingYieldsAnalyzer\")\n",
    "    if 'copy' not in dir():\n",
    "        try:\n",
    "            import copy\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(\"Could not import the copy module in method BtaggingYieldsAnalyzer\")\n",
    "    files = glob.glob(\"{}/{}\".format(directory, globKey))\n",
    "    #deduce names from the filenames, with optional stripKey parameter that defaults to .root\n",
    "    names = [fname.split(\"/\")[-1].replace(stripKey, \"\") for fname in files]\n",
    "    fileDict = {}\n",
    "    oFile = ROOT.TFile.Open(\"{}/BTaggingYields.root\".format(outDirectory).replace(\"//\", \"/\"),\"RECREATE\")\n",
    "    keysDict = {}\n",
    "    numerators_dict = {}\n",
    "    denominator_dict = {}\n",
    "    #For storing numpy contents to validate error and efficiency calculations\n",
    "    yield_dict = {}\n",
    "    yield_dict_num = {}\n",
    "    yield_dict_den = {}\n",
    "    yield_err_dict = {}\n",
    "    yield_err_dict_num = {}\n",
    "    yield_err_dict_den = {}\n",
    "    for name, fname in zip(names, files):\n",
    "        print(name)\n",
    "        #prepare histogram dictionaries\n",
    "        numerators_dict[name] = {}\n",
    "        denominator_dict[name] = {}\n",
    "        yield_dict[name] = {}\n",
    "        yield_dict_num[name] = {}\n",
    "        yield_dict_den[name] = {}\n",
    "        yield_err_dict[name] = {}\n",
    "        yield_err_dict_num[name] = {}\n",
    "        yield_err_dict_den[name] = {}\n",
    "        #Aggregate only works if it's done before rebinning, should per-sample settings be done\n",
    "        if \"Aggregate\" not in numerators_dict.keys():\n",
    "            numerators_dict[\"Aggregate\"] = {}\n",
    "            denominator_dict[\"Aggregate\"] = {}\n",
    "            yield_dict[\"Aggregate\"] = {}\n",
    "            yield_dict_num[\"Aggregate\"] = {}\n",
    "            yield_dict_den[\"Aggregate\"] = {}\n",
    "            yield_err_dict[\"Aggregate\"] = {}\n",
    "            yield_err_dict_num[\"Aggregate\"] = {}\n",
    "            yield_err_dict_den[\"Aggregate\"] = {}\n",
    "        #inFiles\n",
    "        fileDict[name] = ROOT.TFile.Open(fname, \"READ\")\n",
    "        #hist names\n",
    "        keysDict[name] = [hist.GetName() for hist in fileDict[name].GetListOfKeys()]\n",
    "        #Create pairs of names for the numerator and matching denominator histogram\n",
    "        uniqueTuples = [(tbr, hist, hist.replace(tbr, internalKeys.get(\"Denominator\"))) for hist in keysDict[name] for tbr in internalKeys.get(\"Numerators\", [\"NothingHere\"]) if tbr in hist]\n",
    "        \n",
    "        #Get the rebinning lists with a default fallback, but it will be forced when doNumpyValidation is true\n",
    "        if forceDefaultRebin == False:\n",
    "            x_rebin = sample_rebin.get(name, sample_rebin.get(\"default\"))[\"X\"]\n",
    "            y_rebin = sample_rebin.get(name, sample_rebin.get(\"default\"))[\"Y\"]\n",
    "        else:\n",
    "            x_rebin = sample_rebin.get(\"default\")[\"X\"]\n",
    "            y_rebin = sample_rebin.get(\"default\")[\"Y\"]\n",
    "            \n",
    "        #index everything by the numerator for later naming purposes: $SAMPLENAME_$NUMERATOR style\n",
    "        for tbr, numerator, denominator in uniqueTuples:\n",
    "            internals = copy.copy(numerator)\n",
    "            for k, v in internalKeysReplacements.items():\n",
    "                internals = internals.replace(k, v)\n",
    "            #Get the original numerator/denominator histograms...\n",
    "            numerators_dict[name][numerator] = fileDict[name].Get(numerator)\n",
    "            denominator_dict[name][numerator] = fileDict[name].Get(denominator) #Yeah, this will be duplicated x (number of numerators per denominator)\n",
    "            if numerator not in numerators_dict[\"Aggregate\"].keys():\n",
    "                numerators_dict[\"Aggregate\"][numerator] = numerators_dict[name][numerator].Clone(\"{}_{}\".format(\"Aggregate\", numerator.replace(tbr, \"\")))\n",
    "                denominator_dict[\"Aggregate\"][numerator] = denominator_dict[name][numerator].Clone(\"{}_{}_denominator\".format(\"Aggregate\", numerator.replace(tbr, \"\")))\n",
    "                numerators_dict[\"Aggregate\"][numerator].SetDirectory(0)\n",
    "                denominator_dict[\"Aggregate\"][numerator].SetDirectory(0)\n",
    "            else:\n",
    "                numerators_dict[\"Aggregate\"][numerator].Add(numerators_dict[name][numerator])\n",
    "                denominator_dict[\"Aggregate\"][numerator].Add(denominator_dict[name][numerator])\n",
    "                \n",
    "            #Do rebinning, with flow control for optional numpy validation calculations\n",
    "            if doNumpyValidation:\n",
    "                numerators_dict[name][numerator], yield_dict_num[name][numerator], yield_err_dict_num[name][numerator] = \\\n",
    "                                                                rebin2D(numerators_dict[name][numerator],\n",
    "                                                                \"{}_{}\".format(name, internals),\n",
    "                                                                x_rebin,\n",
    "                                                                y_rebin,\n",
    "                                                                return_numpy_arrays=True,\n",
    "                                                                )\n",
    "                denominator_dict[name][numerator], yield_dict_den[name][numerator], yield_err_dict_den[name][numerator] = \\\n",
    "                                                                rebin2D(denominator_dict[name][numerator],\n",
    "                                                                \"{}_{}_denominator\".format(name, internals),\n",
    "                                                                x_rebin,\n",
    "                                                                y_rebin,\n",
    "                                                                return_numpy_arrays=True,\n",
    "                                                                )\n",
    "                if numerator not in yield_dict_num[\"Aggregate\"].keys():\n",
    "                    yield_dict_num[\"Aggregate\"][numerator] = np.copy(yield_dict_num[name][numerator])\n",
    "                    yield_dict_den[\"Aggregate\"][numerator] = np.copy(yield_dict_den[name][numerator])\n",
    "                    yield_err_dict_num[\"Aggregate\"][numerator] = np.copy(yield_err_dict_num[name][numerator])\n",
    "                    yield_err_dict_den[\"Aggregate\"][numerator] = np.copy(yield_err_dict_den[name][numerator])\n",
    "                else:\n",
    "                    np.add(\n",
    "                        yield_dict_num[\"Aggregate\"][numerator],\n",
    "                        yield_dict_num[name][numerator],\n",
    "                        out=yield_dict_num[\"Aggregate\"][numerator]\n",
    "                    )\n",
    "                    np.add(\n",
    "                        yield_dict_den[\"Aggregate\"][numerator],\n",
    "                        yield_dict_den[name][numerator],\n",
    "                        out=yield_dict_den[\"Aggregate\"][numerator]\n",
    "                    )\n",
    "                    np.sqrt(\n",
    "                        np.square(yield_err_dict_num[\"Aggregate\"][numerator]) + \n",
    "                        np.square(yield_err_dict_num[name][numerator]),\n",
    "                        out=yield_err_dict_num[\"Aggregate\"][numerator]\n",
    "                    )\n",
    "                    np.sqrt(\n",
    "                        np.square(yield_err_dict_num[\"Aggregate\"][numerator]) + \n",
    "                        np.square(yield_err_dict_num[name][numerator]),\n",
    "                        out=yield_err_dict_num[\"Aggregate\"][numerator]\n",
    "                    )\n",
    "            else:\n",
    "                numerators_dict[name][numerator] = rebin2D(numerators_dict[name][numerator],\n",
    "                                                           \"{}_{}\".format(name, internals),\n",
    "                                                           x_rebin,\n",
    "                                                           y_rebin,\n",
    "                                                          )\n",
    "                denominator_dict[name][numerator] = rebin2D(denominator_dict[name][numerator],\n",
    "                                                            \"{}_{}_denominator\".format(name, internals),\n",
    "                                                            x_rebin,\n",
    "                                                            y_rebin,\n",
    "                                                            )\n",
    "                yield_dict_num[name][numerator] = None\n",
    "                yield_dict_den[name][numerator] = None\n",
    "                yield_err_dict_num[name][numerator] = None\n",
    "                yield_err_dict_den[name][numerator] = None\n",
    "                \n",
    "            #Do the yield division\n",
    "            #numerators_dict[name][numerator].GetXaxis().SetRange(1, jets_dict[name][jettype][cat][tag].GetNbinsX())\n",
    "            numerators_dict[name][numerator].Divide(denominator_dict[name][numerator])\n",
    "            #Do some overrides to change titles, axis laabels...\n",
    "            if overrides != None:\n",
    "                internals = copy.copy(numerator)\n",
    "                for k, v in internalKeysReplacements.items():\n",
    "                    internals = internals.replace(k, v)\n",
    "                otitle = overrides[\"Title\"].replace(\"$NAME\", name).replace(\"$INTERNALS\", internals)\n",
    "                oxaxis = overrides[\"Xaxis\"]\n",
    "                oyaxis = overrides[\"Yaxis\"]\n",
    "                numerators_dict[name][numerator].SetTitle(otitle)\n",
    "                numerators_dict[name][numerator].GetXaxis().SetTitle(oxaxis)\n",
    "                numerators_dict[name][numerator].GetYaxis().SetTitle(oyaxis)\n",
    "                #make sure we're back here...\n",
    "                oFile.cd()\n",
    "                numerators_dict[name][numerator].Write()\n",
    "                #renaming to prevent name clash with other samples isn't necessary if name included, so this is just a reminder in case that changes\n",
    "            if doNumpyValidation:\n",
    "                yield_err_dict_num[name][numerator], yield_err_dict_num[name][numerator] = numpy_div_and_error(\n",
    "                    yield_dict_num[name][numerator],\n",
    "                    yield_err_dict_num[name][numerator], \n",
    "                    yield_dict_den[name][numerator], \n",
    "                    yield_err_dict_den[name][numerator]\n",
    "                )\n",
    "        #close the input file\n",
    "        fileDict[name].Close()    \n",
    "                    \n",
    "    #Do aggregate calculations...\n",
    "    #Loop through all the aggregate histograms and write them\n",
    "    name = \"Aggregate\"\n",
    "    #Get the rebinning lists with a default fallback, but it will be forced when doNumpyValidation is true\n",
    "    if forceDefaultRebin == False:\n",
    "        x_rebin = sample_rebin.get(name, sample_rebin.get(\"default\"))[\"X\"]\n",
    "        y_rebin = sample_rebin.get(name, sample_rebin.get(\"default\"))[\"Y\"]\n",
    "    else:\n",
    "        x_rebin = sample_rebin.get(\"default\")[\"X\"]\n",
    "        y_rebin = sample_rebin.get(\"default\")[\"Y\"]\n",
    "    for numerator in numerators_dict[\"Aggregate\"].keys():\n",
    "        internals = copy.copy(numerator)\n",
    "        for k, v in internalKeysReplacements.items():\n",
    "            internals = internals.replace(k, v)\n",
    "        if doNumpyValidation:\n",
    "            numerators_dict[name][numerator], yield_dict_num[name][numerator + \"Cross\"], yield_err_dict_num[name][numerator + \"Cross\"] = \\\n",
    "                                                            rebin2D(numerators_dict[name][numerator],\n",
    "                                                            \"{}_{}\".format(name, internals),\n",
    "                                                            x_rebin,\n",
    "                                                            y_rebin,\n",
    "                                                            return_numpy_arrays=True,\n",
    "                                                            )\n",
    "            denominator_dict[name][numerator], yield_dict_den[name][numerator + \"Cross\"], yield_err_dict_den[name][numerator + \"Cross\"] = \\\n",
    "                                                            rebin2D(denominator_dict[name][numerator],\n",
    "                                                            \"{}_{}_denominator\".format(name, internals),\n",
    "                                                            x_rebin,\n",
    "                                                            y_rebin,\n",
    "                                                            return_numpy_arrays=True,\n",
    "                                                            )\n",
    "        else:\n",
    "            numerators_dict[name][numerator] = rebin2D(numerators_dict[name][numerator],\n",
    "                                                        \"{}_{}\".format(name, internals),\n",
    "                                                        x_rebin,\n",
    "                                                        y_rebin,\n",
    "                                                        )\n",
    "            denominator_dict[name][numerator] = rebin2D(denominator_dict[name][numerator],\n",
    "                                                        \"{}_{}_denominator\".format(name, internals),\n",
    "                                                        x_rebin,\n",
    "                                                        y_rebin,\n",
    "                                                        )\n",
    "        #Do the yield division\n",
    "        #numerators_dict[name][numerator].GetXaxis().SetRange(1, jets_dict[name][jettype][cat][tag].GetNbinsX())\n",
    "        numerators_dict[name][numerator].Divide(denominator_dict[name][numerator])\n",
    "        #Do some overrides to change titles, axis laabels...\n",
    "        if overrides != None:\n",
    "            otitle = overrides[\"Title\"].replace(\"$NAME\", name).replace(\"$INTERNALS\", internals)\n",
    "            oxaxis = overrides[\"Xaxis\"]\n",
    "            oyaxis = overrides[\"Yaxis\"]\n",
    "            numerators_dict[name][numerator].SetTitle(otitle)\n",
    "            numerators_dict[name][numerator].GetXaxis().SetTitle(oxaxis)\n",
    "            numerators_dict[name][numerator].GetYaxis().SetTitle(oyaxis)\n",
    "            #make sure we're back here...\n",
    "            oFile.cd()\n",
    "            numerators_dict[name][numerator].Write()\n",
    "            #renaming to prevent name clash with other samples isn't necessary if name included, so this is just a reminder in case that changes\n",
    "            if doNumpyValidation:\n",
    "                yield_err_dict_num[name][numerator], yield_err_dict_num[name][numerator] = numpy_div_and_error(\n",
    "                    yield_dict_num[name][numerator],\n",
    "                    yield_err_dict_num[name][numerator], \n",
    "                    yield_dict_den[name][numerator], \n",
    "                    yield_err_dict_den[name][numerator]\n",
    "                )\n",
    "    #close the output file\n",
    "    oFile.Close() \n",
    " \n",
    "    \n",
    "def BtaggingEfficienciesAnalyzer(directory, outDirectory=\"{}/BtaggingEfficiencies\", globKey=\"*.root\", stripKey=\".root\", \n",
    "                       name_format=\"Btagging*$CAT*$JETTYPE_$TAG\",\n",
    "                       format_dict={\"categories\": [\"Inclusive\", \"nJet4\", \"nJet5\", \"nJet6\", \"nJet7\", \"nJet8+\"],\n",
    "                                    \"jettypes\": [\"bjets\", \"cjets\", \"udsgjets\"],\n",
    "                                    \"tags\": [\"DeepCSV_L\", \"DeepCSV_M\", \"DeepCSV_T\", \"DeepJet_L\", \"DeepJet_M\", \"DeepJet_T\"],\n",
    "                                    \"untag\": \"untagged\", \n",
    "                                   },\n",
    "                       jettype_rebin={\"bjets\":{\"Y\": [0.0, 0.2, 0.4, 0.6, 0.8, 1.0, 1.2, 1.4, 1.6, 1.8, 2.0, 2.2, 2.5],\n",
    "                                               \"X\": [20.0, 30.0, 40.0, 50.0, 60.0, 70.0, 80.0, 90.0, 100.0, 200.0, 2500.0],\n",
    "                                              },\n",
    "                                      \"cjets\":{\"Y\": [0.0, 0.2, 0.4, 0.6, 0.8, 1.0, 1.2, 1.4, 1.6, 1.8, 2.0, 2.2, 2.5],\n",
    "                                               \"X\": [20.0, 30.0, 40.0, 50.0, 60.0, 70.0, 80.0, 90.0, 100.0, 200.0, 2500.0],\n",
    "                                              },\n",
    "                                      \"udsgjets\":{\"Y\": [0.0, 0.2, 0.4, 0.6, 0.8, 1.0, 1.2, 1.4, 1.6, 1.8, 2.0, 2.2, 2.5],\n",
    "                                                  \"X\": [20.0, 30.0, 40.0, 50.0, 60.0, 70.0, 80.0, 90.0, 100.0, 200.0, 2500.0],\n",
    "                                                 },\n",
    "                                     },\n",
    "                       overrides={\"Title\": \"$NAME $JETTYPE $TAG Efficiency($CATEGORY)\",\n",
    "                                  \"Xaxis\": \"Jet p_{T}\",\n",
    "                                  \"Yaxis\": \"Jet |#eta|\",\n",
    "                                 },\n",
    "                       mode=\"RECREATE\",\n",
    "                       doNumpyValidation=False,\n",
    "                       debug=False,debug_dict={}):\n",
    "    \"\"\"For btagging event weight calculations using method 1a and similar (non-shape corrections)\n",
    "    \n",
    "    take list of files in <directory>, with optional <globKey>, and create individual root files containing\n",
    "    each sample's btagging efficiency histograms, based on derived categories. Keys can be parsed with \n",
    "    <name_format> (default 'Btagging*$CAT*$JETTYPE_$TAG') where $CAT, $JETTYPE, and $TAG are cycled through from \n",
    "    their respective input lists, format_dict{<categories>, <jettypes>, <tags>}. The format_dict{<untag>} option \n",
    "    specifies the denominator histogram (where $TAG is replaced by <untag>). A file 'ttWH.root' with \n",
    "    'Btagging*nJet4*bjets_DeepJet_T' will generate a file 'ttWH_BTagEff.root' containing the histogram \n",
    "    'nJet4_bjets_DeepJet_T'\"\"\"\n",
    "    \n",
    "    #old binning in X:\n",
    "    #\"bjets\":\"X\": [20.0, 30.0, 50.0, 70.0, 100.0, 2500.0],\n",
    "    #\"cjets\":\"X\": [20.0, 30.0, 50.0, 60.0, 90.0, 2500.0],\n",
    "    #\"udsgjets\":\"X\": [20.0, 30.0, 40.0, 60.0, 2500.0]\n",
    "    \n",
    "    if \"{}/\" in outDirectory:\n",
    "        outDirectory = outDirectory.format(directory)\n",
    "    print(\"Checking for (and if necessary, creating) directory {}\".format(outDirectory))\n",
    "    if not os.path.isdir(outDirectory):\n",
    "        os.makedirs(outDirectory)\n",
    "    #Get the files\n",
    "    if 'glob' not in dir():\n",
    "        try:\n",
    "            import glob\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(\"Could not import the glob module in method BtaggingEfficienciesAnalyzer\")\n",
    "    files = glob.glob(\"{}/{}\".format(directory, globKey))\n",
    "    #deduce names from the filenames, with optional stripKey parameter that defaults to .root\n",
    "    names = [fname.split(\"/\")[-1].replace(stripKey, \"\") for fname in files]\n",
    "    fileDict = {}\n",
    "    oFileDict = {}\n",
    "    keysDict = {}\n",
    "    nominalDict = {}\n",
    "    keySet = set([])\n",
    "    jets_dict = {}\n",
    "    canvas_dict = {}\n",
    "    untag = format_dict[\"untag\"]\n",
    "    #For storing numpy contents to compute chi-square between efficincy measurements\n",
    "    eff_dict = {}\n",
    "    eff_err_dict = {}\n",
    "    for name, fname in zip(names, files):\n",
    "        print(name)\n",
    "        #prepare histogram dictionaries\n",
    "        jets_dict[name] = {}\n",
    "        canvas_dict[name] = {}\n",
    "        eff_dict[name] = {}\n",
    "        eff_err_dict[name] = {}\n",
    "        if \"Aggregate\" not in jets_dict.keys(): \n",
    "            jets_dict[\"Aggregate\"] = {}\n",
    "        #if \"Aggregate\" not in canvas_dict.keys(): \n",
    "            canvas_dict[\"Aggregate\"] = {}\n",
    "        #if \"Aggregate\" not in eff_dict.keys():\n",
    "            eff_dict[\"Aggregate\"] = {}\n",
    "        #if \"Aggregate\" not in eff_err_dict.keys():\n",
    "            eff_err_dict[\"Aggregate\"] = {}\n",
    "        #inFiles\n",
    "        fileDict[name] = ROOT.TFile.Open(fname, \"READ\")\n",
    "        #hist names\n",
    "        keysDict[name] = [hist.GetName() for hist in fileDict[name].GetListOfKeys() if \"Btagging*\" in hist.GetName()]\n",
    "        #Skip files without Btagging \n",
    "        if len(keysDict[name]) is 0: \n",
    "            print(\"Skipping sample {} whose file contains no histograms containing 'Btagging*'\".format(name))\n",
    "            fileDict[name].Close()\n",
    "            continue\n",
    "        #outFiles\n",
    "        oFileDict[name] = ROOT.TFile.Open(\"{}/{}_BTagEff.root\".format(outDirectory, name).replace(\"//\", \"/\"),\n",
    "                                          \"RECREATE\"\n",
    "                                         )\n",
    "        #cycle through the jettypes\n",
    "        for jettype in format_dict[\"jettypes\"]:\n",
    "            jets_dict[name][jettype] = {}\n",
    "            canvas_dict[name][jettype] = {}\n",
    "            eff_dict[name][jettype] = {}\n",
    "            eff_err_dict[name][jettype] = {}\n",
    "            #Assume all dictionaries are being filled together\n",
    "            if jettype not in jets_dict[\"Aggregate\"].keys(): \n",
    "                jets_dict[\"Aggregate\"][jettype] = {}\n",
    "            #if jettype not in canvas_dict[\"Aggregate\"].keys(): \n",
    "                canvas_dict[\"Aggregate\"][jettype] = {}\n",
    "            #if jettype not in eff_dict[\"Aggregate\"].keys():\n",
    "                eff_dict[\"Aggregate\"][jettype] = {}\n",
    "            #if jettype not in eff_err_dict[\"Aggregate\"].keys():\n",
    "                eff_err_dict[\"Aggregate\"][jettype] = {}\n",
    "            #cycle through the categories\n",
    "            for cat in format_dict[\"categories\"]:\n",
    "                jets_dict[name][jettype][cat] = {}\n",
    "                canvas_dict[name][jettype][cat] = {}\n",
    "                eff_dict[name][jettype][cat] = {}\n",
    "                eff_err_dict[name][jettype][cat] = {}\n",
    "                if cat not in jets_dict[\"Aggregate\"][jettype].keys(): \n",
    "                    jets_dict[\"Aggregate\"][jettype][cat] = {}\n",
    "                #if cat not in canvas_dict[\"Aggregate\"][jettype].keys(): \n",
    "                    canvas_dict[\"Aggregate\"][jettype][cat] = {}\n",
    "                #if cat not in eff_dict[\"Aggregate\"][jettype].keys(): \n",
    "                    eff_dict[\"Aggregate\"][jettype][cat] = {}\n",
    "                #if cat not in eff_err_dict[\"Aggregate\"][jettype].keys(): \n",
    "                    eff_err_dict[\"Aggregate\"][jettype][cat] = {}\n",
    "                keyWithTag = name_format.replace(\"$CAT\", cat).replace(\"$JETTYPE\", jettype)\n",
    "                #make dict for the denominator histograms (untagged jets), but skip this category if it's not in the file\n",
    "                if keyWithTag.replace(\"$TAG\", untag) not in keysDict[name]: continue\n",
    "                jets_dict[name][jettype][cat][untag] = fileDict[name].Get(keyWithTag.replace(\"$TAG\", untag))\n",
    "                #jets_dict[name][jettype][cat][untag].SetDirectory(0)\n",
    "                if doNumpyValidation:\n",
    "                    jets_dict[name][jettype][cat][untag], eff_dict[name][jettype][cat][untag], eff_err_dict[name][jettype][cat][untag] = \\\n",
    "                                                                  rebin2D(jets_dict[name][jettype][cat][untag],\n",
    "                                                                   \"{}_{}_{}_{}\".format(name, cat, jettype, untag),\n",
    "                                                                   jettype_rebin[jettype][\"X\"],\n",
    "                                                                   jettype_rebin[jettype][\"Y\"],\n",
    "                                                                   return_numpy_arrays=True,\n",
    "                                                                  )\n",
    "                else:\n",
    "                    jets_dict[name][jettype][cat][untag] = rebin2D(jets_dict[name][jettype][cat][untag],\n",
    "                                                                   \"{}_{}_{}_{}\".format(name, cat, jettype, untag),\n",
    "                                                                   jettype_rebin[jettype][\"X\"],\n",
    "                                                                   jettype_rebin[jettype][\"Y\"]\n",
    "                                                                  )\n",
    "                    eff_dict[name][jettype][cat][untag] = None\n",
    "                    eff_err_dict[name][jettype][cat][untag] = None\n",
    "                    \n",
    "                if untag not in jets_dict[\"Aggregate\"][jettype][cat].keys():\n",
    "                    jets_dict[\"Aggregate\"][jettype][cat][untag] = jets_dict[name][jettype][cat][untag].Clone(\"{}_{}_{}_{}\".format(\"Aggregate\", cat, jettype, untag))\n",
    "                    jets_dict[\"Aggregate\"][jettype][cat][untag].SetDirectory(0)\n",
    "                    #If doNumpyValidation is false, these values will be None\n",
    "                    if doNumpyValidation:\n",
    "                        eff_dict[\"Aggregate\"][jettype][cat][untag] = np.copy(eff_dict[name][jettype][cat][untag])\n",
    "                        eff_err_dict[\"Aggregate\"][jettype][cat][untag] = np.copy(eff_err_dict[name][jettype][cat][untag])\n",
    "                elif doNumpyValidation:\n",
    "                    jets_dict[\"Aggregate\"][jettype][cat][untag].Add(jets_dict[name][jettype][cat][untag])\n",
    "                    np.add(\n",
    "                        eff_dict[\"Aggregate\"][jettype][cat][untag], \n",
    "                        eff_dict[name][jettype][cat][untag],\n",
    "                        out=eff_dict[\"Aggregate\"][jettype][cat][untag]\n",
    "                    )\n",
    "                    np.sqrt(\n",
    "                        np.square(eff_err_dict[\"Aggregate\"][jettype][cat][untag]) + \n",
    "                        np.square(eff_err_dict[name][jettype][cat][untag]),\n",
    "                        out=eff_err_dict[\"Aggregate\"][jettype][cat][untag]\n",
    "                        )\n",
    "                else:\n",
    "                    jets_dict[\"Aggregate\"][jettype][cat][untag].Add(jets_dict[name][jettype][cat][untag])\n",
    "                    \n",
    "                if debug and jettype == \"bjets\" and cat in [\"Inclusive\"]:# and name in [\"tt_DL-GF\", \"tt_DL\", \"tttt\"]:\n",
    "                    nums = jets_dict[name][jettype][cat][untag].GetBinContent(5, 6)\n",
    "                    numa = jets_dict[\"Aggregate\"][jettype][cat][untag].GetBinContent(5, 6) #x, y\n",
    "                    print(\"untagged {} {} {}/{}\".format(name, cat, nums, numa))\n",
    "                    if doNumpyValidation:\n",
    "                        numss = eff_dict[name][jettype][cat][untag][7, 5] #nYbins-1-y, x\n",
    "                        numaa = eff_dict[\"Aggregate\"][jettype][cat][untag][7, 5]\n",
    "                        print(\"untagged {} {} numpy array {}/{}\".format(name, cat, numss, numaa))\n",
    "                #cycle through all the tag categories available\n",
    "                for tag in format_dict[\"tags\"]:\n",
    "                    jets_dict[name][jettype][cat][tag] = fileDict[name].Get(keyWithTag.replace(\"$TAG\", tag))\n",
    "                    #jets_dict[name][jettype][cat][tag].SetDirectory(0) #Disassociates from the original file\n",
    "                    #Rebin here...\n",
    "                    if doNumpyValidation:\n",
    "                        jets_dict[name][jettype][cat][tag], eff_dict[name][jettype][cat][tag], eff_err_dict[name][jettype][cat][tag] = \\\n",
    "                                                                  rebin2D(jets_dict[name][jettype][cat][tag],\n",
    "                                                                   \"{}_{}_{}\".format(cat, jettype, tag),\n",
    "                                                                   jettype_rebin[jettype][\"X\"],\n",
    "                                                                   jettype_rebin[jettype][\"Y\"],\n",
    "                                                                   return_numpy_arrays=True,\n",
    "                                                                  )\n",
    "                    else:\n",
    "                        jets_dict[name][jettype][cat][tag] = rebin2D(jets_dict[name][jettype][cat][tag],\n",
    "                                                                     \"{}_{}_{}\".format(cat, jettype, tag),\n",
    "                                                                     jettype_rebin[jettype][\"X\"],\n",
    "                                                                     jettype_rebin[jettype][\"Y\"]\n",
    "                                                                    )\n",
    "                    #Make or add to the aggregate histogram, and do numpy calcs if chi-square requested\n",
    "                    if tag not in jets_dict[\"Aggregate\"][jettype][cat].keys():\n",
    "                        jets_dict[\"Aggregate\"][jettype][cat][tag] = jets_dict[name][jettype][cat][tag].Clone(\"{}_{}_{}_{}\".format(\"Aggregate\", cat, jettype, tag))\n",
    "                        jets_dict[\"Aggregate\"][jettype][cat][tag].SetDirectory(0)\n",
    "                        if doNumpyValidation:\n",
    "                            eff_dict[\"Aggregate\"][jettype][cat][tag] = np.copy(eff_dict[name][jettype][cat][tag])\n",
    "                            eff_err_dict[\"Aggregate\"][jettype][cat][tag] = np.copy(eff_err_dict[name][jettype][cat][tag])\n",
    "                    elif doNumpyValidation:\n",
    "                        jets_dict[\"Aggregate\"][jettype][cat][tag].Add(jets_dict[name][jettype][cat][tag])\n",
    "                        np.add(\n",
    "                            eff_dict[\"Aggregate\"][jettype][cat][tag], \n",
    "                            eff_dict[name][jettype][cat][tag],\n",
    "                            out=eff_dict[\"Aggregate\"][jettype][cat][tag]\n",
    "                        )\n",
    "                        np.sqrt(\n",
    "                            np.square(eff_err_dict[\"Aggregate\"][jettype][cat][tag]) + \n",
    "                            np.square(eff_err_dict[name][jettype][cat][tag]),\n",
    "                            out=eff_err_dict[\"Aggregate\"][jettype][cat][tag]\n",
    "                        )\n",
    "                    else:\n",
    "                        jets_dict[\"Aggregate\"][jettype][cat][tag].Add(jets_dict[name][jettype][cat][tag])\n",
    "                        \n",
    "                    if debug and jettype == \"bjets\" and cat == \"Inclusive\" and tag==\"DeepCSV_M\":#and name in [\"tt_DL-GF\", \"tt_DL\", \"tttt\"]:\n",
    "                        nums = jets_dict[name][jettype][cat][tag].GetBinContent(5, 6)\n",
    "                        numa = jets_dict[\"Aggregate\"][jettype][cat][tag].GetBinContent(5, 6) #x, y\n",
    "                        print(\"{} {} {} {}/{}\".format(tag, name, cat, nums, numa))\n",
    "                        if doNumpyValidation:\n",
    "                            numss = eff_dict[name][jettype][cat][tag][7, 5] #nYbins-1-y, x\n",
    "                            numaa = eff_dict[\"Aggregate\"][jettype][cat][tag][7, 5]\n",
    "                            print(\"{} {} {} numpy array {}/{}\".format(tag, name, cat, numss, numaa))\n",
    "                        \n",
    "                    #Do the efficiency division\n",
    "                    jets_dict[name][jettype][cat][tag].GetXaxis().SetRange(1, jets_dict[name][jettype][cat][tag].GetNbinsX())\n",
    "                    jets_dict[name][jettype][cat][tag].Divide(jets_dict[name][jettype][cat][untag])\n",
    "                    #Do some overrides to change titles, axis laabels...\n",
    "                    if overrides != None:\n",
    "                        otitle = overrides[\"Title\"].replace(\"$NAME\", name).replace(\"$JETTYPE\", jettype).replace(\"$TAG\", tag).replace(\"$CATEGORY\", cat)\n",
    "                        oxaxis = overrides[\"Xaxis\"]\n",
    "                        oyaxis = overrides[\"Yaxis\"]\n",
    "                        jets_dict[name][jettype][cat][tag].SetTitle(otitle)\n",
    "                        jets_dict[name][jettype][cat][tag].GetXaxis().SetTitle(oxaxis)\n",
    "                        jets_dict[name][jettype][cat][tag].GetYaxis().SetTitle(oyaxis)\n",
    "                        \n",
    "                    jets_dict[name][jettype][cat][tag].Write()\n",
    "                    #rename to prevent name clash with other samples\n",
    "                    jets_dict[name][jettype][cat][tag].SetName(\"{}_{}_{}_{}\".format(name, cat, jettype, tag))\n",
    "                    \n",
    "                    if doNumpyValidation:\n",
    "                        eff_dict[name][jettype][cat][tag], eff_err_dict[name][jettype][cat][tag] = numpy_div_and_error(\n",
    "                            eff_dict[name][jettype][cat][tag],\n",
    "                            eff_err_dict[name][jettype][cat][tag], \n",
    "                            eff_dict[name][jettype][cat][untag], \n",
    "                            eff_err_dict[name][jettype][cat][untag]\n",
    "                        )\n",
    "                    \n",
    "        fileDict[name].Close()\n",
    "        oFileDict[name].Close()\n",
    "        \n",
    "    #Loop through all the aggregate histograms and write them to a separate file. \n",
    "    aggregateFile = ROOT.TFile.Open(\"{}/{}_BTagEff.root\".format(outDirectory, \"Aggregate\").replace(\"//\", \"/\"),\n",
    "                                          \"RECREATE\")\n",
    "    for jettype, jettype_dict in jets_dict[\"Aggregate\"].items():\n",
    "        for cat, cat_dict in jettype_dict.items():\n",
    "            for tag, tag_hist in cat_dict.items():\n",
    "                jets_dict[\"Aggregate\"][jettype][cat][tag].Divide(jets_dict[\"Aggregate\"][jettype][cat][untag])\n",
    "                jets_dict[\"Aggregate\"][jettype][cat][tag].SetName(\"{}_{}_{}\".format(cat, jettype, tag))\n",
    "                #Override the title, Xaxis, Yaxis for the histogram\n",
    "                if overrides != None:\n",
    "                    otitle = overrides.get(\"Title\", \"NONE\").replace(\"$NAME\", name).replace(\"$JETTYPE\", jettype).replace(\"$TAG\", tag).replace(\"$CATEGORY\", cat)\n",
    "                    oxaxis = overrides.get(\"Xaxis\", \"NONE\")\n",
    "                    oyaxis = overrides.get(\"Yaxis\", \"NONE\")\n",
    "                    if otitle != \"NONE\":\n",
    "                        jets_dict[\"Aggregate\"][jettype][cat][tag].SetTitle(otitle)\n",
    "                    if oxaxis != \"NONE\":\n",
    "                        jets_dict[\"Aggregate\"][jettype][cat][tag].GetXaxis().SetTitle(oxaxis)\n",
    "                    if oyaxis != \"NONE\":\n",
    "                        jets_dict[\"Aggregate\"][jettype][cat][tag].GetYaxis().SetTitle(oyaxis)\n",
    "                jets_dict[\"Aggregate\"][jettype][cat][tag].Write()\n",
    "                jets_dict[\"Aggregate\"][jettype][cat][tag].SetName(\"{}_{}_{}_{}\".format(\"Aggregate\", cat, jettype, tag))\n",
    "                if doNumpyValidation:\n",
    "                    eff_dict[\"Aggregate\"][jettype][cat][tag], eff_err_dict[\"Aggregate\"][jettype][cat][tag] = numpy_div_and_error(\n",
    "                        eff_dict[\"Aggregate\"][jettype][cat][tag],\n",
    "                        eff_err_dict[\"Aggregate\"][jettype][cat][tag], \n",
    "                        eff_dict[\"Aggregate\"][jettype][cat][untag], \n",
    "                        eff_err_dict[\"Aggregate\"][jettype][cat][untag]\n",
    "                    )\n",
    "                    if debug and jettype == \"bjets\" and cat == \"Inclusive\" and tag in [\"DeepCSV_M\"]:\n",
    "                        #print(eff_dict[\"Aggregate\"][jettype][cat][tag])\n",
    "                        #print(eff_err_dict[\"Aggregate\"][jettype][cat][tag])\n",
    "                        debug_dict[\"agg_eff\"] = eff_dict[\"Aggregate\"][jettype][cat][tag]\n",
    "                        debug_dict[\"agg_err\"] = eff_err_dict[\"Aggregate\"][jettype][cat][tag]\n",
    "    aggregateFile.Close()\n",
    "    #return jets_dict, oFileDict\n",
    "\n",
    "def rebin2D(hist, name, xbins, ybins, return_numpy_arrays=False):\n",
    "    \"\"\"Rebin a 2D histogram by project slices in Y, adding them together, and using TH1::Rebin along the X axes,\n",
    "    then create a new histogram with the content of these slices\"\"\"\n",
    "    if return_numpy_arrays:\n",
    "        if 'numpy' not in dir() and 'np' not in dir():\n",
    "            try:\n",
    "                import numpy as np\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(\"Could not import the numpy module in method rebin2D\")\n",
    "    #xbins_vec = ROOT.std.vector(float)(len(xbins))\n",
    "    nxbins = []\n",
    "    xbins_vec = array.array('d', xbins)\n",
    "    for xn, x in enumerate(xbins):\n",
    "        nxbins.append(hist.GetXaxis().FindBin(x))\n",
    "        #xbins_vec[xn] = x\n",
    "        \n",
    "        \n",
    "    #ybins_vec = ROOT.std.vector(float)(len(ybins))\n",
    "    nybins = []\n",
    "    ybins_vec = array.array('d', ybins)\n",
    "    for yn, y in enumerate(ybins):\n",
    "        nybins.append(hist.GetYaxis().FindBin(y))\n",
    "        #ybins_vec[yn] = y\n",
    "    #Get xrange objects that store the bins to be projected and added\n",
    "    ybinsrange = [xrange(nybins[:-1][z], nybins[1:][z]) for z in xrange(len(nybins)-1)]\n",
    "    \n",
    "    #set up the final histogram, copying most of the parameters over\n",
    "    final_hist = ROOT.TH2D(name, hist.GetTitle(), len(xbins)-1, xbins_vec, len(ybins)-1, ybins_vec)\n",
    "    #Xaxis\n",
    "    #final_hist.GetXaxis().SetNdivisions(hist.GetXaxis().GetNdivisions())\n",
    "    final_hist.GetXaxis().SetAxisColor(hist.GetXaxis().GetAxisColor())\n",
    "    final_hist.GetXaxis().SetLabelColor(hist.GetXaxis().GetLabelColor())\n",
    "    final_hist.GetXaxis().SetLabelFont(hist.GetXaxis().GetLabelFont())\n",
    "    final_hist.GetXaxis().SetLabelOffset(hist.GetXaxis().GetLabelOffset())\n",
    "    final_hist.GetXaxis().SetLabelSize(hist.GetXaxis().GetLabelSize())\n",
    "    final_hist.GetXaxis().SetTickLength(hist.GetXaxis().GetTickLength())\n",
    "    final_hist.GetXaxis().SetTitleOffset(hist.GetXaxis().GetTitleOffset())\n",
    "    final_hist.GetXaxis().SetTitleSize(hist.GetXaxis().GetTitleSize())\n",
    "    final_hist.GetXaxis().SetTitleColor(hist.GetXaxis().GetTitleColor())\n",
    "    final_hist.GetXaxis().SetTitleFont(hist.GetXaxis().GetTitleFont())\n",
    "    #Yaxis\n",
    "    #final_hist.GetYaxis().SetNdivisions(hist.GetYaxis().GetNdivisions())\n",
    "    final_hist.GetYaxis().SetAxisColor(hist.GetYaxis().GetAxisColor())\n",
    "    final_hist.GetYaxis().SetLabelColor(hist.GetYaxis().GetLabelColor())\n",
    "    final_hist.GetYaxis().SetLabelFont(hist.GetYaxis().GetLabelFont())\n",
    "    final_hist.GetYaxis().SetLabelOffset(hist.GetYaxis().GetLabelOffset())\n",
    "    final_hist.GetYaxis().SetLabelSize(hist.GetYaxis().GetLabelSize())\n",
    "    final_hist.GetYaxis().SetTickLength(hist.GetYaxis().GetTickLength())\n",
    "    final_hist.GetYaxis().SetTitleOffset(hist.GetYaxis().GetTitleOffset())\n",
    "    final_hist.GetYaxis().SetTitleSize(hist.GetYaxis().GetTitleSize())\n",
    "    final_hist.GetYaxis().SetTitleColor(hist.GetYaxis().GetTitleColor())\n",
    "    final_hist.GetYaxis().SetTitleFont(hist.GetYaxis().GetTitleFont())\n",
    "    \n",
    "    slice_dict = {}\n",
    "    #Begin looping through slices that are to be made, each slice composed of multiple bins in the yrange, potentially\n",
    "    for sn, ybinset in enumerate(ybinsrange):\n",
    "        slice_dict[str(sn)] = {}\n",
    "        #ybinset is an xrange object, so iterate through it for each ybin to be added in this slice\n",
    "        for bn, ybin in enumerate(ybinset):\n",
    "            #Create hist for this slice if it's the first bin being combined\n",
    "            if bn is 0:\n",
    "                slice_dict[str(sn)][\"hist\"] = hist.ProjectionX(\"{}_Yslice{}\".format(hist.GetName(), sn), ybin, ybin)\n",
    "            #THAdd the rest of the bins being combined into this slice\n",
    "            else:\n",
    "                slice_dict[str(sn)][\"hist\"].Add(hist.ProjectionX(\"{}_Yslice{}_subslice{}\".format(hist.GetName(), sn, bn), ybin, ybin))\n",
    "            \n",
    "            #If it's the last bin for this slice, do the X rebinning\n",
    "            if bn is len(ybinset)-1:\n",
    "                #make sure to get the return value, don't try to rebin in place\n",
    "                slice_dict[str(sn)][\"hist\"] = slice_dict[str(sn)][\"hist\"].Rebin(len(xbins)-1, \"\", xbins_vec)\n",
    "        #Carry over slice content and errors to the new histogram, remembering sn starts at 0, and non-underflow\n",
    "        #in histograms begins at 1 (overflow at NBins + 1, requiring us to add 2 when creating an xrange object)\n",
    "        #print(slice_dict[str(sn)])\n",
    "        for fbn in xrange(slice_dict[str(sn)][\"hist\"].GetXaxis().GetNbins()+2):\n",
    "            #sn+1 might be in error if we actually need underflows and overflows in the y range...\n",
    "            final_hist.SetBinContent(fbn, sn+1, slice_dict[str(sn)][\"hist\"].GetBinContent(fbn))\n",
    "            final_hist.SetBinError(fbn, sn+1, slice_dict[str(sn)][\"hist\"].GetBinError(fbn))\n",
    "                                 \n",
    "    \n",
    "    if return_numpy_arrays:\n",
    "        #Create arrays of zeros to be filled after rebinning, with extra rows and columns for over/underflows\n",
    "        #Note: actual bins are len(<axis>bins) - 1, and we add 2 for the x axis to account for under/overflow\n",
    "        #since y-axis will account for it via the ranges actually included when slicing and projections are done\n",
    "        nBinsX = final_hist.GetXaxis().GetNbins()+2\n",
    "        nBinsY = final_hist.GetYaxis().GetNbins()+2\n",
    "        hist_contents = np.zeros((nBinsY, nBinsX), dtype=float)\n",
    "        hist_errors = np.zeros((nBinsY, nBinsX), dtype=float)\n",
    "        #Reverse the y array since numpy counts from top to bottom, and swap X and Y coordinates (row-column)\n",
    "        for x in xrange(nBinsX):\n",
    "            for y in xrange(nBinsY):\n",
    "                hist_contents[nBinsY-1-y, x] = final_hist.GetBinContent(x, y)\n",
    "                hist_errors[nBinsY-1-y, x] = final_hist.GetBinError(x, y)\n",
    "        return final_hist, hist_contents, hist_errors\n",
    "    else:\n",
    "        return final_hist\n",
    "    \n",
    "def numpy_div_and_error(num, num_err, den, den_err):\n",
    "    \"\"\"Take 4 numpy arrays containing the numerator, numerator errors, denominator, and denominator errors.\n",
    "    Compute the division and appropriate error\"\"\"\n",
    "    \n",
    "    if 'numpy' not in dir() and 'np' not in dir():\n",
    "        try:\n",
    "            import numpy as np\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(\"Could not import the numpy module in method numpy_div_and_error\")\n",
    "    \n",
    "    #A = B/C; (dA/A)^2 = (dB/B)^2 + (dC/C)^2. Compute sqrt(RHS) of the error first\n",
    "    #Use the out = zeros and where=(denominator != 0) to prevent NaN in divisions\n",
    "    num_err = np.sqrt(np.add(\n",
    "        np.square(\n",
    "            np.divide(\n",
    "                num_err,\n",
    "                num,\n",
    "                out = np.zeros_like(num),\n",
    "                where=(num!=0)\n",
    "            )\n",
    "        ),\n",
    "        np.square(\n",
    "            np.divide(\n",
    "                den_err,\n",
    "                den,\n",
    "                out = np.zeros_like(den),\n",
    "                where=(den!=0)\n",
    "            )\n",
    "        )\n",
    "    ))\n",
    "    #A = B/C\n",
    "    num = np.divide(\n",
    "        num,\n",
    "        den,\n",
    "        out = np.zeros_like(num),\n",
    "        where=(den!=0)\n",
    "    )\n",
    "    #dA = A* sqrt(RHS of formula) (latter already stored in num_err)\n",
    "    num_err = np.multiply(num_err, num)\n",
    "    return num, num_err\n",
    "\n",
    "def ChiSquareTest(input_file, test_against=\"All\", must_contain = [], must_not_contain=[]):\n",
    "    if 'ctypes' not in dir():\n",
    "        try:\n",
    "            import ctypes\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(\"Could not import the ctypes module in method ChiSquareTest\")\n",
    "    if 'copy' not in dir():\n",
    "        try:\n",
    "            import copy\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(\"Could not import the copy module in method ChiSquareTest\")\n",
    "    f = ROOT.TFile.Open(input_file, \"read\")\n",
    "    #not a smart check against stacks, differing dimensioned histograms... use at one's own risk\n",
    "    h_list_unclean = [h.GetName() for h in f.GetListOfKeys() if \"ROOT.TH\" in str(type(f.Get(h.GetName())))]\n",
    "    h_list = []\n",
    "    for h in h_list_unclean:\n",
    "        if len(must_contain) == 0:\n",
    "            matched_contained = True\n",
    "        else:\n",
    "            matched_contained = False\n",
    "            for mc in must_contain:\n",
    "                if mc in h: matched_contained = True\n",
    "        matched_not_contained = False\n",
    "        for mnc in must_not_contain:\n",
    "            if mnc in h: matched_not_contained = True\n",
    "        if not matched_not_contained and matched_contained: h_list.append(h)\n",
    "        \n",
    "    #print(h_list)\n",
    "    test_against_clean = []\n",
    "    if type(test_against) == str:\n",
    "        if test_against.lower() == \"all\":\n",
    "            test_against_clean = h_list\n",
    "        elif test_against in h_list:\n",
    "            test_against_clean = [test_against]\n",
    "    elif type(test_against) == list:\n",
    "        test_against_clean = [h for h in test_against if h in h_list]\n",
    "    #print(test_against_clean)\n",
    "\n",
    "    tuples = []\n",
    "    for t in test_against_clean:\n",
    "        for h in h_list:\n",
    "            chi2 = ctypes.c_double()\n",
    "            ndf = ctypes.c_int()\n",
    "            igood = ctypes.c_int()\n",
    "            p = f.Get(t).Chi2TestX(f.Get(h), chi2, ndf, igood, \"WW\");\n",
    "            tuples.append((t, h, copy.copy(chi2), copy.copy(ndf), copy.copy(igood), copy.copy(p)))\n",
    "    for tup in tuples:\n",
    "        print(\"{} :: {}\\n\\tTest Result: ChiSquare/ndf: {} ndf: {} p-value: {}\"\\\n",
    "              .format(tup[0], tup[1], float(tup[2].value)/float(tup[3].value), tup[3].value, tup[5]))\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cartesian_product_list(name_format=\"$NUM_$LET_$SYM\", name_tuples=[(\"$NUM\", [\"1\", \"2\"]), (\"$LET\", [\"A\", \"B\", \"C\"]), (\"$SYM\", [\"*\", \"@\"])]):\n",
    "    \"\"\"Take as input a string <name_format> and list of tuples <name_tuple> where a cartesian product of the tuples is formed.\n",
    "    The tuples contain a key-string (also present in the name_format string) and value-list with the replacements to cycle through.\n",
    "    The last tuple is the innermost replacement in the list formed, regardless of placement in the name_format string.\"\"\"\n",
    "    if 'copy' not in dir():\n",
    "        try:\n",
    "            import copy\n",
    "        except:\n",
    "            raise RuntimeError(\"Could not import the copy module in method cartesian_product_list\")\n",
    "    if 'itertools' not in dir():\n",
    "        try:\n",
    "            import itertools\n",
    "        except:\n",
    "            raise RuntimeError(\"Could not import the itertools module in method cartesian_product_list\")\n",
    "    list_of_lists = []\n",
    "    list_of_keys = []\n",
    "    for k, v in name_tuples:\n",
    "        list_of_lists.append(v)\n",
    "        list_of_keys.append(k)\n",
    "    cart_prod = [zip(list_of_keys, l) for l in list(itertools.product(*list_of_lists))]\n",
    "    ret_list = []\n",
    "    for uzip in cart_prod:\n",
    "        nc = copy.copy(name_format)\n",
    "        for k, v in uzip:\n",
    "            nc = nc.replace(k, v)\n",
    "        ret_list.append(nc)\n",
    "    return ret_list\n",
    "\n",
    "def root_to_pdf(directory, outDirectory=\"{}/PDF\", globKey=\"*.root\", stripKey=\".root\", \n",
    "             name_format=\"$CAT_$JETTYPE_$TAG\",\n",
    "             name_tuples=[(\"$JETTYPE\", [\"bjets\", \"cjets\", \"udsgjets\"]), (\"$TAG\", [\"DeepCSV_M\",]),\n",
    "                         (\"$CAT\", [\"Inclusive\", \"nJet4\", \"nJet5\", \"nJet6\", \"nJet7\", \"nJet8+\"])],\n",
    "             draw_option=\"COLZ TEXTE\", draw_min=None, draw_max=None\n",
    "            ):\n",
    "    \"\"\"take list of files in <directory>, with optional <globKey>, and create individual PDF files containing\n",
    "    each file's histograms, based on derived categories. Keys can be parsed with \n",
    "    <name_format> (default '$CAT_$JETTYPE_$TAG') where up to 3 $KEYs are cycled through from \n",
    "    their respective input lists.\"\"\"\n",
    "    if \"{}/\" in outDirectory:\n",
    "        outDirectory = outDirectory.format(directory)\n",
    "    print(\"Checking for (and if necessary, creating) directory {}\".format(outDirectory))\n",
    "    if not os.path.isdir(outDirectory):\n",
    "        os.makedirs(outDirectory)\n",
    "    #Get the files\n",
    "    if 'glob' not in dir():\n",
    "        try:\n",
    "            import glob\n",
    "        except:\n",
    "            raise RuntimeError(\"Could not import the glob module in method root_to_pdf\")\n",
    "    files = glob.glob(\"{}/{}\".format(directory, globKey))\n",
    "    #deduce names from the filenames, with optional stripKey parameter that defaults to .root\n",
    "    names = [fname.split(\"/\")[-1].replace(stripKey, \"\") for fname in files]\n",
    "    fileDict = {}\n",
    "    oFileDict = {}\n",
    "    keysDict = {}\n",
    "    \n",
    "    draw_list = cartesian_product_list(name_format=name_format, name_tuples=name_tuples)\n",
    "    print(draw_list)\n",
    "    c = ROOT.TCanvas(\"c\", \"\", 1200, 900)\n",
    "    c.SetLogx()\n",
    "        \n",
    "    for name, fname in zip(names, files):\n",
    "        print(name)\n",
    "        #inFiles\n",
    "        fileDict[name] = ROOT.TFile.Open(fname, \"READ\")\n",
    "        #hist names\n",
    "        keysDict[name] = [hist.GetName() for hist in fileDict[name].GetListOfKeys()]\n",
    "        #Skip empty files\n",
    "        if len(keysDict[name]) is 0: \n",
    "            print(\"Skipping sample {} whose file contains no histograms\".format(name))\n",
    "            fileDict[name].Close()\n",
    "            continue\n",
    "            \n",
    "        ofname = \"{}/{}.pdf\".format(outDirectory, name).replace(\"//\", \"/\")\n",
    "        dn = 0\n",
    "        dnmax = set(keysDict[name])\n",
    "        dnmax = len(set(keysDict[name]).intersection(set(draw_list)))\n",
    "        for drawable in draw_list:\n",
    "            if drawable not in keysDict[name]:\n",
    "                continue\n",
    "            else:\n",
    "                dn += 1\n",
    "            h = fileDict[name].Get(drawable)\n",
    "            #Text size is based on the marker size (scale factor * pad size * marker size)\n",
    "            h.SetMarkerSize(0.5*h.GetMarkerSize())\n",
    "            if draw_min:\n",
    "                h.SetMinimum(draw_min)\n",
    "            if draw_max:\n",
    "                h.SetMaximum(draw_max)\n",
    "            #good draw options: COLZ TEXTE. Add 2 digits between TEXT and E for rotation (degrees): TEXT90E does 90deg rotation\n",
    "            h.SetStats(0) #disable stats box, it's in the way...\n",
    "            h.Draw(draw_option)\n",
    "            c.Draw()\n",
    "            if dn == 1:\n",
    "                print(\"Opening {}\".format(ofname))\n",
    "                c.SaveAs(ofname + \"(\")\n",
    "            elif dn == dnmax:\n",
    "                print(\"Closing {}\".format(ofname))\n",
    "                c.SaveAs(ofname + \")\")\n",
    "            else:\n",
    "                c.SaveAs(ofname)\n",
    "                \n",
    "        fileDict[name].Close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeJetEfficiencyReport(input_stats_dict, directory, levels_of_iterest=\"All\"):\n",
    "    if not os.path.isdir(directory):\n",
    "        os.makedirs(directory)\n",
    "    stats_dict = collections.OrderedDict()\n",
    "    all_names = []\n",
    "    for name, name_dict in input_stats_dict.items():\n",
    "        all_names.append(name)\n",
    "        for level, level_dict in name_dict.items():\n",
    "            if level not in stats_dict.keys():\n",
    "                stats_dict[level] = collections.OrderedDict()\n",
    "            if name not in stats_dict[level].keys():\n",
    "                stats_dict[level][name] = collections.OrderedDict()\n",
    "            if levels_of_interest is not \"All\" and level not in levels_of_interest: continue\n",
    "            for category, category_dict in level_dict.items():\n",
    "                if category not in stats_dict[level].keys():\n",
    "                    stats_dict[level][name][category] = collections.OrderedDict()\n",
    "                for stat_name, stat_obj in category_dict.items():\n",
    "                    stats_dict[level][name][category][stat_name] = [name, category, stat_name, str(stat_obj.GetMean()), \n",
    "                                                                    str(stat_obj.GetMeanErr()), str(stat_obj.GetRMS())]\n",
    "                        \n",
    "    for level, level_dict in stats_dict.items():\n",
    "        ofname = \"{}/{}_JetEfficiencyReport.csv\".format(directory, level).replace(\"//\", \"/\")\n",
    "        print(\"Opening {}\".format(ofname))\n",
    "        with open(ofname, \"w\") as f:\n",
    "            f.write(\"Sample,Category,Jet Counter,Mean,MeanError,RMS\\n\")\n",
    "            for name, name_dict in level_dict.items():\n",
    "                for category, category_dict in name_dict.items():\n",
    "                    for stat_name, stat_obj in category_dict.items():\n",
    "                        line = \",\".join(stat_obj) + \"\\n\"\n",
    "                        f.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histDir = \"select_20200414\"\n",
    "#pprint.pprint(stats['DYJets_DL']['ElMu_selection'])\n",
    "mode=\"RECREATE\"\n",
    "#mode=\"UPDATE\"\n",
    "if doHLTMeans == True:\n",
    "    makeHLTReport(stats, histDir)\n",
    "if doHistos == True:\n",
    "    writeHistos(histos1D, histDir, \"All\", mode=mode)\n",
    "    mode=\"UPDATE\"\n",
    "if doBtaggingEfficiencies == True:\n",
    "    writeHistos(btagging, histDir, \"All\", mode=mode)\n",
    "    mode=\"UPDATE\"\n",
    "    BtaggingEfficienciesAnalyzer(\"{}/ElMu_selection\".format(histDir))\n",
    "if doBtaggingYields == True:\n",
    "    writeHistos(btagging, histDir, \"BtaggingYields\", mode=mode)\n",
    "    mode=\"UPDATE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbdict={}\n",
    "#BtaggingEfficienciesAnalyzer(\"select_20200323/ElMu_selection\", doNumpyValidation=True, debug=True, debug_dict=dbdict)\n",
    "BtaggingYieldsAnalyzer(\"select_20200409/ElMu_selection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ChiSquareTest(\"select_20200403/ElMu_selection/BtaggingYields/BTaggingYields.root\", test_against=\"Aggregate__deepcsv\", \n",
    "              must_not_contain = [\"up\", \"down\"])\n",
    "ChiSquareTest(\"select_20200403/ElMu_selection/BtaggingYields/BTaggingYields.root\", test_against=\"Aggregate__deepcsv\", \n",
    "              must_contain = [\"Aggregate\"])\n",
    "root_to_pdf(\"select_20200403/ElMu_selection/BtaggingYields\",\n",
    "            outDirectory=\"{}/PDFSamples\",\n",
    "            name_format=\"$NAME__$ALGO$VAR\",\n",
    "            name_tuples=[(\"$NAME\", [\"Aggregate\", \"tt_DL-GF\", \"tt_DL\", \"tttt\", \"ttH\",\" tt_SL-GF\", \"tt_SL\", \n",
    "                                    \"DYJets_DL\", \"ST_tW\", \"ST_tbarW\", \"ttHH\", \"ttWH\", \"ttWJets\", \"ttWW\", \"ttWZ\",\n",
    "                                    \"ttZJets\", \"ttZZ\", \"ttZH\", \"tttJ\", \"tttW\"]), \n",
    "                         (\"$ALGO\", [\"deepcsv\"]),\n",
    "                         (\"$VAR\", [\"\",])],\n",
    "            draw_option=\"COLZ TEXT45E\", draw_min=0.8, draw_max=1.2)\n",
    "root_to_pdf(\"select_20200403/ElMu_selection/BtaggingYields\",\n",
    "            outDirectory=\"{}/PDFVariations\",\n",
    "            name_format=\"$NAME__$ALGO$VAR\",\n",
    "            name_tuples=[(\"$NAME\", [\"Aggregate\",]), \n",
    "                         (\"$ALGO\", [\"deepcsv\"]),\n",
    "                         (\"$VAR\", [\"\", \"_shape_up_hf\", \"_shape_down_hf\"])],\n",
    "            draw_option=\"COLZ TEXT45E\", draw_min=0.8, draw_max=1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=10, linewidth=125)\n",
    "print(dbdict['agg_eff'])\n",
    "print(\"-----------------------------------------------------------------------------------------------\")\n",
    "print(dbdict['agg_err'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if doHistos == True:\n",
    "    histoCombine(\"{}/ElMu_selection\".format(histDir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histDir = \"select_20200323\"\n",
    "root_to_pdf(\"{}/ElMu_selection/BtaggingEfficiencyNotWorking\".format(histDir),\n",
    "         name_tuples=[(\"$JETTYPE\", [\"bjets\", \"cjets\", \"udsgjets\"]), (\"$TAG\", [\"DeepCSV_M\", \"DeepJet_M\"]),\n",
    "                         (\"$CAT\", [\"Inclusive\",])],\n",
    "         draw_option=\"COLZ TEXT45E\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "makeJetEfficiencyReport(effic, \"{}/ElMu_selection/BtaggingEfficiency\".format(histDir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rootDict = {}\n",
    "histDir = \"select_20200310\"\n",
    "if not os.path.isdir(histDir):\n",
    "    os.makedirs(histDir)\n",
    "for name, levels_dict in histos1D.items():\n",
    "    #if \"DY\" not in name and \"t\" not in name: continue\n",
    "    #if theSampleDict[name][\"isData\"] == True: continue\n",
    "    print(name, end='')\n",
    "    #print(theSampleDict[name].keys())\n",
    "    print(\" - c=\" + str(theSampleDict[name][\"color\"]))\n",
    "    for level, obj_dict in levels_dict.items():\n",
    "        if level not in levels_of_interest: continue\n",
    "        if not os.path.isdir(histDir + \"/\" + level):\n",
    "            os.makedirs(histDir + \"/\" + level)\n",
    "        print(\"\\t\" + level)\n",
    "        print(obj_dict.keys())\n",
    "        for pre_obj_name, obj_val in obj_dict[\"Mountains\"].items():\n",
    "            obj_name = \"Mountains_\" + pre_obj_name\n",
    "            print(\"\\t\\t\" + obj_name)\n",
    "            for hname, hist in obj_val.items():\n",
    "                #print(\"\\t\\t\\t\" + hname)\n",
    "                #help(hist)\n",
    "                dictKey = pre_obj_name + \"_\" + hname\n",
    "                if dictKey not in rootDict:\n",
    "                    rootDict[dictKey] = ROOT.TFile.Open(\"{}.root\"\\\n",
    "                                 .format(histDir + \"/\" + level + \"/\"+ dictKey), \"RECREATE\")\n",
    "                rootDict[dictKey].cd()\n",
    "                hptr = hist.GetPtr()\n",
    "                oldname = hptr.GetName()\n",
    "                hptr.SetName(\"{}\".format(name))\n",
    "                hptr.Write()\n",
    "                hptr.SetName(\"{}\".format(oldname)) #Avoid overwriting things by switching back, save from segfault\n",
    "for f in rootDict.values():\n",
    "    f.Close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for name, report in reports.items():\n",
    "    if \"El\" in name or \"Mu\" in name: continue\n",
    "    print(\"{}\".format(name))\n",
    "    report.Print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fff = ROOT.TCanvas(\"fff\", \"\", 800, 600)\n",
    "fff.cd()\n",
    "histos1D[\"tt_DL-GF\"][\"MuMu_selection\"][\"Mountains\"]['nMediumDeepJet2']['Muon_InvMass'].Draw(\"COLZ TEXT\")\n",
    "fff.Draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stacks = {}\n",
    "stacksource = {} #Create sortable lists to fill stacks from\n",
    "stacksource_data = {} #create separte list to append all the data to, so that they can be conbined into one hist file and added to the stacksoure at the end\n",
    "model_dict = [histos1D[k] for k in theSampleDict.keys() if theSampleDict[k][\"isData\"] == False]\n",
    "model_dict = model_dict[0]\n",
    "if len(model_dict) < 1:\n",
    "    raise RuntimeError(\"Failure, no histogram dictionary found to form stacks from\")\n",
    "for level, obj_dict in model_dict.items():\n",
    "    if level not in levels_of_interest: continue\n",
    "    stacks[level] = {}\n",
    "    stacksource[level] = {}\n",
    "    stacksource_data[level] = {}\n",
    "    for obj_name, obj_val in obj_dict.items():\n",
    "        if obj_name == \"Mountains\": continue #extra depth to account for, custom implementation until next version developed\n",
    "        stacks[level][obj_name] = {}\n",
    "        stacksource[level][obj_name] = {}\n",
    "        stacksource_data[level][obj_name] = {}\n",
    "        for hname, hist in obj_val.items():\n",
    "            stacks[level][obj_name][hname] = []\n",
    "            stacks[level][obj_name][hname].append(ROOT.THStack(\"s_{}_{}_{}\".format(level, obj_name, hname), \"{}_{}_{}\".format(level, obj_name, hname)))\n",
    "            stacksource[level][obj_name][hname] = []\n",
    "            stacksource_data[level][obj_name][hname] = []\n",
    "    for pre_obj_name, obj_val in obj_dict[\"Mountains\"].items():\n",
    "        obj_name = \"Mountains_\" + pre_obj_name\n",
    "        stacks[level][obj_name] = {}\n",
    "        stacksource[level][obj_name] = {}\n",
    "        stacksource_data[level][obj_name] = {}\n",
    "        for hname, hist in obj_val.items():\n",
    "            stacks[level][obj_name][hname] = []\n",
    "            stacks[level][obj_name][hname].append(ROOT.THStack(\"s_{}_{}_{}\".format(level, obj_name, hname), \"{}_{}_{}\".format(level, obj_name, hname)))\n",
    "            stacksource[level][obj_name][hname] = []\n",
    "            stacksource_data[level][obj_name][hname] = []\n",
    "for name, levels_dict in histos1D.items():\n",
    "    #if \"DY\" not in name and \"t\" not in name: continue\n",
    "    #if theSampleDict[name][\"isData\"] == True: continue\n",
    "    print(name, end='')\n",
    "    #print(theSampleDict[name].keys())\n",
    "    print(\" - c=\" + str(theSampleDict[name][\"color\"]))\n",
    "    for level, obj_dict in levels_dict.items():\n",
    "        if level not in levels_of_interest: continue\n",
    "        print(\"\\t\" + level)\n",
    "        for pre_obj_name, obj_val in obj_dict[\"Mountains\"].items():\n",
    "            obj_name = \"Mountains_\" + pre_obj_name\n",
    "            print(\"\\t\\t\" + obj_name)\n",
    "            for hname, hist in obj_val.items():\n",
    "                print(\"\\t\\t\\t\" + hname)\n",
    "                #help(hist)\n",
    "                hptr = hist.GetPtr().Clone()\n",
    "                hptr.SetFillColor(theSampleDict[name][\"color\"])\n",
    "                hptr.SetLineColor(theSampleDict[name][\"color\"])\n",
    "                #stacks[level][obj_name][hname].Add(hptr)\n",
    "                #stacksource[level][obj_name][hname].append((hptr, hptr.GetIntegral()))\n",
    "                #Integral fails sometimes, use sum of weights...\n",
    "                if theSampleDict[name][\"isData\"] == False:\n",
    "                    stacksource[level][obj_name][hname].append((hptr, hptr.GetSumOfWeights(), theSampleDict[name][\"isData\"]))\n",
    "                else:\n",
    "                    stacksource_data[level][obj_name][hname].append((hptr, hptr.GetSumOfWeights(), theSampleDict[name][\"isData\"]))\n",
    "        for obj_name, obj_val in obj_dict.items():\n",
    "            if obj_name == \"Mountains\": continue #extra depth to account for, custom implementation until next version developed\n",
    "            print(\"\\t\\t\" + obj_name)\n",
    "            for hname, hist in obj_val.items():\n",
    "                print(\"\\t\\t\\t\" + hname)\n",
    "                #help(hist)\n",
    "                hptr = hist.GetPtr().Clone()\n",
    "                hptr.SetFillColor(theSampleDict[name][\"color\"])\n",
    "                hptr.SetLineColor(theSampleDict[name][\"color\"])\n",
    "                #stacks[level][obj_name][hname].Add(hptr)\n",
    "                #stacksource[level][obj_name][hname].append((hptr, hptr.GetIntegral()))\n",
    "                #Integral fails sometimes, use sum of weights...\n",
    "                if theSampleDict[name][\"isData\"] == False:\n",
    "                    stacksource[level][obj_name][hname].append((hptr, hptr.GetSumOfWeights(), theSampleDict[name][\"isData\"]))\n",
    "                else:\n",
    "                    stacksource_data[level][obj_name][hname].append((hptr, hptr.GetSumOfWeights(), theSampleDict[name][\"isData\"]))\n",
    "print()\n",
    "#Now cycle through and sort each list, once it contains all hists from every source (outermost loop - name - above)\n",
    "print(stacksource_data)\n",
    "for level, obj_dict in model_dict.items():\n",
    "    if level not in levels_of_interest: continue\n",
    "    for pre_obj_name, obj_val in obj_dict[\"Mountains\"].items():\n",
    "        obj_name = \"Mountains_\" + pre_obj_name\n",
    "        for hname, hist in obj_val.items():\n",
    "            #Sort the MC-only histograms\n",
    "            stacksource[level][obj_name][hname].sort(key=lambda b: b[1], reverse=False)\n",
    "            \n",
    "            #Create a MC-only histogram for statistics purposes\n",
    "            tmpMC = None\n",
    "            for himc, h_mc in enumerate(stacksource[level][obj_name][hname]):\n",
    "                if himc == 0:\n",
    "                    #take first histo\n",
    "                    tmpMC = h_mc[0].Clone()\n",
    "                    tmpMC.SetTitle(\"MC\")\n",
    "                else:\n",
    "                    #hadd the other histos\n",
    "                    #tmpMC = tmpMC + h_mc[0].Clone()\n",
    "                    tmpMC.Add(h_mc[0].Clone())\n",
    "            if tmpMC != None:\n",
    "                #tmpMC.SetMarkerStyle(0)\n",
    "                tmpMC.SetLineColor(ROOT.kRed)  #FIXME Color from largest sample?\n",
    "                tmpMC.SetFillColorAlpha(ROOT.kRed, 0) #FIXME\n",
    "            stacks[level][obj_name][hname].append(tmpMC)\n",
    "            \n",
    "            tmp = None\n",
    "            for hid, h_data in enumerate(stacksource_data[level][obj_name][hname]):\n",
    "                if hid == 0:\n",
    "                    #take first histo\n",
    "                    tmp = h_data[0].Clone()\n",
    "                    tmp.SetTitle(\"Data\")\n",
    "                else:\n",
    "                    #hadd the other histos\n",
    "                    #tmp = tmp + h_data[0].Clone()\n",
    "                    tmp.Add(h_data[0].Clone())\n",
    "            if tmp != None:\n",
    "                tmp.SetMarkerStyle(0) #20 round dot, with SetMarkerSize(1.0) in an example\n",
    "                tmp.SetLineColor(ROOT.kBlack)\n",
    "                tmp.SetFillColorAlpha(ROOT.kWhite, 0)\n",
    "            stacks[level][obj_name][hname].append(tmp)\n",
    "                \n",
    "            for hptrTup in stacksource[level][obj_name][hname]:\n",
    "                #add to the THStack in the first position of the tuple\n",
    "                stacks[level][obj_name][hname][0].Add(hptrTup[0])\n",
    "    for obj_name, obj_val in obj_dict.items():\n",
    "        if obj_name == \"Mountains\": continue #extra depth to account for, custom implementation until next version developed\n",
    "        for hname, hist in obj_val.items():\n",
    "            #Sort the MC-only histograms\n",
    "            stacksource[level][obj_name][hname].sort(key=lambda b: b[1], reverse=False)\n",
    "            \n",
    "            #Create a MC-only histogram for statistics purposes\n",
    "            tmpMC = None\n",
    "            for himc, h_mc in enumerate(stacksource[level][obj_name][hname]):\n",
    "                if himc == 0:\n",
    "                    #take first histo\n",
    "                    tmpMC = h_mc[0].Clone()\n",
    "                    tmpMC.SetTitle(\"MC\")\n",
    "                else:\n",
    "                    #hadd the other histos\n",
    "                    #tmpMC = tmpMC + h_mc[0].Clone()\n",
    "                    tmpMC.Add(h_mc[0].Clone())\n",
    "            if tmpMC != None:\n",
    "                #tmpMC.SetMarkerStyle(0)\n",
    "                tmpMC.SetLineColor(ROOT.kRed) #FIXME Color set to highest integral's\n",
    "                tmpMC.SetLineWidth(0)\n",
    "                tmpMC.SetFillColorAlpha(ROOT.kRed, 0)\n",
    "            stacks[level][obj_name][hname].append(tmpMC)\n",
    "            \n",
    "            tmp = None\n",
    "            for hid, h_data in enumerate(stacksource_data[level][obj_name][hname]):\n",
    "                if hid == 0:\n",
    "                    #take first histo\n",
    "                    tmp = h_data[0].Clone()\n",
    "                    tmp.SetTitle(\"Data\")\n",
    "                else:\n",
    "                    #hadd the other histos\n",
    "                    #tmp = tmp + h_data[0].Clone()\n",
    "                    tmp.Add(h_data[0].Clone())\n",
    "            if tmp != None:\n",
    "                tmp.SetMarkerStyle(0) #20 round dot, with SetMarkerSize(1.0) in an example\n",
    "                tmp.SetLineColor(ROOT.kBlack)\n",
    "                #tmp.SetFillColorAlpha(ROOT.kWhite, 0)\n",
    "            stacks[level][obj_name][hname].append(tmp)\n",
    "                \n",
    "            for hptrTup in stacksource[level][obj_name][hname]:\n",
    "                #add to the THStack in the first position of the tuple\n",
    "                stacks[level][obj_name][hname][0].Add(hptrTup[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c2 = ROOT.TCanvas()\n",
    "#c2.cd()\n",
    "#histos2D[\"tttt\"][\"ElMu_selection\"][\"Mountains\"][\"nMediumDeepCSV0\"][\"npvsGood_vs_nTrueInt\"].Draw(\"COLZ\")\n",
    "#c2.Draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "leg = ROOT.TLegend(0.75,0.80, 0.95, 0.90)\n",
    "#leg = ROOT.TLegend(0.15,0.10)\n",
    "#leg.SetFillColor(0)\n",
    "#leg.SetBorderSize(0)\n",
    "leg.SetNColumns(2)\n",
    "leg.SetTextSize(0.03)\n",
    "leg_colors = set([smpl[\"color\"] for smpl in theSampleDict.values()])\n",
    "leg_tuple = [(smpl[0], smpl[1]) for smpl in leg_dict.items() if smpl[1] in leg_colors]\n",
    "leg_hists = {}\n",
    "for samplecategory, color in leg_tuple:\n",
    "    leg_hists[color] = ROOT.TH1D(samplecategory, samplecategory, 0, 0, 1)\n",
    "    if samplecategory != \"Data\":\n",
    "        leg_hists[color].SetFillColor(color)\n",
    "        leg.AddEntry(leg_hists[color], samplecategory, \"F\")\n",
    "    else:\n",
    "        leg.AddEntry(leg_hists[color], samplecategory, \"P\")\n",
    "%jsroot on \n",
    "#help(ROOT.TLegend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%jsroot off\n",
    "for obj_name, obj_dict in stacks[\"ElMu_selection\"].items():\n",
    "    if obj_name != \"Jets\": continue\n",
    "    print(obj_name)\n",
    "    for sname, stack in obj_dict.items():\n",
    "        c = ROOT.TCanvas(\"cs_{}_{}\".format(obj_name, sname), \"\", 800, 600)\n",
    "        c.cd()\n",
    "        c.Update()\n",
    "        #For data first\n",
    "        #if len(stack) > 1:\n",
    "        #    stack[1].Draw(\"PE1\")\n",
    "        #    stack[0].Draw(\"HIST SAME\")\n",
    "        #else:\n",
    "        #    stack[0].Draw(\"HIST\")\n",
    "        #if len(stack) > 1:\n",
    "        #    stack[1].Draw(\"PE1 SAME\")\n",
    "        #for MC first\n",
    "        stack[0].Draw(\"HIST S\")\n",
    "        #The `mode` has up to nine digits that can be set to on (1 or 2), off (0).\n",
    " \n",
    "         #mode = ksiourmen  (default = 000001111)\n",
    "         #k = 1;  kurtosis printed\n",
    "         #k = 2;  kurtosis and kurtosis error printed\n",
    "         #s = 1;  skewness printed\n",
    "         #s = 2;  skewness and skewness error printed\n",
    "         #i = 1;  integral of bins printed\n",
    "         #i = 2;  integral of bins with option \"width\" printed\n",
    "         #o = 1;  number of overflows printed\n",
    "         #u = 1;  number of underflows printed\n",
    "         #r = 1;  standard deviation printed\n",
    "         #r = 2;  standard deviation and standard deviation error printed\n",
    "         #m = 1;  mean value printed\n",
    "         #m = 2;  mean and mean error values printed\n",
    "         #e = 1;  number of entries printed\n",
    "         #n = 1;  name of histogram is printed\n",
    "        if len(stack) > 1 and stack[1] != None:\n",
    "            ROOT.gStyle.SetOptStat(1111111)\n",
    "            stack[1].Draw(\"SAMES\")\n",
    "        if len(stack) > 2 and stack[2] != None:\n",
    "            ROOT.gStyle.SetOptStat(1111111)\n",
    "            stack[2].Draw(\"PE1 SAMES\")\n",
    "        # Add header\n",
    "        cms_label = ROOT.TLatex()\n",
    "        cms_label.SetTextSize(0.04)\n",
    "        cms_label.DrawLatexNDC(0.16, 0.92, \"#bf{CMS Preliminary}\")\n",
    "        header = ROOT.TLatex()\n",
    "        header.SetTextSize(0.03)\n",
    "        header.DrawLatexNDC(0.63, 0.92, \"#sqrt{{s}} = 13 TeV, L_{{int}} = {0} fb^{{-1}}\".format(lumi[era]))\n",
    "        leg.Draw()\n",
    "        c.Draw()\n",
    "        if len(stack) > 1 and stack[1] != None:\n",
    "            ROOT.gStyle.SetOptStat(1001111)\n",
    "            StatBox = stack[1].GetListOfFunctions().FindObject(\"stats\")\n",
    "            StatBox.SetY1NDC(0.7)\n",
    "            StatBox.SetY2NDC(0.5)\n",
    "            #StatBox.SetName(\"MC\")\n",
    "        if len(stack) > 2 and stack[2] != None:\n",
    "            #ROOT.gStyle.SetOptStat(111111)\n",
    "            StatBox = stack[2].GetListOfFunctions().FindObject(\"stats\")\n",
    "            StatBox.SetY1NDC(0.5)\n",
    "            StatBox.SetY2NDC(0.3)\n",
    "            #StatBox.SetLabel(\"Data\")\n",
    "        if not os.path.isdir(\"./{channel}/{object_name}/\".format(channel=fileChannel, object_name=obj_name)):\n",
    "            os.makedirs(\"./{channel}/{object_name}/\".format(channel=fileChannel, object_name=obj_name))\n",
    "        c.SetLogy()\n",
    "        c.SaveAs(\"./{channel}/{object_name}/{hname}_LOGY{filetype}\".format(channel=fileChannel, object_name=obj_name, wgtVar=theWeight, hname=sname, filetype=theFormat))\n",
    "        c.SetLogy(0)\n",
    "        c.SaveAs(\"./{channel}/{object_name}/{hname}_LINY{filetype}\".format(channel=fileChannel, object_name=obj_name, wgtVar=theWeight, hname=sname, filetype=theFormat))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for obj_name, obj_dict in stacks[\"ElMu_selection\"].items():\n",
    "    if obj_name != \"Muons\": continue\n",
    "    print(obj_name)\n",
    "    for sname, stack in obj_dict.items():\n",
    "        c = ROOT.TCanvas(\"cs_{}_{}\".format(obj_name, sname), \"\", 800, 600)\n",
    "        c.cd()\n",
    "        c.Update()\n",
    "        #For data first\n",
    "        #if len(stack) > 1:\n",
    "        #    stack[1].Draw(\"PE1\")\n",
    "        #    stack[0].Draw(\"HIST SAME\")\n",
    "        #else:\n",
    "        #    stack[0].Draw(\"HIST\")\n",
    "        #if len(stack) > 1:\n",
    "        #    stack[1].Draw(\"PE1 SAME\")\n",
    "        #for MC first\n",
    "        stack[0].Draw(\"HIST S\")\n",
    "        if len(stack) > 1 and stack[1] != None:\n",
    "            ROOT.gStyle.SetOptStat(1111111)\n",
    "            stack[1].Draw(\"SAMES\")\n",
    "        if len(stack) > 2 and stack[2] != None:\n",
    "            ROOT.gStyle.SetOptStat(1111111)\n",
    "            stack[2].Draw(\"PE1 SAMES\")\n",
    "        # Add header\n",
    "        cms_label = ROOT.TLatex()\n",
    "        cms_label.SetTextSize(0.04)\n",
    "        cms_label.DrawLatexNDC(0.16, 0.92, \"#bf{CMS Preliminary}\")\n",
    "        header = ROOT.TLatex()\n",
    "        header.SetTextSize(0.03)\n",
    "        header.DrawLatexNDC(0.63, 0.92, \"#sqrt{{s}} = 13 TeV, L_{{int}} = {0} fb^{{-1}}\".format(lumi[era]))\n",
    "        leg.Draw()\n",
    "        c.Draw()\n",
    "        if len(stack) > 1 and stack[1] != None:\n",
    "            ROOT.gStyle.SetOptStat(1001111)\n",
    "            StatBox = stack[1].GetListOfFunctions().FindObject(\"stats\")\n",
    "            StatBox.SetY1NDC(0.7)\n",
    "            StatBox.SetY2NDC(0.5)\n",
    "            #StatBox.SetName(\"MC\")\n",
    "        if len(stack) > 2 and stack[2] != None:\n",
    "            #ROOT.gStyle.SetOptStat(111111)\n",
    "            StatBox = stack[2].GetListOfFunctions().FindObject(\"stats\")\n",
    "            StatBox.SetY1NDC(0.5)\n",
    "            StatBox.SetY2NDC(0.3)\n",
    "            #StatBox.SetLabel(\"Data\")\n",
    "        if not os.path.isdir(\"./{channel}/{object_name}/\".format(channel=fileChannel, object_name=obj_name)):\n",
    "            os.makedirs(\"./{channel}/{object_name}/\".format(channel=fileChannel, object_name=obj_name))\n",
    "        c.SetLogy()\n",
    "        c.SaveAs(\"./{channel}/{object_name}/{hname}_LOGY{filetype}\".format(channel=fileChannel, object_name=obj_name, wgtVar=theWeight, hname=sname, filetype=theFormat))\n",
    "        c.SetLogy(0)\n",
    "        c.SaveAs(\"./{channel}/{object_name}/{hname}_LINY{filetype}\".format(channel=fileChannel, object_name=obj_name, wgtVar=theWeight, hname=sname, filetype=theFormat))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for obj_name, obj_dict in stacks[\"ElMu_selection\"].items():\n",
    "    if obj_name != \"Electrons\": continue\n",
    "    print(obj_name)\n",
    "    for sname, stack in obj_dict.items():\n",
    "        c = ROOT.TCanvas(\"cs_{}_{}\".format(obj_name, sname), \"\", 800, 600)\n",
    "        c.cd()\n",
    "        c.Update()\n",
    "        #For data first\n",
    "        #if len(stack) > 1:\n",
    "        #    stack[1].Draw(\"PE1\")\n",
    "        #    stack[0].Draw(\"HIST SAME\")\n",
    "        #else:\n",
    "        #    stack[0].Draw(\"HIST\")\n",
    "        #if len(stack) > 1:\n",
    "        #    stack[1].Draw(\"PE1 SAME\")\n",
    "        #for MC first\n",
    "        stack[0].Draw(\"HIST S\")\n",
    "        if len(stack) > 1 and stack[1] != None:\n",
    "            ROOT.gStyle.SetOptStat(1111111)\n",
    "            stack[1].Draw(\"SAMES\")\n",
    "        if len(stack) > 2 and stack[2] != None:\n",
    "            ROOT.gStyle.SetOptStat(1111111)\n",
    "            stack[2].Draw(\"PE1 SAMES\")\n",
    "        # Add header\n",
    "        cms_label = ROOT.TLatex()\n",
    "        cms_label.SetTextSize(0.04)\n",
    "        cms_label.DrawLatexNDC(0.16, 0.92, \"#bf{CMS Preliminary}\")\n",
    "        header = ROOT.TLatex()\n",
    "        header.SetTextSize(0.03)\n",
    "        header.DrawLatexNDC(0.63, 0.92, \"#sqrt{{s}} = 13 TeV, L_{{int}} = {0} fb^{{-1}}\".format(lumi[era]))\n",
    "        leg.Draw()\n",
    "        c.Draw()\n",
    "        if len(stack) > 1 and stack[1] != None:\n",
    "            ROOT.gStyle.SetOptStat(1001111)\n",
    "            StatBox = stack[1].GetListOfFunctions().FindObject(\"stats\")\n",
    "            StatBox.SetY1NDC(0.7)\n",
    "            StatBox.SetY2NDC(0.5)\n",
    "            #StatBox.SetName(\"MC\")\n",
    "        if len(stack) > 2 and stack[2] != None:\n",
    "            #ROOT.gStyle.SetOptStat(111111)\n",
    "            StatBox = stack[2].GetListOfFunctions().FindObject(\"stats\")\n",
    "            StatBox.SetY1NDC(0.5)\n",
    "            StatBox.SetY2NDC(0.3)\n",
    "            #StatBox.SetLabel(\"Data\")\n",
    "        if not os.path.isdir(\"./{channel}/{object_name}/\".format(channel=fileChannel, object_name=obj_name)):\n",
    "            os.makedirs(\"./{channel}/{object_name}/\".format(channel=fileChannel, object_name=obj_name))\n",
    "        c.SetLogy()\n",
    "        c.SaveAs(\"./{channel}/{object_name}/{hname}_LOGY{filetype}\".format(channel=fileChannel, object_name=obj_name, wgtVar=theWeight, hname=sname, filetype=theFormat))\n",
    "        c.SetLogy(0)\n",
    "        c.SaveAs(\"./{channel}/{object_name}/{hname}_LINY{filetype}\".format(channel=fileChannel, object_name=obj_name, wgtVar=theWeight, hname=sname, filetype=theFormat))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for obj_name, obj_dict in stacks[\"ElMu_selection\"].items():\n",
    "    if obj_name != \"Leptons\": continue\n",
    "    print(obj_name)\n",
    "    for sname, stack in obj_dict.items():\n",
    "        c = ROOT.TCanvas(\"cs_{}_{}\".format(obj_name, sname), \"\", 800, 600)\n",
    "        c.cd()\n",
    "        c.Update()\n",
    "        #For data first\n",
    "        #if len(stack) > 1:\n",
    "        #    stack[1].Draw(\"PE1\")\n",
    "        #    stack[0].Draw(\"HIST SAME\")\n",
    "        #else:\n",
    "        #    stack[0].Draw(\"HIST\")\n",
    "        #if len(stack) > 1:\n",
    "        #    stack[1].Draw(\"PE1 SAME\")\n",
    "        #for MC first\n",
    "        stack[0].Draw(\"HIST S\")\n",
    "        if len(stack) > 1 and stack[1] != None:\n",
    "            ROOT.gStyle.SetOptStat(1111111)\n",
    "            stack[1].Draw(\"SAMES\")\n",
    "        if len(stack) > 2 and stack[2] != None:\n",
    "            ROOT.gStyle.SetOptStat(1111111)\n",
    "            stack[2].Draw(\"PE1 SAMES\")\n",
    "        # Add header\n",
    "        cms_label = ROOT.TLatex()\n",
    "        cms_label.SetTextSize(0.04)\n",
    "        cms_label.DrawLatexNDC(0.16, 0.92, \"#bf{CMS Preliminary}\")\n",
    "        header = ROOT.TLatex()\n",
    "        header.SetTextSize(0.03)\n",
    "        header.DrawLatexNDC(0.63, 0.92, \"#sqrt{{s}} = 13 TeV, L_{{int}} = {0} fb^{{-1}}\".format(lumi[era]))\n",
    "        leg.Draw()\n",
    "        c.Draw()\n",
    "        if len(stack) > 1 and stack[1] != None:\n",
    "            ROOT.gStyle.SetOptStat(1001111)\n",
    "            StatBox = stack[1].GetListOfFunctions().FindObject(\"stats\")\n",
    "            StatBox.SetY1NDC(0.7)\n",
    "            StatBox.SetY2NDC(0.5)\n",
    "            #StatBox.SetName(\"MC\")\n",
    "        if len(stack) > 2 and stack[2] != None:\n",
    "            #ROOT.gStyle.SetOptStat(111111)\n",
    "            StatBox = stack[2].GetListOfFunctions().FindObject(\"stats\")\n",
    "            StatBox.SetY1NDC(0.5)\n",
    "            StatBox.SetY2NDC(0.3)\n",
    "            #StatBox.SetLabel(\"Data\")\n",
    "        if not os.path.isdir(\"./{channel}/{object_name}/\".format(channel=fileChannel, object_name=obj_name)):\n",
    "            os.makedirs(\"./{channel}/{object_name}/\".format(channel=fileChannel, object_name=obj_name))\n",
    "        c.SetLogy()\n",
    "        c.SaveAs(\"./{channel}/{object_name}/{hname}_LOGY{filetype}\".format(channel=fileChannel, object_name=obj_name, wgtVar=theWeight, hname=sname, filetype=theFormat))\n",
    "        c.SetLogy(0)\n",
    "        c.SaveAs(\"./{channel}/{object_name}/{hname}_LINY{filetype}\".format(channel=fileChannel, object_name=obj_name, wgtVar=theWeight, hname=sname, filetype=theFormat))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for obj_name, obj_dict in stacks[\"ElMu_selection\"].items():\n",
    "    if obj_name != \"EventVars\": continue\n",
    "    print(obj_name)\n",
    "    for sname, stack in obj_dict.items():\n",
    "        c = ROOT.TCanvas(\"cs_{}_{}\".format(obj_name, sname), \"\", 800, 600)\n",
    "        c.cd()\n",
    "        c.Update()\n",
    "        #For data first\n",
    "        #if len(stack) > 1:\n",
    "        #    stack[1].Draw(\"PE1\")\n",
    "        #    stack[0].Draw(\"HIST SAME\")\n",
    "        #else:\n",
    "        #    stack[0].Draw(\"HIST\")\n",
    "        #if len(stack) > 1:\n",
    "        #    stack[1].Draw(\"PE1 SAME\")\n",
    "        #for MC first\n",
    "        stack[0].Draw(\"HIST S\")\n",
    "        if len(stack) > 1 and stack[1] != None:\n",
    "            ROOT.gStyle.SetOptStat(1111111)\n",
    "            stack[1].Draw(\"SAMES\")\n",
    "        if len(stack) > 2 and stack[2] != None:\n",
    "            ROOT.gStyle.SetOptStat(1111111)\n",
    "            stack[2].Draw(\"PE1 SAMES\")\n",
    "        # Add header\n",
    "        cms_label = ROOT.TLatex()\n",
    "        cms_label.SetTextSize(0.04)\n",
    "        cms_label.DrawLatexNDC(0.16, 0.92, \"#bf{CMS Preliminary}\")\n",
    "        header = ROOT.TLatex()\n",
    "        header.SetTextSize(0.03)\n",
    "        header.DrawLatexNDC(0.63, 0.92, \"#sqrt{{s}} = 13 TeV, L_{{int}} = {0} fb^{{-1}}\".format(lumi[era]))\n",
    "        leg.Draw()\n",
    "        c.Draw()\n",
    "        if len(stack) > 1 and stack[1] != None:\n",
    "            ROOT.gStyle.SetOptStat(1001111)\n",
    "            StatBox = stack[1].GetListOfFunctions().FindObject(\"stats\")\n",
    "            StatBox.SetY1NDC(0.7)\n",
    "            StatBox.SetY2NDC(0.5)\n",
    "            #StatBox.SetName(\"MC\")\n",
    "        if len(stack) > 2 and stack[2] != None:\n",
    "            #ROOT.gStyle.SetOptStat(111111)\n",
    "            StatBox = stack[2].GetListOfFunctions().FindObject(\"stats\")\n",
    "            StatBox.SetY1NDC(0.5)\n",
    "            StatBox.SetY2NDC(0.3)\n",
    "            #StatBox.SetLabel(\"Data\")\n",
    "        if not os.path.isdir(\"./{channel}/{object_name}/\".format(channel=fileChannel, object_name=obj_name)):\n",
    "            os.makedirs(\"./{channel}/{object_name}/\".format(channel=fileChannel, object_name=obj_name))\n",
    "        c.SetLogy()\n",
    "        c.SaveAs(\"./{channel}/{object_name}/{hname}_LOGY{filetype}\".format(channel=fileChannel, object_name=obj_name, wgtVar=theWeight, hname=sname, filetype=theFormat))\n",
    "        c.SetLogy(0)\n",
    "        c.SaveAs(\"./{channel}/{object_name}/{hname}_LINY{filetype}\".format(channel=fileChannel, object_name=obj_name, wgtVar=theWeight, hname=sname, filetype=theFormat))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#%jsroot on\n",
    "unblind_whitelist = set([])\n",
    "CanvasCache = {}\n",
    "CanvasCache[\"Open/Close\"] = ROOT.TCanvas(\"open_close\", \"\", 800, 100)\n",
    "\n",
    "for obj_name, obj_dict in stacks[\"ElMu_selection\"].items():\n",
    "    if \"Mountains\" not in obj_name: continue\n",
    "    print(obj_name)\n",
    "    CanvasCache[obj_name] = {}\n",
    "    #Save to a pdf using the '.pdf(' string to make a file that stays open for subsequent writes to the same filename. To be closed by '.pdf)' \n",
    "    CanvasCache[\"Open/Close\"].SaveAs(\"./{channel}/{object_name}_All.pdf(\".format(channel=fileChannel.replace(\"_selection\", \"\"), object_name=obj_name.replace(\"Mountains_\", \"\")))\n",
    "    for sname, stack in sorted(obj_dict.items()):\n",
    "        c = None\n",
    "        if \"_vs_\" in sname: #hack to draw 2D histos in the 1D histo dict\n",
    "            CanvasCache[obj_name][sname] = ROOT.TCanvas(\"cs_{}_{}\".format(obj_name, sname), \"\", 800, 1800)\n",
    "            CanvasCache[obj_name][sname].Divide(1,3)\n",
    "            CanvasCache[obj_name][sname].cd(1)\n",
    "        else:\n",
    "        #if True:\n",
    "            CanvasCache[obj_name][sname] = ROOT.TCanvas(\"cs_{}_{}\".format(obj_name, sname), \"\", 800, 600)\n",
    "            CanvasCache[obj_name][sname].cd()\n",
    "        CanvasCache[obj_name][sname].SetLogy()\n",
    "        CanvasCache[obj_name][sname].Update()\n",
    "        #For data first\n",
    "        #if len(stack) > 1:\n",
    "        #    stack[1].Draw(\"PE1\")\n",
    "        #    stack[0].Draw(\"HIST SAME\")\n",
    "        #else:\n",
    "        #    stack[0].Draw(\"HIST\")\n",
    "        #if len(stack) > 1:\n",
    "        #    stack[1].Draw(\"PE1 SAME\")\n",
    "        #for MC first\n",
    "        if \"_vs_\" in sname: #hack to draw 2D histos in the 1D histo dict\n",
    "            stack[0].Draw(\"\")\n",
    "        else:\n",
    "            stack[0].Draw(\"HIST S\")\n",
    "        #Draw the MC summed histogram for stats (better way with THStack? why the fuck don't people document this in a useful way?)\n",
    "        if len(stack) > 1 and stack[1] != None:\n",
    "            ROOT.gStyle.SetOptStat(1111111)\n",
    "            if \"_vs_\" in sname: #hack to draw 2D histos in the 1D histo dict\n",
    "                stack[1].Draw(\"SAMES\") #Also SAME0, SAMES0 which 'do not use the z axis of the previous plot'\n",
    "                CanvasCache[obj_name][sname].cd(2)\n",
    "                tmp1 = stack[1].Clone()\n",
    "                tmp1.SetLineColor(ROOT.kBlue)\n",
    "                tmp1.SetFillColorAlpha(ROOT.kGreen, 0.7)\n",
    "                tmp1.Draw(\"VIOLINX\")\n",
    "                #stack[1].ProfileX().Draw(\"E3\")\n",
    "                CanvasCache[obj_name][sname].cd(3)\n",
    "                tmp1.Draw(\"VIOLINY\")\n",
    "                #stack[1].ProfileY().Draw(\"E3\")\n",
    "                CanvasCache[obj_name][sname].cd(1)\n",
    "            else:\n",
    "                stack[1].Draw(\"SAMES HIST\")\n",
    "        #Draw the data histogram, assuming that it's unblinded (\"blind\" not in the name and or in the whitelist )\n",
    "        if len(stack) > 2 and stack[2] != None and (\"blind\" not in obj_name or obj_name in unblind_whitelist):\n",
    "            ROOT.gStyle.SetOptStat(1111111)\n",
    "            if \"_vs_\" in sname: #hack to draw 2D histos in the 1D histo dict\n",
    "                stack[2].Draw(\"SAMES\") #Maybe add ARR for arrow mode? or something else\n",
    "                CanvasCache[obj_name][sname].cd(2)\n",
    "                tmp2 = stack[2].Clone()\n",
    "                tmp2.SetLineColor(ROOT.kRed)\n",
    "                tmp2.SetFillColorAlpha(ROOT.kGray, 0.4)\n",
    "                tmp2.Draw(\"SAMES CANDLEX\")\n",
    "                #stack[2].ProfileX().Draw(\"PE1 SAMES\")\n",
    "                CanvasCache[obj_name][sname].cd(3)\n",
    "                tmp2.Draw(\"SAMES CANDLEY\")\n",
    "                #stack[2].ProfileY().Draw(\"PE1 SAMES\")\n",
    "                CanvasCache[obj_name][sname].cd(1)\n",
    "            else:\n",
    "                stack[2].Draw(\"PE1 SAMES\")\n",
    "        # Add header\n",
    "        cms_label = ROOT.TLatex()\n",
    "        cms_label.SetTextSize(0.04)\n",
    "        cms_label.DrawLatexNDC(0.16, 0.92, \"#bf{CMS Internal}\")\n",
    "        header = ROOT.TLatex()\n",
    "        header.SetTextSize(0.03)\n",
    "        header.DrawLatexNDC(0.63, 0.92, \"#sqrt{{s}} = 13 TeV, L_{{int}} = {0} fb^{{-1}}\".format(lumi[era]))\n",
    "        leg.Draw()\n",
    "        CanvasCache[obj_name][sname].Draw()\n",
    "        if len(stack) > 1 and stack[1] != None:\n",
    "            ROOT.gStyle.SetOptStat(1001111)\n",
    "            StatBox = stack[1].GetListOfFunctions().FindObject(\"stats\")\n",
    "            StatBox.SetY1NDC(0.7)\n",
    "            StatBox.SetY2NDC(0.5)\n",
    "            #StatBox.SetName(\"MC\")\n",
    "        if len(stack) > 2 and stack[2] != None and (\"blind\" not in obj_name or obj_name in unblind_whitelist):\n",
    "            #ROOT.gStyle.SetOptStat(111111)\n",
    "            StatBox = stack[2].GetListOfFunctions().FindObject(\"stats\")\n",
    "            StatBox.SetY1NDC(0.5)\n",
    "            StatBox.SetY2NDC(0.3)\n",
    "            #StatBox.SetLabel(\"Data\")\n",
    "\n",
    "        if not os.path.isdir(\"./{channel}/{object_name}/\".format(channel=fileChannel, object_name=obj_name)):\n",
    "            os.makedirs(\"./{channel}/{object_name}/\".format(channel=fileChannel, object_name=obj_name))\n",
    "        CanvasCache[obj_name][sname].SetLogy()\n",
    "        CanvasCache[obj_name][sname].SaveAs(\"./{channel}/{object_name}/{hname}_LOGY{filetype}\".format(channel=fileChannel, object_name=obj_name, wgtVar=theWeight, hname=sname, filetype=theFormat))\n",
    "        CanvasCache[obj_name][sname].SaveAs(\"./{channel}/{object_name}_All.pdf\".format(channel=fileChannel.replace(\"_selection\", \"\"), object_name=obj_name.replace(\"Mountains_\", \"\"), wgtVar=theWeight, hname=sname, filetype=theFormat))\n",
    "        CanvasCache[obj_name][sname].SetLogy(0)\n",
    "        CanvasCache[obj_name][sname].SaveAs(\"./{channel}/{object_name}/{hname}_LINY{filetype}\".format(channel=fileChannel, object_name=obj_name, wgtVar=theWeight, hname=sname, filetype=theFormat))\n",
    "        CanvasCache[obj_name][sname].SaveAs(\"./{channel}/{object_name}_All.pdf\".format(channel=fileChannel.replace(\"_selection\", \"\"), object_name=obj_name.replace(\"Mountains_\", \"\"), wgtVar=theWeight, hname=sname, filetype=theFormat))\n",
    "    #Close the pdf using '.pdf)' \n",
    "    CanvasCache[\"Open/Close\"].SaveAs(\"./{channel}/{object_name}_All.pdf)\".format(channel=fileChannel.replace(\"_selection\", \"\"), object_name=obj_name.replace(\"Mountains_\", \"\")))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masterfinish2 = time.time() #clock gives cpu time, not accurate multi-core?\n",
    "print(\"Took {}m {}s to process in real-time including plots\".format((masterfinish2 - masterstart)//60, (masterfinish - masterstart)%60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From example: https://root.cern.ch/doc/master/df103__NanoAODHiggsAnalysis_8py_source.html\n",
    "\n",
    "#def plot(sig, bkg, data, x_label, filename):\n",
    "#     \"\"\"\n",
    "#     Plot invariant mass for signal and background processes from simulated\n",
    "#     events overlay the measured data.\n",
    "#     \"\"\"\n",
    "#     # Canvas and general style options\n",
    "#     ROOT.gStyle.SetOptStat(0)\n",
    "#     ROOT.gStyle.SetTextFont(42)\n",
    "#     d = ROOT.TCanvas(\"d\", \"\", 800, 700)\n",
    "#     d.SetLeftMargin(0.15)\n",
    "# \n",
    "#     # Get signal and background histograms and stack them to show Higgs signal\n",
    "#     # on top of the background process\n",
    "#     h_bkg = bkg\n",
    "#     h_cmb = sig.Clone()\n",
    "# \n",
    "#     h_cmb.Add(h_bkg)\n",
    "#     h_cmb.SetTitle(\"\")\n",
    "#     h_cmb.GetXaxis().SetTitle(x_label)\n",
    "#     h_cmb.GetXaxis().SetTitleSize(0.04)\n",
    "#     h_cmb.GetYaxis().SetTitle(\"N_{Events}\")\n",
    "#     h_cmb.GetYaxis().SetTitleSize(0.04)\n",
    "#     h_cmb.SetLineColor(ROOT.kRed)\n",
    "#     h_cmb.SetLineWidth(2)\n",
    "#     h_cmb.SetMaximum(18)\n",
    "#     h_bkg.SetLineWidth(2)\n",
    "#     h_bkg.SetFillStyle(1001)\n",
    "#     h_bkg.SetLineColor(ROOT.kBlack)\n",
    "#     h_bkg.SetFillColor(ROOT.kAzure - 9)\n",
    "# \n",
    "#     # Get histogram of data points\n",
    "#     h_data = data\n",
    "#     h_data.SetLineWidth(1)\n",
    "#     h_data.SetMarkerStyle(20)\n",
    "#     h_data.SetMarkerSize(1.0)\n",
    "#     h_data.SetMarkerColor(ROOT.kBlack)\n",
    "#     h_data.SetLineColor(ROOT.kBlack)\n",
    "# \n",
    "#     # Draw histograms\n",
    "#     h_cmb.DrawCopy(\"HIST\")\n",
    "#     h_bkg.DrawCopy(\"HIST SAME\")\n",
    "#     h_data.DrawCopy(\"PE1 SAME\")\n",
    "# \n",
    "#     # Add legend\n",
    "#     legend = ROOT.TLegend(0.62, 0.70, 0.82, 0.88)\n",
    "#     legend.SetFillColor(0)\n",
    "#     legend.SetBorderSize(0)\n",
    "#     legend.SetTextSize(0.03)\n",
    "#     legend.AddEntry(h_data, \"Data\", \"PE1\")\n",
    "#     legend.AddEntry(h_bkg, \"ZZ\", \"f\")\n",
    "#     legend.AddEntry(h_cmb, \"m_{H} = 125 GeV\", \"f\")\n",
    "#     legend.Draw()\n",
    "# \n",
    "#     # Add header\n",
    "#     cms_label = ROOT.TLatex()\n",
    "#     cms_label.SetTextSize(0.04)\n",
    "#     cms_label.DrawLatexNDC(0.16, 0.92, \"#bf{CMS Open Data}\")\n",
    "#     header = ROOT.TLatex()\n",
    "#     header.SetTextSize(0.03)\n",
    "#     header.DrawLatexNDC(0.63, 0.92, \"#sqrt{s} = 8 TeV, L_{int} = 11.6 fb^{-1}\")\n",
    "# \n",
    "#     # Save plot\n",
    "#     d.SaveAs(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gg1 = ROOT.ROOT.RDF.SaveGraph(the_df['tttt']['ElMu_selection'], './mydot.dot')\n",
    "#!dot -Tsvg mydot.dot -o mydot.svg\n",
    "#listOfImageNames = ['./mydot.svg',\n",
    "#                    ]\n",
    "#\n",
    "#for imageName in listOfImageNames:\n",
    "#    display(SVG(filename=imageName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c = ROOT.TCanvas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stacks[\"ElMu_selection\"][\"Mountains_nMediumDeepJet0\"][\"Muon_pfRelIso03_chg_vs_MET\"]\n",
    "#stacksource_data[\"ElMu_selection\"][\"Mountains_nMediumDeepJet0\"][\"Muon_pfRelIso03_chg_vs_MET\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  },
  "sparkconnect": {
   "bundled_options": [],
   "list_of_options": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
